{
 "awd_id": "1140752",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Teaching Software Modularity through Architectural Review",
 "cfda_num": "47.076",
 "org_code": "11040200",
 "po_phone": "7032922832",
 "po_email": "ptymann@nsf.gov",
 "po_sign_block_name": "Paul Tymann",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 68444.0,
 "awd_amount": 68444.0,
 "awd_min_amd_letter_date": "2012-07-03",
 "awd_max_amd_letter_date": "2012-07-03",
 "awd_abstract_narration": "Drexel University, Carnegie Mellon University and the University of Hawaii are advancing the education of software engineers through research on teaching software modularity. This project addresses designing for modularity with an approach based on design rule theory, design structure matrix modeling, and architecture review. Activities include development of labs and homework assignments featuring a series of evolution scenarios for realistic software applications. A teaching package which includes the activities, instructional materials, and a tool for detecting modularity problems is being constructed. The tool is used to identify design problems within student implementations. Several approaches to performing architecture reviews are being evaluated to determine which approach best helps students design better modularized software.\r\n\r\nDesigning for modularity is a fundamental topic in educating software engineers, yet there has been little rigorous research on how to teach it.  This project leverages research results to facilitate teaching practice and has the potential to advance our basic understanding of the causes of design problems that may eventually result in maintenance difficulties. Project results may fundamentally change the way software design is taught by introducing rigorous modularity analysis techniques and semi-automatic architecture review into the classroom, resulting in better trained software designers who are equipped with the knowledge, skills, and tools to produce software that incurs much lower maintenance costs.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yuanfang",
   "pi_last_name": "Cai",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yuanfang Cai",
   "pi_email_addr": "yfcai@cs.drexel.edu",
   "nsf_id": "000244198",
   "pi_start_date": "2012-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Drexel University",
  "inst_street_address": "3141 CHESTNUT ST",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158956342",
  "inst_zip_code": "191042875",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "DREXEL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "XF3XM9642N96"
 },
 "perf_inst": {
  "perf_inst_name": "Drexel University",
  "perf_str_addr": "3141 Chestnut Street",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191042875",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "153600",
   "pgm_ele_name": "S-STEM-Schlr Sci Tech Eng&Math"
  },
  {
   "pgm_ele_code": "751300",
   "pgm_ele_name": "TUES-Type 1 Project"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0412",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001213DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "13XX",
   "app_name": "H-1B FUND, EHR, NSF",
   "app_symb_id": "045176",
   "fund_code": "1300XXXXDB",
   "fund_name": "H-1B FUND, EDU, NSF",
   "fund_symb_id": "045176"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 68444.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our objective in this research project was to advance our understanding of why students make modularity mistakes in software design and development, and how we can train students to correct these mistakes and avoid making them in the first place.&nbsp; We proposed a combination of review techniques and tools that would, we hypothesized, address the shortcomings of existing approaches to teaching software design, so that modularity problems&mdash;primarily unwanted dependencies&mdash;in students&rsquo; implementation could be automatically detected, and so that students could better understand the negative implications of these dependencies.&nbsp; <br /><br />To achieve this we prosecuted the following project activities: <br /><br />1. We created a suite of pedagogical materials for software design that we employed to understand why students make modularity mistakes and how to automatically detect these mistakes. These materials included: a pre-survey, several lab assignments, an associated architecture review form that guides the student in assessing the impacts of possible future changes, a model solution and associated DSM (design structure matrix) showing the modular structure of the model solution, and a post-survey.<br /><br />2. We conducted a set of experiments at Drexel,&nbsp; Cal Poly, and Carnegie Mellon University with more than one hundred undergraduate students. In these experiments, we separated students into three groups: a control group (which completed their design assignment without any formal design review), a self-review group, and an instructor-guided review group.&nbsp; <br /><br />3. We analyzed the results of those experiments and commented on the implications of the findings for software design and for the teaching of software design. <br /><br />To make the instructor-guided review more standardized so that other instructors can be properly trained, in our latest experiment at Drexel university, for each student who participated in the instructor-guided review, we recorded the review process so that both the instruction process and student responses could be further studied. <br /><br />The experiments revealed that even for the best students, who excel in programming and in the theory of design patterns, it is still extremely easy for them to unintentionally add extra dependencies, which seriously undermine the intent (and value) of the designed software structure, and negatively impact software maintainability.<br /><br />Furthermore, we learned that a DSM, by itself, is not sufficient to aid the students in recognizing and correcting their mistakes.&nbsp; A combination of the DSM and architecture review appears to be more efficient and leads to better outcomes.<br /><br />This, then, is a guide to how software design should be taught to undergraduate students.&nbsp; First, they need to get rapid feedback on their designs, and design mistakes.&nbsp; Second, they need to receive a combination of automated analysis, which gives precise and rapid feedback, and human-directed reviews, which help the students to understand the consequences of the poor decisions that they made. Third, software coding without making the intended modular structure explicit can lead to working, but extremely hard to maintain software, which would be the root cause of high maintenance costs we have observed in software industry.&nbsp;&nbsp;&nbsp; <br /><br />We have repeated the experiments in all three universities, and have obtained similar conclusions after interviewing more than 100 students: the tool reviewed problems that were impossible for the students to detect otherwise, and the architecture review was the key for the students to understand the consequences of design flaws. Currently the tool we developed to automate the analysis, Titan, has been downloaded by 12 universities all over the world. <br /><br />We have published the results of our studies i...",
  "por_txt_cntn": "\nOur objective in this research project was to advance our understanding of why students make modularity mistakes in software design and development, and how we can train students to correct these mistakes and avoid making them in the first place.  We proposed a combination of review techniques and tools that would, we hypothesized, address the shortcomings of existing approaches to teaching software design, so that modularity problems&mdash;primarily unwanted dependencies&mdash;in students\u00c6 implementation could be automatically detected, and so that students could better understand the negative implications of these dependencies.  \n\nTo achieve this we prosecuted the following project activities: \n\n1. We created a suite of pedagogical materials for software design that we employed to understand why students make modularity mistakes and how to automatically detect these mistakes. These materials included: a pre-survey, several lab assignments, an associated architecture review form that guides the student in assessing the impacts of possible future changes, a model solution and associated DSM (design structure matrix) showing the modular structure of the model solution, and a post-survey.\n\n2. We conducted a set of experiments at Drexel,  Cal Poly, and Carnegie Mellon University with more than one hundred undergraduate students. In these experiments, we separated students into three groups: a control group (which completed their design assignment without any formal design review), a self-review group, and an instructor-guided review group.  \n\n3. We analyzed the results of those experiments and commented on the implications of the findings for software design and for the teaching of software design. \n\nTo make the instructor-guided review more standardized so that other instructors can be properly trained, in our latest experiment at Drexel university, for each student who participated in the instructor-guided review, we recorded the review process so that both the instruction process and student responses could be further studied. \n\nThe experiments revealed that even for the best students, who excel in programming and in the theory of design patterns, it is still extremely easy for them to unintentionally add extra dependencies, which seriously undermine the intent (and value) of the designed software structure, and negatively impact software maintainability.\n\nFurthermore, we learned that a DSM, by itself, is not sufficient to aid the students in recognizing and correcting their mistakes.  A combination of the DSM and architecture review appears to be more efficient and leads to better outcomes.\n\nThis, then, is a guide to how software design should be taught to undergraduate students.  First, they need to get rapid feedback on their designs, and design mistakes.  Second, they need to receive a combination of automated analysis, which gives precise and rapid feedback, and human-directed reviews, which help the students to understand the consequences of the poor decisions that they made. Third, software coding without making the intended modular structure explicit can lead to working, but extremely hard to maintain software, which would be the root cause of high maintenance costs we have observed in software industry.    \n\nWe have repeated the experiments in all three universities, and have obtained similar conclusions after interviewing more than 100 students: the tool reviewed problems that were impossible for the students to detect otherwise, and the architecture review was the key for the students to understand the consequences of design flaws. Currently the tool we developed to automate the analysis, Titan, has been downloaded by 12 universities all over the world. \n\nWe have published the results of our studies in:  \n1) Yuanfang Cai, Rick Kazman, Ciera Jaspan, Jonathan Aldrich (2013): Introducing Tool-Supported Architecture Review into Software Design Education. Conference on Software Engineering Education and Training (CSEE&a..."
 }
}