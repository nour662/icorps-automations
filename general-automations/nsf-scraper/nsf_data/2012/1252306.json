{
 "awd_id": "1252306",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: EAGER: Exploratory Research on Scalable Resiliency Through Shadow Computing and Differential Data Replication",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2013-01-01",
 "awd_exp_date": "2015-12-31",
 "tot_intn_awd_amt": 299800.0,
 "awd_amount": 299800.0,
 "awd_min_amd_letter_date": "2012-08-20",
 "awd_max_amd_letter_date": "2012-08-20",
 "awd_abstract_narration": "As our reliance on IT continues to increase,  future applications will involve the processing of massive amounts of data and will require an exascale computing infrastructure in which the number of computing, communications and storage elements will increase by several orders of magnitude. Such an infrastructure will inevitably incorporate new classes of high density, low latency and low power non-volatile memory. This, in turn, will increase by orders of magnitude the rate of failures making resiliency a major concern. \r\n\r\nThis project addresses this resiliency challenge by taking a radical approach to fault-tolerance, which goes beyond the current approach of checkpointing and rollback recover. It introduces innovative and scalable fault-tolerance mechanisms, namely shadow-computing and quality-of-data (QoD) aware replication, as building blocks for a ?tunable? resiliency framework that leverage the new and emerging memory technology and takes into consideration the nature of the data and the requirements of the underlying application. \r\n\r\nIt is expected that the project will lead to new insights into the multi-faceted and challenging resiliency problem in exascale computing platforms. The expected outcomes of the project are a new fault-tolerance computational model and a suite of QoD-aware replication methods that, when combined with storage level resiliency, will lead to high availability with minimized access delay in exascale computing environments.\r\n \r\nThe project seeks to involve graduate and undergraduate students in all its research thrusts. In addition to their contributions in the research activities, involved students also participate fully in outreach, dissemination and community efforts activities. The project also seeks to leverage existing collaboration with industrial partners to involve students in summer internships and provide them with first hand exposure to research and development in an industrial setting. A main objective of the recruiting effort is to seek the involvement of students from minorities and under-represented groups in the project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Rami",
   "pi_last_name": "Melhem",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Rami Melhem",
   "pi_email_addr": "melhem@cs.pitt.edu",
   "nsf_id": "000214915",
   "pi_start_date": "2012-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Taieb",
   "pi_last_name": "Znati",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Taieb Znati",
   "pi_email_addr": "znati@cs.pitt.edu",
   "nsf_id": "000286797",
   "pi_start_date": "2012-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pittsburgh",
  "inst_street_address": "4200 FIFTH AVENUE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4126247400",
  "inst_zip_code": "152600001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "UNIVERSITY OF PITTSBURGH - OF THE COMMONWEALTH SYSTEM OF HIGHER EDUCATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "MKAGLD59JRL1"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pittsburgh",
  "perf_str_addr": "123 University Club",
  "perf_city_name": "Pittsburgh",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152132303",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 299800.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A new resilience mechanism was proposed for both High Performance Computing (HPC) and Cloud Computing environments. Maximizing throughput is the main objective of the former while satisfying Service Level Agreements (SLAs) become a critical aspect of the latter. As the demand for HPC and cloud computing accelerates, the underlying infrastructure is expected to ensure performance, reliability and cost-effectiveness, even with multifold increase in the number of computing, storage and communication components.</p>\n<p>Current resilience approaches rely upon either time or hardware redundancy in order to tolerate failure. The first approach, which rolls back the computation and re-executes after a failure, is subject to a significant delay. The second approach exploits hardware redundancy and executes multiple instances of the same task in parallel to overcome failure and guarantee that at least one replica reaches completion. This solution, however, increases the energy consumption for a given service, which in turn might outweigh the profit gained by providing the service. The trade-off between performance, fault-tolerance and power consumption calls for new frameworks which is energy and performance aware when dealing with failures.</p>\n<p>To this end, we introduce Shadow Computing to address the above trade-off challenge. The basic tenet of Shadow Computing is to associate with each main process a suite of &ldquo;shadows&rdquo; whose sizes depend on the &ldquo;criticality&rdquo; of the application and its performance requirements. Similar to traditional process replication, Shadow Computing ensures successful task completion by concurrently running multiple instances (processes) of the same task. Contrary to the traditional approach, however, Shadow Computing executes the main process of the task at the speed required to maximize profit and slows down the execution of the shadow processes to save energy. Slowing down the shadows can be achieved by either reducing the voltage/frequency of the processor or by co-locating multiple shadows on the same process. Adjusting the speed of execution enables a parameterized trade-off between response time, energy consumption and hardware redundancy. This allows for the optimization of throughput in case of HPC, and the maximization of the expected profit by accounting for income, potential penalties and energy cost, in the case of Cloud Computing.</p>\n<p>Results from sensitivity study using both analytical models and simulation showed that Shadow Computing can achieve significant energy savings (up to 30%) and profit gains (up to 19%) compared to traditional process replication and checkpointing-restart, without violating the resilience or SLA constraints. An initial implementation of Shadow Computing in an MPI environment showd the feasibility of the concept and its potential for increasing resilience at a reasonable overhead cost.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/15/2016<br>\n\t\t\t\t\tModified by: Rami&nbsp;Melhem</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nA new resilience mechanism was proposed for both High Performance Computing (HPC) and Cloud Computing environments. Maximizing throughput is the main objective of the former while satisfying Service Level Agreements (SLAs) become a critical aspect of the latter. As the demand for HPC and cloud computing accelerates, the underlying infrastructure is expected to ensure performance, reliability and cost-effectiveness, even with multifold increase in the number of computing, storage and communication components.\n\nCurrent resilience approaches rely upon either time or hardware redundancy in order to tolerate failure. The first approach, which rolls back the computation and re-executes after a failure, is subject to a significant delay. The second approach exploits hardware redundancy and executes multiple instances of the same task in parallel to overcome failure and guarantee that at least one replica reaches completion. This solution, however, increases the energy consumption for a given service, which in turn might outweigh the profit gained by providing the service. The trade-off between performance, fault-tolerance and power consumption calls for new frameworks which is energy and performance aware when dealing with failures.\n\nTo this end, we introduce Shadow Computing to address the above trade-off challenge. The basic tenet of Shadow Computing is to associate with each main process a suite of \"shadows\" whose sizes depend on the \"criticality\" of the application and its performance requirements. Similar to traditional process replication, Shadow Computing ensures successful task completion by concurrently running multiple instances (processes) of the same task. Contrary to the traditional approach, however, Shadow Computing executes the main process of the task at the speed required to maximize profit and slows down the execution of the shadow processes to save energy. Slowing down the shadows can be achieved by either reducing the voltage/frequency of the processor or by co-locating multiple shadows on the same process. Adjusting the speed of execution enables a parameterized trade-off between response time, energy consumption and hardware redundancy. This allows for the optimization of throughput in case of HPC, and the maximization of the expected profit by accounting for income, potential penalties and energy cost, in the case of Cloud Computing.\n\nResults from sensitivity study using both analytical models and simulation showed that Shadow Computing can achieve significant energy savings (up to 30%) and profit gains (up to 19%) compared to traditional process replication and checkpointing-restart, without violating the resilience or SLA constraints. An initial implementation of Shadow Computing in an MPI environment showd the feasibility of the concept and its potential for increasing resilience at a reasonable overhead cost.\n\n\t\t\t\t\tLast Modified: 01/15/2016\n\n\t\t\t\t\tSubmitted by: Rami Melhem"
 }
}