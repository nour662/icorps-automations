{
 "awd_id": "1208623",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI-Small: Multi-modal sensor skin and garments for healthcare and home robots",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2017-05-31",
 "tot_intn_awd_amt": 1349766.0,
 "awd_amount": 1349766.0,
 "awd_min_amd_letter_date": "2012-09-04",
 "awd_max_amd_letter_date": "2013-11-07",
 "awd_abstract_narration": "The objective of this research is to answer fundamental design questions for multi-functional robotic skin sensors, optimize their placement onto assistive robotic devices, have the robot and human \"learn\" how to use the skin sensors efficiently, and quantitatively assess the impact of this assistive technology to humans. The approach is to design and fabricate integrated micro-scale sensors in conjunction with iterative simulation and experimental studies of the performance of physical human-robot interaction enabled by this technology. \r\nIntellectual Merit\r\nThis project will contribute efficient algorithms for optimal placement and data networking of distributed skin sensors on robots; new learning and control algorithms to sense human intent and improve interactivity; practical robotic skin and garment hardware with distributed sensors to include tactile, thermal imaging, and acceleration sensing in flexible materials that can be easily attached on and peeled off robots; and new metrics to evaluate the impact of this skin to humans including level of assistance, safety, ease of use, aesthetics, and therapeutic benefits.\r\nBroader Impacts\r\nCo-robots of the future will share their living spaces with humans, and, like people, will wear sensor skins and clothing that must be interconnected, fitted, cleaned, repaired, and replaced. In addition to aesthetic purposes that increase societal acceptance, these sensorized garments will also enhance robot perception of the environment, and enable extraordinary levels of safety, cooperation, and therapy for humans. The research proposed here will unlock near-term and also unforeseen applications of robotic skin with broad applicability, and especially to home assistance, medical rehabilitation, and prosthetics.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dan",
   "pi_last_name": "Popa",
   "pi_mid_init": "O",
   "pi_sufx_name": "",
   "pi_full_name": "Dan O Popa",
   "pi_email_addr": "dan.popa@louisville.edu",
   "nsf_id": "000085594",
   "pi_start_date": "2012-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Lewis",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Frank L Lewis",
   "pi_email_addr": "lewis@uta.edu",
   "nsf_id": "000257326",
   "pi_start_date": "2012-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Donald",
   "pi_last_name": "Butler",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Donald P Butler",
   "pi_email_addr": "dbutler@uta.edu",
   "nsf_id": "000177367",
   "pi_start_date": "2012-09-04",
   "pi_end_date": "2013-11-07"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Zeynep",
   "pi_last_name": "Celik-Butler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zeynep Celik-Butler",
   "pi_email_addr": "zbutler@uta.edu",
   "nsf_id": "000186658",
   "pi_start_date": "2012-09-04",
   "pi_end_date": "2013-11-07"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Woo Ho",
   "pi_last_name": "Lee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Woo Ho Lee",
   "pi_email_addr": "wooholee@icloud.com",
   "nsf_id": "000169202",
   "pi_start_date": "2013-11-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nicoleta",
   "pi_last_name": "Bugnariu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nicoleta Bugnariu",
   "pi_email_addr": "nicoleta.bugnariu@unthsc.edu",
   "nsf_id": "000572755",
   "pi_start_date": "2012-09-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Muthu",
   "pi_last_name": "Wijesundara",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Muthu B Wijesundara",
   "pi_email_addr": "muthuw@uta.edu",
   "nsf_id": "000655522",
   "pi_start_date": "2013-11-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Arlington",
  "inst_street_address": "701 S NEDDERMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "ARLINGTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "8172722105",
  "inst_zip_code": "760199800",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT ARLINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "LMLUKUPJJ9N3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Arlington",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "760190016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 1349766.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, a team of investigators from University of Texas at Arlington (UTA) and the University of North Texas Science Center (UNTHSC) collaborated to research topics related to the design, fabrication, integration and testing of multi-modal skin sensors placed on the body and limbs of collaborative robots for the home, on upper limb prosthetic devices, and on rehabilitation hardware for stroke patients. During the project we fabricated and evaluated numerous robot skin sensor prototypes containing pressure, temperature, and inertial sensors in our three leading applications.&nbsp;</p>\n<p>Pressure sensor arrays were embedded in polymer materials including Silicone rubbers, Frubber and embedded into a molder prosthetic adapter socket and into a flexible garment (a glove). Sensors were fabricated using clean room microtechnologies, as well as novel 3D printing methods. Four generations of sensor arrays were experimentally tested in our lab, with results showing appropriate performance.</p>\n<p>Larger arrays of up to 128 tactels were embedded onto a \"Can\" and \"Box\" end-effector that a robot and a person can hold together. We studied design tradeoffs of spatial resolution, size, and number of tactels to the performance of physical human - robot interaction using both custom made computer simulation environments (SkinSim), and experimentation with a specially constructed skin testing station in our labs.</p>\n<p>Other sensor modalities for interaction were studied, including temperature (IR) sensors, accelerometer/gyroscope arrays, electromyography, nail pressure sensors, and other video-based data acquisition systems.</p>\n<p>Among the findings of our research are important observations to guide the design and optimal placement of while body skin sensors on future robots and prosthetic devices:</p>\n<p>1) Dense tactile sensor arrays below millimeter sizes are not needed unless placed on surfaces with small curvature radii. Denser sensor arrays will improve the center of pressure estimate during interaction, but will degrade the interaction control performance.&nbsp;</p>\n<p>2) Dense accelerometer/gyro arrays are not very useful for estimating robot pose. More than 2 accelerometers per link provide only incremental information returns.&nbsp;</p>\n<p>3) Adding force sensors co-located with electromyographic (EMG) sensors will improve the utilization of robotic prostheses.</p>\n<p>4) Sensorized garments such as gloves or sensor arrays placed on a healthy limb of an amputee, can be effectively used to mirror or train a robotic prosthetic limb in a more natural and intuitive manner compared to the state of art.</p>\n<p>5) Human gaze and hand video-based measurements can be used to anticipate the interaction intent of humans just before coming into physical contact with a robot.</p>\n<p>Durign this project we also worked on using the sensor information to control robotic devices such as home robots and a commercial prosthetic hand. A novel learning control algorithm inspired by nature was formulated to control the interaction between people and robots. Our neuroadaptive controller (NAC) has the following properties:</p>\n<p>1) Can adapt to wide range of robots, human users, and robot skin sensors. This means that accurate models of the robot, human and robot skins are not needed for the NAC to perform as prescribed. Superior learning properties make the NAC more intuitive to use and more robust against hardware changes/user preference changes over time.</p>\n<p>2) The NAC is safer than convenitonal schemes due to stability guarantees we can offer during interaction. Unlike many other learning controllers, we can guarantee that the robot behavior during interaction will be stable and predictable within known error bounds.</p>\n<p>3) The NAC is relatively simple to implement and run on modern computer platforms deployed on robots. A public ROS code repository, SkinLearn was made available at the end of our grant for the community.</p>\n<p>4) Results showed that the NAC generates smaller interaction forces, jerk patterns, and trajectory errors than conventional algorithms. Conclusions were drawn after experiments with the PR-2 for programming by demonstration and stroke rehabilitation exercises.</p>\n<p>During the last two years of the grant we combined the poweful neuroadaptive controller (NAC) with arrays of tactile sensors to show that calibration of sensors can be avoided prior to deployment on a robot. This is a very important finding of our project, pointing to future advancements in which the measurements of relatively inexpensive, nonlinear, imprecise and degrading skin sensor arrays can be learned on the fly by robots during operation. This observation will be invaluable in lowering the cost of fabrication, deployment, calibration, and maintance of future skin sensors.</p>\n<p>Broader impact: this project resulted in 5 Ph.D. and 7 M.S. theses, and funded the research of dozens of undergradate engineering students and physical therapy graduate students. The grant resulted in numerous publications - over 20 journal papers, 3 patents pending and 1 patent granted. Two code repositories (SkinSim and SkinLearn) have been shared with the robotics community. A start-up company was spun off the PI's lab to commercialize robot skins for a variety of industries, including the robotics and automation, medical, and the consummer electronics markets.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/29/2017<br>\n\t\t\t\t\tModified by: Dan&nbsp;O&nbsp;Popa</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965189049_20161208_170110--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965189049_20161208_170110--rgov-800width.jpg\" title=\"Pressure sensors\"><img src=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965189049_20161208_170110--rgov-66x44.jpg\" alt=\"Pressure sensors\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Pressure sensors fabricated in the cleanroom on Kapton flexible sheets. Image shows three 11x11 tactel arrays with 1mm spacing.</div>\n<div class=\"imageCredit\">Joshua Baptist</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa</div>\n<div class=\"imageTitle\">Pressure sensors</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965356753_DSCN9995--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965356753_DSCN9995--rgov-800width.jpg\" title=\"4x4 SkinCell\"><img src=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965356753_DSCN9995--rgov-66x44.jpg\" alt=\"4x4 SkinCell\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">SkinCell - a 4x4 Array of Tactile sensors, interconnected to sensor electronics, and tested using a custom made controlled pressure loading instrument.</div>\n<div class=\"imageCredit\">M. Nasser Saadatzi</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa</div>\n<div class=\"imageTitle\">4x4 SkinCell</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965652170_healthy_limb--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965652170_healthy_limb--rgov-800width.jpg\" title=\"Healthy Limb Adapter\"><img src=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503965652170_healthy_limb--rgov-66x44.jpg\" alt=\"Healthy Limb Adapter\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Healthy Limb Adapter, created to interface the hand of a healthy user to a multi-fingered prosthetic robotic hand. The adapter can accelerate the test more intuitive interfaces for prosthetic users such as a sensorized gloves, or robot skin sensors.</div>\n<div class=\"imageCredit\">Oguz Yetkin</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa</div>\n<div class=\"imageTitle\">Healthy Limb Adapter</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503966066328_youbot--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503966066328_youbot--rgov-800width.jpg\" title=\"Sensorized Youbot End-Effector\"><img src=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503966066328_youbot--rgov-66x44.jpg\" alt=\"Sensorized Youbot End-Effector\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The Kuka Youbot end effector was sensorized with tactile patches and infrared temperature sensors for human-robot interaction experiments. Results show that robot guidance by direct physical touch is more precise and intuitive than teleoperation.</div>\n<div class=\"imageCredit\">NGS Lab</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa</div>\n<div class=\"imageTitle\">Sensorized Youbot End-Effector</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503967040552_phri-w-pr2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503967040552_phri-w-pr2--rgov-800width.jpg\" title=\"Neuroadaptive Control with PR-2\"><img src=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1503967040552_phri-w-pr2--rgov-66x44.jpg\" alt=\"Neuroadaptive Control with PR-2\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Neuroadaptive Controller is being tested with the PR-2 by UT Arlington Graduate Student Sven Cremer. The physical interaction is made possible by tactile sensors or the PR-2 force-torque sensor.</div>\n<div class=\"imageCredit\">Sven Cremer</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa</div>\n<div class=\"imageTitle\">Neuroadaptive Control with PR-2</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1504012908961_devices--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1504012908961_devices--rgov-800width.jpg\" title=\"Devices Prototyped\"><img src=\"/por/images/Reports/POR/2017/1208623/1208623_10210527_1504012908961_devices--rgov-66x44.jpg\" alt=\"Devices Prototyped\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image contains pictures of several iterations of sensorized gloves and healthy limb adapter for the prosthetic experiments conducted in this project, a tactile can (OctoCan) and tactile box prototyped as accessories for the PR-2 and Kuka Youbot robots.</div>\n<div class=\"imageCredit\">NGS Lab</div>\n<div class=\"imageSubmitted\">Dan&nbsp;O&nbsp;Popa</div>\n<div class=\"imageTitle\">Devices Prototyped</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn this project, a team of investigators from University of Texas at Arlington (UTA) and the University of North Texas Science Center (UNTHSC) collaborated to research topics related to the design, fabrication, integration and testing of multi-modal skin sensors placed on the body and limbs of collaborative robots for the home, on upper limb prosthetic devices, and on rehabilitation hardware for stroke patients. During the project we fabricated and evaluated numerous robot skin sensor prototypes containing pressure, temperature, and inertial sensors in our three leading applications. \n\nPressure sensor arrays were embedded in polymer materials including Silicone rubbers, Frubber and embedded into a molder prosthetic adapter socket and into a flexible garment (a glove). Sensors were fabricated using clean room microtechnologies, as well as novel 3D printing methods. Four generations of sensor arrays were experimentally tested in our lab, with results showing appropriate performance.\n\nLarger arrays of up to 128 tactels were embedded onto a \"Can\" and \"Box\" end-effector that a robot and a person can hold together. We studied design tradeoffs of spatial resolution, size, and number of tactels to the performance of physical human - robot interaction using both custom made computer simulation environments (SkinSim), and experimentation with a specially constructed skin testing station in our labs.\n\nOther sensor modalities for interaction were studied, including temperature (IR) sensors, accelerometer/gyroscope arrays, electromyography, nail pressure sensors, and other video-based data acquisition systems.\n\nAmong the findings of our research are important observations to guide the design and optimal placement of while body skin sensors on future robots and prosthetic devices:\n\n1) Dense tactile sensor arrays below millimeter sizes are not needed unless placed on surfaces with small curvature radii. Denser sensor arrays will improve the center of pressure estimate during interaction, but will degrade the interaction control performance. \n\n2) Dense accelerometer/gyro arrays are not very useful for estimating robot pose. More than 2 accelerometers per link provide only incremental information returns. \n\n3) Adding force sensors co-located with electromyographic (EMG) sensors will improve the utilization of robotic prostheses.\n\n4) Sensorized garments such as gloves or sensor arrays placed on a healthy limb of an amputee, can be effectively used to mirror or train a robotic prosthetic limb in a more natural and intuitive manner compared to the state of art.\n\n5) Human gaze and hand video-based measurements can be used to anticipate the interaction intent of humans just before coming into physical contact with a robot.\n\nDurign this project we also worked on using the sensor information to control robotic devices such as home robots and a commercial prosthetic hand. A novel learning control algorithm inspired by nature was formulated to control the interaction between people and robots. Our neuroadaptive controller (NAC) has the following properties:\n\n1) Can adapt to wide range of robots, human users, and robot skin sensors. This means that accurate models of the robot, human and robot skins are not needed for the NAC to perform as prescribed. Superior learning properties make the NAC more intuitive to use and more robust against hardware changes/user preference changes over time.\n\n2) The NAC is safer than convenitonal schemes due to stability guarantees we can offer during interaction. Unlike many other learning controllers, we can guarantee that the robot behavior during interaction will be stable and predictable within known error bounds.\n\n3) The NAC is relatively simple to implement and run on modern computer platforms deployed on robots. A public ROS code repository, SkinLearn was made available at the end of our grant for the community.\n\n4) Results showed that the NAC generates smaller interaction forces, jerk patterns, and trajectory errors than conventional algorithms. Conclusions were drawn after experiments with the PR-2 for programming by demonstration and stroke rehabilitation exercises.\n\nDuring the last two years of the grant we combined the poweful neuroadaptive controller (NAC) with arrays of tactile sensors to show that calibration of sensors can be avoided prior to deployment on a robot. This is a very important finding of our project, pointing to future advancements in which the measurements of relatively inexpensive, nonlinear, imprecise and degrading skin sensor arrays can be learned on the fly by robots during operation. This observation will be invaluable in lowering the cost of fabrication, deployment, calibration, and maintance of future skin sensors.\n\nBroader impact: this project resulted in 5 Ph.D. and 7 M.S. theses, and funded the research of dozens of undergradate engineering students and physical therapy graduate students. The grant resulted in numerous publications - over 20 journal papers, 3 patents pending and 1 patent granted. Two code repositories (SkinSim and SkinLearn) have been shared with the robotics community. A start-up company was spun off the PI's lab to commercialize robot skins for a variety of industries, including the robotics and automation, medical, and the consummer electronics markets.\n\n\t\t\t\t\tLast Modified: 08/29/2017\n\n\t\t\t\t\tSubmitted by: Dan O Popa"
 }
}