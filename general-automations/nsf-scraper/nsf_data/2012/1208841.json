{
 "awd_id": "1208841",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Collaborative Research: Prior-free probabilistic inferential methods for \"large-p-small-n\" linear regression problems",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2012-06-15",
 "awd_exp_date": "2015-05-31",
 "tot_intn_awd_amt": 85000.0,
 "awd_amount": 85000.0,
 "awd_min_amd_letter_date": "2012-06-15",
 "awd_max_amd_letter_date": "2013-05-10",
 "awd_abstract_narration": "The investigators study prior-free probabilistic inference with \"large p, small n\" regression analysis.  This is made possible in the new framework of Inferential Models (IMs) proposed recently by the investigators.  Statistical results produced by IMs are probabilistic and have desirable frequency properties.  In this study, the investigators develop IM-based methods for linear and certain non-linear regression analysis.  A sequence of topics in the context of large-p-small-n regression to be investigated include (1) variable selection in Gaussian regression models; (2) robust Student-t regression; and (3) binary regression models.\r\n\r\nLinear regression is one of the most commonly used methodologies in statistical applications.  However, desirable prior-free and frequency-calibrated probabilistic inference, particularly in the important variable selection context, has not been available until the recent development of IMs.  The IM framework provides a new and promising alternative to the well-known Bayesian and frequentist methods for various high-dimensional problems researchers currently face.  In this study, the investigators develop new statistical methods and computing software, generating useful tools for applied statisticians and scientists who are challenged by very-high-dimensional data in carrying out regression analysis.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Chuanhai",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Chuanhai Liu",
   "pi_email_addr": "chuanhai@purdue.edu",
   "nsf_id": "000096911",
   "pi_start_date": "2012-06-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "250 N. Univeristy St.",
  "perf_city_name": "West Lafayette",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479072066",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 40199.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 44801.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Statistical inference is an essential component of the scientific process but, arguably, the subject is lacking a solid foundation. As we proceed into the 21st century, data sets are becoming larger and problems are becoming more complicated, so we should not rely on the understanding of the 20th century. The main goal of this project was to develop a solid foundation of statistics based on a new perspective. With this new foundation, the PIs believe that that the field of statistics will be prepared to handle the challenges of the current \"big data\" era and beyond.</p>\n<p>The challenge of the statistical inference problem is that there is a probability model, depending on some parameter, that describes how data are generated, but reversing this process -- that is, working backward from the observed data to the unknown parameter -- has no unique solution. If reliable prior information is available, then the rules of probability can guide<br />this reversal. However, when no reliable prior information is available, as is often the case in real problems, then it is not clear how to proceed. One can introduce some \"default\" prior information and proceed with the reversal but, in this case, there is no guarantee that the result will be meaningful in any sense. So, the fundamental question, arguably the most important unsolved problem in statistics, is how to carry out this reversal in a meaningful way in the absence of prior information. This project provides a new answer to this important question.</p>\n<p>It is well known that the model that can be understood as to describe how data are generated can be represented in terms ofan auxiliary variable, a quantity that drives the randomization or uncertainty about the to-be-observed data. Existing approaches have attempted to use this representation and the information about the auxiliary variable to drive the above reversal, but without success. The new approach recognizes that the uncertainty in the reversal process cannot be described by the ordinary rules of probability. Instead, the PIs propose to use random sets to predict the unobserved value of the auxiliary variable, the result being a measure that assigns three categories -- \"true\", \"false\", \"don't know\" -- to each assertion about the unknown parameter, instead of the usual \"true\" or \"false\" categorization provided by probability. The PIs show that this third category is essential in order for the corresponding reversal to be valid in the sense that there is a meaningful scale on which to interpret the numerical values. No other frameworks can claim this validity property in general.</p>\n<p>This new framework -- called \"inferential models\" (IMs) -- provides some interesting new perspectives on the standard statistical theory. For example, from the perspective of accurately predicting the the unobserved auxiliary variable, it is clear that if some feature of that auxiliary variable is actually observed, which is the case in many standard problems, then there is no need to predict that feature and, from a particular application of this intuition, the PIs rediscover sufficient statistics. More generally, an operation of reducing the dimension of the auxiliary variable before prediction leads to a notion of efficiency and by defining and \"optimal\" or \"most efficient\" IM one can potentially remove the non-uniqueness of the representation of the data-generating model. These fundamental ideas are being used to develop new methodology in some interesting and important problems. Some of these are summarized in the PIs' book on IMs to be published in Fall 2015.</p>\n<p>Although significant progress has been made toward a new and solid foundation of statistical inference, there is still much more work to be done. In particular, challenging high-dimensional problems are currently out of reach. The PIs hope that the work done so far, supported by the NSF, communicated in journ...",
  "por_txt_cntn": "\nStatistical inference is an essential component of the scientific process but, arguably, the subject is lacking a solid foundation. As we proceed into the 21st century, data sets are becoming larger and problems are becoming more complicated, so we should not rely on the understanding of the 20th century. The main goal of this project was to develop a solid foundation of statistics based on a new perspective. With this new foundation, the PIs believe that that the field of statistics will be prepared to handle the challenges of the current \"big data\" era and beyond.\n\nThe challenge of the statistical inference problem is that there is a probability model, depending on some parameter, that describes how data are generated, but reversing this process -- that is, working backward from the observed data to the unknown parameter -- has no unique solution. If reliable prior information is available, then the rules of probability can guide\nthis reversal. However, when no reliable prior information is available, as is often the case in real problems, then it is not clear how to proceed. One can introduce some \"default\" prior information and proceed with the reversal but, in this case, there is no guarantee that the result will be meaningful in any sense. So, the fundamental question, arguably the most important unsolved problem in statistics, is how to carry out this reversal in a meaningful way in the absence of prior information. This project provides a new answer to this important question.\n\nIt is well known that the model that can be understood as to describe how data are generated can be represented in terms ofan auxiliary variable, a quantity that drives the randomization or uncertainty about the to-be-observed data. Existing approaches have attempted to use this representation and the information about the auxiliary variable to drive the above reversal, but without success. The new approach recognizes that the uncertainty in the reversal process cannot be described by the ordinary rules of probability. Instead, the PIs propose to use random sets to predict the unobserved value of the auxiliary variable, the result being a measure that assigns three categories -- \"true\", \"false\", \"don't know\" -- to each assertion about the unknown parameter, instead of the usual \"true\" or \"false\" categorization provided by probability. The PIs show that this third category is essential in order for the corresponding reversal to be valid in the sense that there is a meaningful scale on which to interpret the numerical values. No other frameworks can claim this validity property in general.\n\nThis new framework -- called \"inferential models\" (IMs) -- provides some interesting new perspectives on the standard statistical theory. For example, from the perspective of accurately predicting the the unobserved auxiliary variable, it is clear that if some feature of that auxiliary variable is actually observed, which is the case in many standard problems, then there is no need to predict that feature and, from a particular application of this intuition, the PIs rediscover sufficient statistics. More generally, an operation of reducing the dimension of the auxiliary variable before prediction leads to a notion of efficiency and by defining and \"optimal\" or \"most efficient\" IM one can potentially remove the non-uniqueness of the representation of the data-generating model. These fundamental ideas are being used to develop new methodology in some interesting and important problems. Some of these are summarized in the PIs' book on IMs to be published in Fall 2015.\n\nAlthough significant progress has been made toward a new and solid foundation of statistical inference, there is still much more work to be done. In particular, challenging high-dimensional problems are currently out of reach. The PIs hope that the work done so far, supported by the NSF, communicated in journal publications and the forthcoming book, and disseminated in conference and seminar presen..."
 }
}