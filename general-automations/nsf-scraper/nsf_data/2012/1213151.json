{
 "awd_id": "1213151",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "AF: Large: Collaborative Research: Exploiting Duality between Meta-Algorithms and Complexity",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2017-06-30",
 "tot_intn_awd_amt": 1250000.0,
 "awd_amount": 1250000.0,
 "awd_min_amd_letter_date": "2012-06-20",
 "awd_max_amd_letter_date": "2015-07-10",
 "awd_abstract_narration": "Meta-algorithms are algorithms that take  other algorithms as input.\r\nMeta-algorithms are important in a variety of applications, from\r\nminimizing circuits in VLSI to verifying hardware and software to\r\nmachine learning. Lower bound proofs show that computational problems\r\nare difficult in the sense of requiring a prohibitive\r\namount of time, memory, or other resource to solve.\r\nThis is particularly important in the context of cryptography,\r\nwhere it is vital to ensure that no feasible adversary can break\r\na code. Surprisingly, recent research by the PIs and others\r\nshows that designing meta-algorithms is, in a formal sense, \r\nequivalent to proving lower bounds. In other words, one can prove a \r\nnegative (the non-existence of a small circuit to solve a problem) by \r\na positive (devising a new meta-algorithm).  This was the key to a \r\nbreakthrough by PI Williams, proving lower bounds on constant \r\ndepth circuits with modular arithmetic gates.\r\n\r\nThe proposed research will utilize this connection both to\r\ndesign new meta-algorithms and to prove new lower bounds.\r\nA primary focus will be on meta-algorithms for\r\ndeciding if a given algorithm is 'trivial' or not, such as algorithms\r\nfor the Boolean satisfiability problem.  The proposed research will devise new\r\nalgorithms that improve over exhaustive search for many variants\r\nof satisfiability.  On the other hand, it will also explore\r\ncomplexity-theoretic limitations on how much improvement is\r\npossible, using reductions and lower bounds for restricted\r\nmodels. Satisfiability will provide a starting point for a more\r\ngeneral understanding of the exact complexities of other NP-complete\r\nproblems such as the traveling salesman problem and k-colorability.\r\nThe proposal addresses both worst-case performance and the use\r\nof fast algorithms as heuristics for solving this problem.\r\n\r\nThis exploration will be mainly mathematical.  However, when\r\nnew algorithms and heuristics are developed, they will be\r\nimplemented and the resulting software made widely available.\r\nThis research will be incorporated in courses taught by\r\nthe PI's, at both graduate and undergraduate levels.\r\nBoth graduate and undergraduate students will perform research\r\nas part of the project.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Russell",
   "pi_last_name": "Impagliazzo",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Russell G Impagliazzo",
   "pi_email_addr": "russell@cs.ucsd.edu",
   "nsf_id": "000110864",
   "pi_start_date": "2012-06-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Samuel",
   "pi_last_name": "Buss",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Samuel R Buss",
   "pi_email_addr": "sbuss@ucsd.edu",
   "nsf_id": "000259320",
   "pi_start_date": "2012-06-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ramamohan",
   "pi_last_name": "Paturi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ramamohan Paturi",
   "pi_email_addr": "paturi@cs.ucsd.edu",
   "nsf_id": "000210626",
   "pi_start_date": "2012-06-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "Department of Computer Science",
  "perf_city_name": "La Jolla",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930404",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 679264.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 279564.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 291172.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intellectual merit:</p>\n<p>Theoretical computer science concerns itsefl with the computational resources, such as number of operations or bits of memory, needed for a computer to solve a computational problem.&nbsp; Traditionally, this is divided into two sub-fields: algorithm design, seeking new ways of&nbsp; solving problems quickly or with few resources; and computational complexity, characterizing those problems that are ``hard'' in that they require large amount of resources.&nbsp; Our research utilizes pardoxical connections between these two areas to make progress on both.&nbsp; We utilize reasons why some problems are difficult to design new algorithms that solve other problems more efficiently, and utilize new algorithms for some problems to show others are difficult.&nbsp;</p>\n<p>As an example, we designed a new learning algorithm that, from random labelled examples of a function of a certain type (that can be expressed witha small number of&nbsp; layers of and, or and parity gates) , deduces how to compute the function.&nbsp; This solved a problem that had been open for twenty-five years, and won the Best Paper Award in CCC 16.&nbsp; This algorithm is completely different from any in the previous learning theory literature, because it combines two ideas from complexity theory:&nbsp; the circuit size lower bound of Smolensky and Razborov and the way to remove randomness from algorithms of Nisan and Wigderson.&nbsp;</p>\n<p>We also bridge another divide, showing connections between the difficulty of ``very hard'' problems that require exponential time and those that are relatively easy, but where algorithm designers are stil working to make further improvements.&nbsp;</p>\n<p>For example, the stable marriage problem was introduced by Gale (and was cited in his Nobel Prize for Economics as a major contribution).&nbsp; The problem is to match partners in&nbsp; a way that each prefers to be together than to split up.&nbsp; It is widely used for applications such as pairing medical residents with hospitals.&nbsp; It has a reasonably efficient algorithm, but if the input is described succinctly, one could hope to improve it.&nbsp; We examined this problem from a complexity-theoretic viewpoint.&nbsp; We obtained both new algorithms for many special cases of stable marriage, and hardness results, showing that improving algorithms for other cases would violate a widely-held conjecture about the complexity of NP-complete problems.</p>\n<p>We also made progress on understanding the complexity of hard problems, such as Satisfiability.&nbsp; New algorithms for hard problems were designed, many using circuit lower bound arguments.&nbsp; Somewhat paradoxically, satisfiability is both a known hard problem, and a problem where heuristic algorithms solve many typical instances.&nbsp; We used proof complexity to characterize both the power and limitations of different heuristic methods; for example, we gave provable examples where the new clause learning technique is superior to any implementation of the more old-fashioned Davis-Putnam style algorithms.&nbsp; We also showed the first examples where memory restrictions would cost a SAT-solver&nbsp; a huge amount of time, even if the memory is&nbsp; much larger&nbsp; than the input formula.</p>\n<p>Broader impact:</p>\n<p>The divide between algorithm design and computational complexity was social as well as technical.&nbsp; Since researchers in the two communities are not used to each other's ideas and approaches, and have mostly worked separately, technical results explaining the connections between the two areas do not automatically create collaborations between the fields.&nbsp;&nbsp; With this in mind, we organized a special semester at the Simons Insitute in Berkeley to bring together the two groups of researchers.&nbsp; This semester program was very successful, both in terms of the research that was performed during the semester (solving many open problems and launching many new directions), and&nbsp; in creating collaborations between algorithm designer s and complexity theorists.&nbsp; Moreover, many young researchers became involved and interested in this new approach.</p>\n<p>We also looked for ways to use theoretical ideas to improve undergraduate education.&nbsp; One way we did this is by directly involving undergraduates in our reseach, such as Cody Murray, now a Ph.D. student at MIT working with non-UCSD PI Wlliiams.&nbsp; We also used algorithmic problems that came up in our research as example problems in undergraduate courses.&nbsp; Finally, some of us were instrumental in creating programs to ease the transition into the computer science major for new&nbsp; undergraduates with little computing experience.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/28/2017<br>\n\t\t\t\t\tModified by: Ramamohan&nbsp;Paturi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIntellectual merit:\n\nTheoretical computer science concerns itsefl with the computational resources, such as number of operations or bits of memory, needed for a computer to solve a computational problem.  Traditionally, this is divided into two sub-fields: algorithm design, seeking new ways of  solving problems quickly or with few resources; and computational complexity, characterizing those problems that are ``hard'' in that they require large amount of resources.  Our research utilizes pardoxical connections between these two areas to make progress on both.  We utilize reasons why some problems are difficult to design new algorithms that solve other problems more efficiently, and utilize new algorithms for some problems to show others are difficult. \n\nAs an example, we designed a new learning algorithm that, from random labelled examples of a function of a certain type (that can be expressed witha small number of  layers of and, or and parity gates) , deduces how to compute the function.  This solved a problem that had been open for twenty-five years, and won the Best Paper Award in CCC 16.  This algorithm is completely different from any in the previous learning theory literature, because it combines two ideas from complexity theory:  the circuit size lower bound of Smolensky and Razborov and the way to remove randomness from algorithms of Nisan and Wigderson. \n\nWe also bridge another divide, showing connections between the difficulty of ``very hard'' problems that require exponential time and those that are relatively easy, but where algorithm designers are stil working to make further improvements. \n\nFor example, the stable marriage problem was introduced by Gale (and was cited in his Nobel Prize for Economics as a major contribution).  The problem is to match partners in  a way that each prefers to be together than to split up.  It is widely used for applications such as pairing medical residents with hospitals.  It has a reasonably efficient algorithm, but if the input is described succinctly, one could hope to improve it.  We examined this problem from a complexity-theoretic viewpoint.  We obtained both new algorithms for many special cases of stable marriage, and hardness results, showing that improving algorithms for other cases would violate a widely-held conjecture about the complexity of NP-complete problems.\n\nWe also made progress on understanding the complexity of hard problems, such as Satisfiability.  New algorithms for hard problems were designed, many using circuit lower bound arguments.  Somewhat paradoxically, satisfiability is both a known hard problem, and a problem where heuristic algorithms solve many typical instances.  We used proof complexity to characterize both the power and limitations of different heuristic methods; for example, we gave provable examples where the new clause learning technique is superior to any implementation of the more old-fashioned Davis-Putnam style algorithms.  We also showed the first examples where memory restrictions would cost a SAT-solver  a huge amount of time, even if the memory is  much larger  than the input formula.\n\nBroader impact:\n\nThe divide between algorithm design and computational complexity was social as well as technical.  Since researchers in the two communities are not used to each other's ideas and approaches, and have mostly worked separately, technical results explaining the connections between the two areas do not automatically create collaborations between the fields.   With this in mind, we organized a special semester at the Simons Insitute in Berkeley to bring together the two groups of researchers.  This semester program was very successful, both in terms of the research that was performed during the semester (solving many open problems and launching many new directions), and  in creating collaborations between algorithm designer s and complexity theorists.  Moreover, many young researchers became involved and interested in this new approach.\n\nWe also looked for ways to use theoretical ideas to improve undergraduate education.  One way we did this is by directly involving undergraduates in our reseach, such as Cody Murray, now a Ph.D. student at MIT working with non-UCSD PI Wlliiams.  We also used algorithmic problems that came up in our research as example problems in undergraduate courses.  Finally, some of us were instrumental in creating programs to ease the transition into the computer science major for new  undergraduates with little computing experience.\n\n\t\t\t\t\tLast Modified: 11/28/2017\n\n\t\t\t\t\tSubmitted by: Ramamohan Paturi"
 }
}