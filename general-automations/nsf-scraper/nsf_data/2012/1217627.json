{
 "awd_id": "1217627",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC: Small: Understanding, Sensing, and Accommodating Situational Impairments in Mobile Computing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 499722.0,
 "awd_amount": 499722.0,
 "awd_min_amd_letter_date": "2012-08-27",
 "awd_max_amd_letter_date": "2013-08-06",
 "awd_abstract_narration": "A \"computer user\" can no longer be thought of only as a person sitting at a desk in a consistent and comfortable working environment. Today's typical computer user is now holding a mobile device smaller than his or her hand, is outdoors, under the sun or in the rain, and perhaps even in motion, such as walking or riding. However, mobile devices have no deep awareness of the environments in which they are being used, or how those environments will affect their users' abilities to act. Addressing this challenge is the central idea of this project. The approach is to better understand, through scientific means, and better accommodate, through clever sensing and design, the \"situational impairments\" that affect the new-typical computer user for our age, the mobile user. Although situational impairments have been noted in the past, few research efforts have looked at how to sense impairing effects and what to do about them. Innovations in sensors, inference, and user interfaces have the potential to improve mobile interaction in the presence of situational impairments, showing that accessibility is \"for everyone.\"\r\n  \r\nThe intellectual merits of this work include: (a) scientific studies of some situationally-impairing factors on human performance; (b) the invention and evaluation of ten projects designed to address and reduce the negative effects of certain situational impairments; (c) the development of clever reusable sensing techniques to enable the creation of those projects and advance the capabilities of mobile devices; and (d) the study of the \"crossover potential\" of projects to people with physical or health-induced impairments and disabilities.\r\n  \r\nThe broader impacts of this work include: (a) pushing the capabilities of mobile devices and sensing technologies to become more useful and usable, especially in varied contexts, which may have relevance to mobile field workers; (b) concretely demonstrating that accessibility research benefits everyone, and that all people incur impairments of one form or another in their lives; (c) contributing to public health and safety by reducing the dangers situational impairments cause, particularly to and from people driving automobiles, and (d) creating a graduate course, an undergraduate workshop, and a grades 6-12 science unit on ability-based design for mobile computing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jacob",
   "pi_last_name": "Wobbrock",
   "pi_mid_init": "O",
   "pi_sufx_name": "",
   "pi_full_name": "Jacob O Wobbrock",
   "pi_email_addr": "wobbrock@uw.edu",
   "nsf_id": "000433583",
   "pi_start_date": "2012-08-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shwetak",
   "pi_last_name": "Patel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shwetak Patel",
   "pi_email_addr": "shwetak@cs.washington.edu",
   "nsf_id": "000352562",
   "pi_start_date": "2012-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981051016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 166496.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 333226.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This grant, entitled \"Understanding, Sensing, and Accommodating Situational Impairments in Mobile Computing,\" was about making mobile devices such as smartphones even smarter than they are today. Today's devices are used while people are \"on the go\" but they have little idea of how they are being used. The typical computer user from decades ago was at a desk in a quiet setting and controlled environment. But today's typical computer user is out in the world, walking with his or her smartphone, and subject to any number of what we call \"situational impairments.\" That term means that people are <em>impaired</em>&nbsp;when using devices on the go. Prior research has shown, for example, that users only attend to the screen for about 4-seconds on average before having to look away. That's one kind of situational impairment, namely distraction. Others include cold or wet weather, encumbering baggage, darkness, glare, ambient noise, one-handed use, bodily vibration due to walking or riding, and many other factors. This grant was about making devices smarter so they can be easier to use amidst such situationally-impairing factors.</p>\n<p>The results of this grant were the development of six (6) new systems that invented new \"smarts\" for mobile devices like smartphones. These new smarts include new abilities to sense surroundings and people's behavior, and new abilities to respond so that people can more easily use their devices. Here is a list of these six (6) new inventions and what they mean. Note that all of the sensing achieved here is from already on-device commodity sensors, making it possible for today's devices to employ these techniques:</p>\n<p>WalkType, a technology that uses the accelerometers on a smartphone to detect the user is walking and make typing on the touch screen keyboard more accurate;</p>\n<p>GripSense, a technology that uses accelerometers, a gyroscope, the vibration motor, and the touch screen to detect how a user is holding a device, and enabling pressure-sensing without an explicit pressure sensor;</p>\n<p>ContextType, which combines aspects of WalkType and GripSense to make touch screen typing more accurate based on how a user is holding a device and which fingers they are using;</p>\n<p>Tongue-in-Cheek, a technology that uses non-invasive wireless X-band Doppler radar to sense tongue gestures in the mouth;</p>\n<p>SwitchBack, a technology that uses a smartphone's front-facing camera to detect where the eyes are looking at the screen, and highlight the place a user was last looking when he returns his gaze to the screen after looking away;</p>\n<p>Smart Touch, a technology that makes touch screens of all kinds more accurate, including for people with disabilities, by matching their touches to previously seen touches no matter how \"messy\" those touches might be.</p>\n<p>Taken together, the above technologies and techniques embody a great deal of new know-how and invention that can inform technologists and designers of how to make our devices smarter, more aware, more usable, and even safer to use while moving through our complex, dynamic world.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/05/2016<br>\n\t\t\t\t\tModified by: Jacob&nbsp;O&nbsp;Wobbrock</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis grant, entitled \"Understanding, Sensing, and Accommodating Situational Impairments in Mobile Computing,\" was about making mobile devices such as smartphones even smarter than they are today. Today's devices are used while people are \"on the go\" but they have little idea of how they are being used. The typical computer user from decades ago was at a desk in a quiet setting and controlled environment. But today's typical computer user is out in the world, walking with his or her smartphone, and subject to any number of what we call \"situational impairments.\" That term means that people are impaired when using devices on the go. Prior research has shown, for example, that users only attend to the screen for about 4-seconds on average before having to look away. That's one kind of situational impairment, namely distraction. Others include cold or wet weather, encumbering baggage, darkness, glare, ambient noise, one-handed use, bodily vibration due to walking or riding, and many other factors. This grant was about making devices smarter so they can be easier to use amidst such situationally-impairing factors.\n\nThe results of this grant were the development of six (6) new systems that invented new \"smarts\" for mobile devices like smartphones. These new smarts include new abilities to sense surroundings and people's behavior, and new abilities to respond so that people can more easily use their devices. Here is a list of these six (6) new inventions and what they mean. Note that all of the sensing achieved here is from already on-device commodity sensors, making it possible for today's devices to employ these techniques:\n\nWalkType, a technology that uses the accelerometers on a smartphone to detect the user is walking and make typing on the touch screen keyboard more accurate;\n\nGripSense, a technology that uses accelerometers, a gyroscope, the vibration motor, and the touch screen to detect how a user is holding a device, and enabling pressure-sensing without an explicit pressure sensor;\n\nContextType, which combines aspects of WalkType and GripSense to make touch screen typing more accurate based on how a user is holding a device and which fingers they are using;\n\nTongue-in-Cheek, a technology that uses non-invasive wireless X-band Doppler radar to sense tongue gestures in the mouth;\n\nSwitchBack, a technology that uses a smartphone's front-facing camera to detect where the eyes are looking at the screen, and highlight the place a user was last looking when he returns his gaze to the screen after looking away;\n\nSmart Touch, a technology that makes touch screens of all kinds more accurate, including for people with disabilities, by matching their touches to previously seen touches no matter how \"messy\" those touches might be.\n\nTaken together, the above technologies and techniques embody a great deal of new know-how and invention that can inform technologists and designers of how to make our devices smarter, more aware, more usable, and even safer to use while moving through our complex, dynamic world.\n\n\t\t\t\t\tLast Modified: 12/05/2016\n\n\t\t\t\t\tSubmitted by: Jacob O Wobbrock"
 }
}