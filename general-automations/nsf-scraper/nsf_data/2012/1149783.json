{
 "awd_id": "1149783",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Visual Tracking with Online and Prior Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-01-01",
 "awd_exp_date": "2022-09-30",
 "tot_intn_awd_amt": 473798.0,
 "awd_amount": 492698.0,
 "awd_min_amd_letter_date": "2011-12-27",
 "awd_max_amd_letter_date": "2021-12-13",
 "awd_abstract_narration": "This project develops efficient and effective algorithms to handle challenging problems in visual tracking such as drift, heavy occlusion, and failure recovery. The research team is developing an integrated framework in which object detection, tracking and recognition are addressed simultaneously. Within this framework, the prior knowledge is learned from a large set of images pertaining to object classes of interest. Such knowledge serves as long-term memory for the proposed appearance models which are then adapted to unseen new object instances. In addition, a top-down saliency model for each object class of interest is developed in order to handle heavy occlusion and failure recovery. The project has four major components: developing algorithms for learning visual prior and transferring knowledge for online appearance model, designing tracking algorithms that handle draft with the proposed appearance model, modeling top-down saliency maps to handle full occlusion and tracking failure, evaluating state-of-the-art algorithms with a large benchmark dataset. \r\n\r\nThis project provides a building block for robust object tracking, which can be applied to motion analysis, surveillance, and multi-object tracking.  The developed top-down saliency map provides a flexible way to represent objects, which can be extended to object detection and segmentation.  The proposed tracking library and benchmark data set provide a platform for evaluation of advances in object tracking.  This research is integrated with education and outreach by courses and activities aimed at attracting students to this field and encouraging interdisciplinary collaborations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ming-Hsuan",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ming-Hsuan Yang",
   "pi_email_addr": "mhyang@ucmerced.edu",
   "nsf_id": "000508933",
   "pi_start_date": "2011-12-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California - Merced",
  "inst_street_address": "5200 N LAKE RD",
  "inst_street_address_2": "",
  "inst_city_name": "MERCED",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2092012039",
  "inst_zip_code": "953435001",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "CA13",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, MERCED",
  "org_prnt_uei_num": "",
  "org_uei_num": "FFM7VPAG8P92"
 },
 "perf_inst": {
  "perf_inst_name": "University of California - Merced",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "953435001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "CA13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "773100",
   "pgm_ele_name": "Other Global Learning & Trng"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "5950",
   "pgm_ref_txt": "SWITZERLAND"
  },
  {
   "pgm_ref_code": "5979",
   "pgm_ref_txt": "Europe and Eurasia"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 85604.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 99331.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 94753.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 99017.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 113993.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this project, we have developed visual tracking&nbsp;mehtods&nbsp;to&nbsp;advaance&nbsp;this field by addressing several&nbsp;impoarant&nbsp;problems:</p>\n<p><strong>Goal 1:</strong>&nbsp;How do we develop models and algorithms for training a detector specific to the target object&nbsp;online&nbsp;and use it for tracking?</p>\n<p><strong>Goal 2:</strong>&nbsp;As there always exist some ambiguities about the target location, how do we exploit adaptive appearance models to tackle drift problems? Furthermore, how do we handle occlusions and recover from tracking failures?</p>\n<p><strong>Goal 3:</strong>&nbsp;How do we learn generic visual prior for object tracking?</p>\n<p><strong>Goal 4:&nbsp;</strong>&nbsp;Given the learned visual prior, how can we transfer and leverage it with&nbsp;online&nbsp;learning for visual tracking?</p>\n<p><strong>Goal 5:&nbsp;</strong>&nbsp;How do we assess the advances of tracking algorithms using sound criteria and provide</p>\n<p class=\"p1\">a code library with benchmarks?</p>\n<p>We have developed a number of methods to deal with these problems.&nbsp;</p>\n<p><strong>Goal 1</strong></p>\n<p>We have developed one method that exploits how to draw and classify samples around the target object for effective visual tracking (VITAL: Visual Tracking via Adversarial Learning in&nbsp;CVPR&nbsp;2018).</p>\n<p>Another approach we develop is to analyze he spatial information of deep features and propose two complementary regressions for robust visual tracking. This paper (Learning Spatial-Aware Regressions for Visual Tracking) was presented in&nbsp;CVPR&nbsp;2018.</p>\n<p>In 2018, we developed one method that exploits how to extract target-aware features for effective visual tracking (Target-Aware Deep Tracking in&nbsp;CVPR&nbsp;2019).</p>\n<p>In 2020,&nbsp;we extended our method that uses reciprocal learning (NeurIPS&nbsp;2018) for more results in visual tracking and published one paper entitled &ldquo;Learning Recurrent Memory Activation Networks for Visual Tracking&rdquo; in IEEE Transactions on Image Processing (accepted in 2020).</p>\n<p>In 2021, we developed one method for 3D human tracking (&ldquo;Self-Attentive 3D Human Pose and Shape Estimation from Videos&rdquo;) and publish it in&nbsp;CVIU.</p>\n<p><strong>Goal 2</strong></p>\n<p>In 2016, we developed two methods that exploit hierarchical features and efficient classifiers for effective visual tracking. The developed algorithms are also effective in dealing with drift and heavy occlusion (goal 4).</p>\n<p>In 2017, we developed one method that exploits hierarchical features and residual learning for effective visual tracking (CREST:&nbsp;Convolutional&nbsp;residual learning for visual tracking in&nbsp;CVPR&nbsp;2017), and one algorithm based on correlation particle filters and deep features (Multi-task correlation particle filter for robust visual tracking in&nbsp;CVPR&nbsp;2017). The developed algorithms are also effective in dealing with drift and heavy occlusion (goal 4).</p>\n<p>In 2020, we exploited adaptive methods for object detection, and related tasks as tracking can be formulated within the tracking-by-detection paradigm. We published several papers on how to adapt learned representation for object detection, including &ldquo;Progressive Representation Adaptation for Weakly Supervised Object Localization&rdquo; in&nbsp;PAMI&nbsp;2020, &ldquo;Unsupervised Domain Adaptation for&nbsp;Spatio-Temporal Action Localization&rdquo; in&nbsp;BMVC&nbsp;2020; &ldquo;Every Pixel Matters: Center-Aware Feature Alignment for Domain Adaptive Object Detection&rdquo; in&nbsp;ECCV&nbsp;2020; and &ldquo;Progressive Domain Adaptation for Object Detection&rdquo; in&nbsp;WACV&nbsp;2020.</p>\n<p>In 2021, we developed a&nbsp;spatio-temporal representation learning method and publish one paper entitled &ldquo;Spatiotemporal&nbsp;Contrastive Video Representation Learning&rdquo; in&nbsp;CVPR&nbsp;2021.</p>\n<p><strong>Goal 3</strong></p>\n<p>In 2018, we addressed object tracking undergoing large appearance changes with end-to-end trainable visual attention maps. The findings are summarized in a&nbsp;NeurIPS&nbsp;2018 paper (Deep Attentive Tracking via Reciprocative Learning).&nbsp;</p>\n<p>In 2019, we analyzed attribute-specific features for visual tracking under different situations (illumination, occlusion, fast motion, etc.). This paper (Learning Attribute-Specific Representation for Visual Tracking) was presented in&nbsp;AAAI&nbsp;2019.</p>\n<p>In 2020, we further improved our prior work on detecting multiple people in complex scenes (ECCV&nbsp;2016) and published one paper entitled &ldquo;Tracking Persons-of-Interest via Unsupervised Representation Adaption&rdquo; in&nbsp;IJCV.</p>\n<p><strong>Goal 4</strong></p>\n<p>In 2017, we developed an effective method that exploits collaborative models for multi-object tracking in which targets appear in different camera views with frequent occlusions (Online&nbsp;multi-object tracking via robust collaborative model and sample selection in&nbsp;CVIU&nbsp;2017).</p>\n<p>In 2020, we developed a benchmark dataset and tracking method for multiple-object tracking entitled &ldquo;UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking&rdquo;&nbsp;CVIU&nbsp;2020.</p>\n<p><strong>Goal 5</strong></p>\n<p>In 2015, we developed a complete benchmark dataset with 100 sequences with labeled ground truth and larger scale performance evaluation (OTB-100 benchmark dataset). This has been made to the public for performance evaluation (http://cvlab.hanyang.ac.kr/tracker_benchmark/).</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/29/2023<br>\n\t\t\t\t\tModified by: Ming-Hsuan&nbsp;Yang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn this project, we have developed visual tracking mehtods to advaance this field by addressing several impoarant problems:\n\nGoal 1: How do we develop models and algorithms for training a detector specific to the target object online and use it for tracking?\n\nGoal 2: As there always exist some ambiguities about the target location, how do we exploit adaptive appearance models to tackle drift problems? Furthermore, how do we handle occlusions and recover from tracking failures?\n\nGoal 3: How do we learn generic visual prior for object tracking?\n\nGoal 4:  Given the learned visual prior, how can we transfer and leverage it with online learning for visual tracking?\n\nGoal 5:  How do we assess the advances of tracking algorithms using sound criteria and provide\na code library with benchmarks?\n\nWe have developed a number of methods to deal with these problems. \n\nGoal 1\n\nWe have developed one method that exploits how to draw and classify samples around the target object for effective visual tracking (VITAL: Visual Tracking via Adversarial Learning in CVPR 2018).\n\nAnother approach we develop is to analyze he spatial information of deep features and propose two complementary regressions for robust visual tracking. This paper (Learning Spatial-Aware Regressions for Visual Tracking) was presented in CVPR 2018.\n\nIn 2018, we developed one method that exploits how to extract target-aware features for effective visual tracking (Target-Aware Deep Tracking in CVPR 2019).\n\nIn 2020, we extended our method that uses reciprocal learning (NeurIPS 2018) for more results in visual tracking and published one paper entitled \"Learning Recurrent Memory Activation Networks for Visual Tracking\" in IEEE Transactions on Image Processing (accepted in 2020).\n\nIn 2021, we developed one method for 3D human tracking (\"Self-Attentive 3D Human Pose and Shape Estimation from Videos\") and publish it in CVIU.\n\nGoal 2\n\nIn 2016, we developed two methods that exploit hierarchical features and efficient classifiers for effective visual tracking. The developed algorithms are also effective in dealing with drift and heavy occlusion (goal 4).\n\nIn 2017, we developed one method that exploits hierarchical features and residual learning for effective visual tracking (CREST: Convolutional residual learning for visual tracking in CVPR 2017), and one algorithm based on correlation particle filters and deep features (Multi-task correlation particle filter for robust visual tracking in CVPR 2017). The developed algorithms are also effective in dealing with drift and heavy occlusion (goal 4).\n\nIn 2020, we exploited adaptive methods for object detection, and related tasks as tracking can be formulated within the tracking-by-detection paradigm. We published several papers on how to adapt learned representation for object detection, including \"Progressive Representation Adaptation for Weakly Supervised Object Localization\" in PAMI 2020, \"Unsupervised Domain Adaptation for Spatio-Temporal Action Localization\" in BMVC 2020; \"Every Pixel Matters: Center-Aware Feature Alignment for Domain Adaptive Object Detection\" in ECCV 2020; and \"Progressive Domain Adaptation for Object Detection\" in WACV 2020.\n\nIn 2021, we developed a spatio-temporal representation learning method and publish one paper entitled \"Spatiotemporal Contrastive Video Representation Learning\" in CVPR 2021.\n\nGoal 3\n\nIn 2018, we addressed object tracking undergoing large appearance changes with end-to-end trainable visual attention maps. The findings are summarized in a NeurIPS 2018 paper (Deep Attentive Tracking via Reciprocative Learning). \n\nIn 2019, we analyzed attribute-specific features for visual tracking under different situations (illumination, occlusion, fast motion, etc.). This paper (Learning Attribute-Specific Representation for Visual Tracking) was presented in AAAI 2019.\n\nIn 2020, we further improved our prior work on detecting multiple people in complex scenes (ECCV 2016) and published one paper entitled \"Tracking Persons-of-Interest via Unsupervised Representation Adaption\" in IJCV.\n\nGoal 4\n\nIn 2017, we developed an effective method that exploits collaborative models for multi-object tracking in which targets appear in different camera views with frequent occlusions (Online multi-object tracking via robust collaborative model and sample selection in CVIU 2017).\n\nIn 2020, we developed a benchmark dataset and tracking method for multiple-object tracking entitled \"UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking\" CVIU 2020.\n\nGoal 5\n\nIn 2015, we developed a complete benchmark dataset with 100 sequences with labeled ground truth and larger scale performance evaluation (OTB-100 benchmark dataset). This has been made to the public for performance evaluation (http://cvlab.hanyang.ac.kr/tracker_benchmark/).\n\n \n\n \n\n\t\t\t\t\tLast Modified: 01/29/2023\n\n\t\t\t\t\tSubmitted by: Ming-Hsuan Yang"
 }
}