{
 "awd_id": "1218931",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Non-parametric Approximate Dynamic Programming for Continuous Domains",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Weng-keen Wong",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 458000.0,
 "awd_min_amd_letter_date": "2012-07-24",
 "awd_max_amd_letter_date": "2017-07-20",
 "awd_abstract_narration": "This project concerns a machine learning technique known as reinforcement learning, which is related to, but distinct from, the notion of reinforcement learning used in psychology.  The common element is that both views study changes in behavior that result from experience.  In the machine learning case, the behaviors are often decision making in dynamic environments, such as controlling a robot, a factory, inventory levels for a warehouse or even drug dosage levels.   Current theoretical development in this area guarantees that optimal decisions can be made by reinforcement learning algorithms, but only under restrictive assumptions that are difficult to ensure in practice.  Efforts to apply reinforcement learning to significant practical problems have enjoyed some success, but such efforts often forgo theoretical guarantees and rely upon tedious parameter adjustments by experts (human trial and error) to achieve success.\r\n\r\nThis research seeks to reduce the amount of human trial and error needed to make reinforcement learning successful, thereby making it a more accessible tool to a wider range of people.  Specifically, it will focus on algorithms for domains described by continuous variables, seeking to provide stronger theoretical guarantees for such domains as well as an approach that balances the anticipated benefit of trying new things with the benefit of sticking to what is already known about a problem (exploration vs. exploitation).  A practical benefit of success in this area would be improved techniques that make it easier for people to deploy algorithms that learn and improve performance in a variety of practical tasks like those mentioned above: robot or factory control, inventory management, or drug delivery.\r\n\r\nThis project plans to use a model helicopter as a challenge domain, but it is not about helicopter control per se.   Rather, it seeks to develop general techniques that can apply to many problems, including helicopters, and will use model helicopters as an inexpensive and fun way to motivate students.  The project aims to develop a model helicopter simulator (to reduce the cost and risk of trying everything on an actual helicopter) and plans to make this simulator available to the research community, providing a fun and challenging benchmark problem.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ronald",
   "pi_last_name": "Parr",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ronald Parr",
   "pi_email_addr": "parr@cs.duke.edu",
   "nsf_id": "000188767",
   "pi_start_date": "2012-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "Duke University  2200 W. Main St",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 450000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project sought to develop new algorithms for control problems (simple robotic or factory automation tasks) in domains that are described by continuous variables and, potentially, controlled by continuous actions. This is distinct from classical reinforcement learning, which has often focused on domains described by discrete variables and discrete actions, though this project is not the only work focused on the continuous case.</p>\n<p>The type of algorithms explored in this project are referred to as, \"non-parametric\", which means that these algorithms store previous experiences and use these experiences to infer how to act in the future. This differs from the currently popular approach of deep neural networks, which use a fixed set of parameters determined in advance and then tunes these parameters during learning.</p>\n<p>The theoretical results obtained in this project show the data and computational efficiency of using non-parametric methods, as well as the potential practical benefit through some simulated domains. Also, new algorithms were developed to allow agents operating in the same or closely related domains to benefit from each other's experiences. The results provide conditions under which near-optimal performance can be guaranteed.</p>\n<p><span>A limitation of non-parametric approaches is that they often are, \"weak,\" <span>approximators</span>, which means that, unlike neural networks (which offer few or no guarantees) non-parametric methods cannot extrapolate very far beyond what they have seen. This makes them subject to the so-called curse of dimensionality, in which algorithms struggle to scale to complex problems described by many variables. This is because non-parametric methods are often forced to \"fill\" the space with samples to obtain meaningful guarantees of performance and the volume that must be filled grows exponentially with number of variables used to describe the problem.</span></p>\n<p>One way to mitigate (though not eliminate) the curse of dimensionality is to have a problem-specific notion of distance that helps calibrate the density of samples needed in a manner tailored to the problem. To this end, the project sought to develop algorithms for learning problem specific distance functions in a manner that is compatible with reinforcement learning. Preliminary results show that a new algorithm developed for this purpose may have promise in reducing the number of samples needed to obtain good performance.</p>\n<p>This project involved an undergraduate researcher and contributed to the training of three graduate students.</p>\n<p>Potential benefits from this line of research include the development of controllers with guaranteed performance on multidimensional control/automation problems.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/14/2018<br>\n\t\t\t\t\tModified by: Ronald&nbsp;Parr</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project sought to develop new algorithms for control problems (simple robotic or factory automation tasks) in domains that are described by continuous variables and, potentially, controlled by continuous actions. This is distinct from classical reinforcement learning, which has often focused on domains described by discrete variables and discrete actions, though this project is not the only work focused on the continuous case.\n\nThe type of algorithms explored in this project are referred to as, \"non-parametric\", which means that these algorithms store previous experiences and use these experiences to infer how to act in the future. This differs from the currently popular approach of deep neural networks, which use a fixed set of parameters determined in advance and then tunes these parameters during learning.\n\nThe theoretical results obtained in this project show the data and computational efficiency of using non-parametric methods, as well as the potential practical benefit through some simulated domains. Also, new algorithms were developed to allow agents operating in the same or closely related domains to benefit from each other's experiences. The results provide conditions under which near-optimal performance can be guaranteed.\n\nA limitation of non-parametric approaches is that they often are, \"weak,\" approximators, which means that, unlike neural networks (which offer few or no guarantees) non-parametric methods cannot extrapolate very far beyond what they have seen. This makes them subject to the so-called curse of dimensionality, in which algorithms struggle to scale to complex problems described by many variables. This is because non-parametric methods are often forced to \"fill\" the space with samples to obtain meaningful guarantees of performance and the volume that must be filled grows exponentially with number of variables used to describe the problem.\n\nOne way to mitigate (though not eliminate) the curse of dimensionality is to have a problem-specific notion of distance that helps calibrate the density of samples needed in a manner tailored to the problem. To this end, the project sought to develop algorithms for learning problem specific distance functions in a manner that is compatible with reinforcement learning. Preliminary results show that a new algorithm developed for this purpose may have promise in reducing the number of samples needed to obtain good performance.\n\nThis project involved an undergraduate researcher and contributed to the training of three graduate students.\n\nPotential benefits from this line of research include the development of controllers with guaranteed performance on multidimensional control/automation problems. \n\n\t\t\t\t\tLast Modified: 08/14/2018\n\n\t\t\t\t\tSubmitted by: Ronald Parr"
 }
}