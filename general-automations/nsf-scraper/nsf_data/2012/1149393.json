{
 "awd_id": "1149393",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Sensing the World with the Distributed Camera",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-03-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 498956.0,
 "awd_amount": 498956.0,
 "awd_min_amd_letter_date": "2012-02-29",
 "awd_max_amd_letter_date": "2012-02-29",
 "awd_abstract_narration": "We live in a world of ubiquitous imagery, where the number of images at our fingertips is growing at a seemingly exponential rate.  These images come from a wide variety of sources, including Internet mapping sites, webcams, surveillance and reconnaissance cameras, and millions of photographers around the world uploading billions and billions of images to photo-sharing websites.  Taken together, these sources of photos can be thought of as constituting a distributed camera capturing the entire world at unprecedented scale, and continually documenting its cities, mountains, buildings, people, and events.\r\n\r\nThis research is creating the basic computational tools for \"calibrating\" this distributed camera, through use of a world-wide database of 3D models built from Internet photo collections using computer vision techniques.  The focus is on creating faster, more robust algorithms for 3D reconstruction from unstructured photo collections, as well as techniques for world-scale pose estimation--computing precisely where in the world a photo was taken from image data alone.  These tools are yielding large, world-wide databases of calibrated imagery that can help answer questions in science (e.g., finding all available photos of Central Park so as to track flowering times of different plants) and engineering (e.g., finding all of the photos ever taken of a particular bridge to help figure out why it collapsed), and have impact other areas including security, consumer photography, and multimedia search.  This research is closely integrated with education and outreach, and includes plans for a summer workshop for high-school students to engage with 3D vision technologies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Snavely",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Noah Snavely",
   "pi_email_addr": "snavely@cs.cornell.edu",
   "nsf_id": "000533237",
   "pi_start_date": "2012-02-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "373 Pine Tree Rd.",
  "perf_city_name": "Ithaca",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "745300",
   "pgm_ele_name": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 498956.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-bf4c591e-7fff-e39f-4a47-cf5ce8c5031d\"> </span></p>\n<p dir=\"ltr\"><span>This National Science Foundation CAREER award supported research that drove major advances in the fields of computer vision and computer graphics, particularly in reconstructing 3D models from one or more 2D images, and in using these models to recognize the locations of new images. This funding </span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>helped train ten graduate and undergraduate students in vision, graphics, and machine learning research</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>supported research published in twenty publications that have collectively been cited over 1,000 times to date and used by researchers across vision, graphics, and learning</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>led to the creation of ten new vision and graphics datasets, and</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>supported the CURIE Academy during the summer of 2015, where our team engaged over 50 high school girls in a successful week-long STEM research activity (</span><a href=\"http://www.news.cornell.edu/stories/2015/08/curie-academy-fuels-girls-passion-engineering\"><span>Cornell Chronicle link</span></a><span>)</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Our research </span></p>\n<ul>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>created new algorithms for automatically reconstructing 3D models from large numbers of Internet photos in a fast and robust way ? for instance, see our work on </span><a href=\"http://www.cs.cornell.edu/projects/1dsfm\"><span>1DSfM</span></a><span> and </span><a href=\"http://www.cs.cornell.edu/projects/chronology/\"><span>Scene Chronology</span></a><span> summarized on our </span><a href=\"http://www.cs.cornell.edu/projects/bigsfm/\"><span>BigSFM website</span></a></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>devised new methods for reasoning about materials in images, by creating new datasets for material understanding (such as </span><a href=\"http://opensurfaces.cs.cornell.edu\"><span>OpenSurfaces</span></a><span>, </span><a href=\"http://intrinsic.cs.cornell.edu\"><span>Intrinsic Images in the Wild</span></a><span>, and </span><a href=\"http://opensurfaces.cs.cornell.edu/publications/minc/\"><span>Materials in Context</span></a><span>) and developing new approaches to material recognition</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>led to new approaches for location recognition ? determining where a photo is from image data alone (a kind of ?Visual GPS?) ? which have inspired advances in industry in camera tracking for virtual and augmented reality, and</span></p>\n</li>\n<li dir=\"ltr\">\n<p dir=\"ltr\"><span>resulted in new capabilities for reasoning about 3D shape from </span><em>single</em><span> 2D images using deep learning (see our </span><a href=\"http://www.cs.cornell.edu/projects/megadepth/\"><span>MegaDepth project</span></a><span>).</span></p>\n</li>\n</ul>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>We have published code and data for all of the work above, helping researchers and practitioners in many different areas benefit from and build on our work. Thanks to this NSF Award, we now understand much more about 3D reconstruction, location recognition, and material understanding, leading to better 3D models from images, and faster, more accurate ways to find the position of a camera using imagery alone. The impact of this work is also multiplied by the training received by the many students involved in the project, and by the young scholars inspired by the CURIE Academy workshop we led.</span></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/04/2019<br>\n\t\t\t\t\tModified by: Noah&nbsp;Snavely</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546619852308_cornell_models--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546619852308_cornell_models--rgov-800width.jpg\" title=\"3D reconstructions from 2D images\"><img src=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546619852308_cornell_models--rgov-66x44.jpg\" alt=\"3D reconstructions from 2D images\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">These images depict 3D reconstructions of various buildings automatically created from sets of 2D photos using our methods.</div>\n<div class=\"imageCredit\">Noah Snavely</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Noah&nbsp;Snavely</div>\n<div class=\"imageTitle\">3D reconstructions from 2D images</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620005796_rome_location_recognition--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620005796_rome_location_recognition--rgov-800width.jpg\" title=\"Visual GPS\"><img src=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620005796_rome_location_recognition--rgov-66x44.jpg\" alt=\"Visual GPS\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image illustrates our Visual GPS capability ? taking an input image (left) and automatically determining its position and orientation (right) without knowing anything in advance about its location. This capability is useful for virtual and augmented reality and a host of other applications.</div>\n<div class=\"imageCredit\">Noah Snavely</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Noah&nbsp;Snavely</div>\n<div class=\"imageTitle\">Visual GPS</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620132723_material_recognition--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620132723_material_recognition--rgov-800width.jpg\" title=\"Material Recognition\"><img src=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620132723_material_recognition--rgov-66x44.jpg\" alt=\"Material Recognition\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This image demonstrates our research on automatically understanding materials from images. Each input image (top) is automatically segmented into regions corresponding to different materials, such as brick, metal, and plastic. (See color key on the right.)</div>\n<div class=\"imageCredit\">Noah Snavely</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Noah&nbsp;Snavely</div>\n<div class=\"imageTitle\">Material Recognition</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620277742_megadepth--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620277742_megadepth--rgov-800width.jpg\" title=\"Predicting 3D from Single 2D Images\"><img src=\"/por/images/Reports/POR/2019/1149393/1149393_10155965_1546620277742_megadepth--rgov-66x44.jpg\" alt=\"Predicting 3D from Single 2D Images\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This figure illustrates our research on automatically inferring 3D depth from 2D images. The top row shows input photos, and the bottom row depicts corresponding depth maps automatically predicted for each photo using our deep learning approach (blue = closer, and red = further).</div>\n<div class=\"imageCredit\">Noah Snavely</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Noah&nbsp;Snavely</div>\n<div class=\"imageTitle\">Predicting 3D from Single 2D Images</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThis National Science Foundation CAREER award supported research that drove major advances in the fields of computer vision and computer graphics, particularly in reconstructing 3D models from one or more 2D images, and in using these models to recognize the locations of new images. This funding \n\n\nhelped train ten graduate and undergraduate students in vision, graphics, and machine learning research\n\n\nsupported research published in twenty publications that have collectively been cited over 1,000 times to date and used by researchers across vision, graphics, and learning\n\n\nled to the creation of ten new vision and graphics datasets, and\n\n\nsupported the CURIE Academy during the summer of 2015, where our team engaged over 50 high school girls in a successful week-long STEM research activity (Cornell Chronicle link)\n\n\n\n \nOur research \n\n\ncreated new algorithms for automatically reconstructing 3D models from large numbers of Internet photos in a fast and robust way ? for instance, see our work on 1DSfM and Scene Chronology summarized on our BigSFM website\n\n\ndevised new methods for reasoning about materials in images, by creating new datasets for material understanding (such as OpenSurfaces, Intrinsic Images in the Wild, and Materials in Context) and developing new approaches to material recognition\n\n\nled to new approaches for location recognition ? determining where a photo is from image data alone (a kind of ?Visual GPS?) ? which have inspired advances in industry in camera tracking for virtual and augmented reality, and\n\n\nresulted in new capabilities for reasoning about 3D shape from single 2D images using deep learning (see our MegaDepth project).\n\n\n\n \nWe have published code and data for all of the work above, helping researchers and practitioners in many different areas benefit from and build on our work. Thanks to this NSF Award, we now understand much more about 3D reconstruction, location recognition, and material understanding, leading to better 3D models from images, and faster, more accurate ways to find the position of a camera using imagery alone. The impact of this work is also multiplied by the training received by the many students involved in the project, and by the young scholars inspired by the CURIE Academy workshop we led.\n\n \n\n\t\t\t\t\tLast Modified: 01/04/2019\n\n\t\t\t\t\tSubmitted by: Noah Snavely"
 }
}