{
 "awd_id": "1245997",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: CC-NIE Integration: Transforming Computational Science with ADAMANT (Adaptive Data-Aware Multi-Domain Application Network Topologies)",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2013-01-01",
 "awd_exp_date": "2015-12-31",
 "tot_intn_awd_amt": 279976.0,
 "awd_amount": 279976.0,
 "awd_min_amd_letter_date": "2012-09-10",
 "awd_max_amd_letter_date": "2012-09-10",
 "awd_abstract_narration": "Workflows, especially data-driven workflows and workflow ensembles are becoming a centerpiece of modern computational science. However, scientists lack the tools that integrate the operation of workflow-driven science applications on top of dynamic infrastructures that link campus, institutional and national resources into connected arrangements targeted at solving a specific problem. These tools must (a) orchestrate the infrastructure in response to application demands, (b) manage application lifetime on top of the infrastructure by monitoring various workflow steps and modifying slices in response to application demands, and (c) integrate data movement with the workflows to optimize performance.\r\n \r\nProject ADAMANT (Adaptive Data-Aware Multi-domain Application Network Topologies) brings together researchers from RENCI/UNC Chapel Hill, Duke University and USC/ISI and two successful software tools to solve these problems: Pegasus workflow management system and ORCA resource control framework, developed for NSF GENI. The integration of Pegasus and ORCA enables powerful application- and data-driven virtual topology embedding into multiple institutional and national substrates (providers of cyber-resources, like computation, storage and networks). ADAMANT leverages ExoGENI - an NSF-funded GENI testbed, as well as national providers of on-demand bandwidth services (NLR, I2, ESnet) and existing OSG computational resources to create elastic, isolated environments to execute complex distributed tasks. This approach improves the performance of these applications and, by explicitly including data movement planning into the application workflow, enables new unique capabilities for distributed data-driven \"Big Science\" applications.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Chase",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey S Chase",
   "pi_email_addr": "chase@cs.duke.edu",
   "nsf_id": "000164438",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277080129",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 279976.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project brought together teams of computer scientists from RENCI/UNC Chapel Hill, USC/ISI and Duke University with the purpose of studying the feasibility of complex scientific computational workflows running on dynamically provisioned cloud-like widely distributed cyber-infrastructure. The distributed cyber-infrastructure included a collection of institutional clouds, which included computational and storage resources linked with on-demand network connections that can all be provisioned together using ORCA control software, developed at Duke and RENCI and deployed in ExoGENI testbed (http://www.exogeni.net). The workflows were managed by the Pegasus workflow management system (http://pegasus.isi.edu/) designed at ISI. The project team developed software elements to tie these two systems together with a performance feedback mechanism called ShadowQ and also provide an easy-to-use portal for scientists to submit and monitor their computational workflows. <br /><br />The main goal of the project was to prove that such a coupling between workflows and infrastructure was indeed possible, i.e. it was possible for a workflow to provision the necessary infrastructure for itself and manage it through the workflow execution process. The benefits of this approach include improved usability by domain scientists compared to existing institutional and grid resources as well better predictability and performance and security isolation, which speed up the discovery process and encourage more scientists to use existing resources. <br /><br />The project focused on two science domains - astronomy and high-throughput gene sequencing. The two domains were chosen because they rely heavily on workflow management systems and have widely differing computational requirements. The ADAMANT team demonstrated support for workflows from these domains at various venues including the SuperComputing (SC) conference. <br /><br />The astronomy application used was the Montage image mosaic engine from the NASA&rsquo;s Infrared Processing and Analysis Center. Montage contains a suite of tools to analyze, reproject, rectify, and co-add images taken by telescopes, such as the Spitzer.&nbsp; The steps in the workflow include reprojecting the input images, background correction, and adding the resulting pixels into the final output image. Telescopes takes a large number of images at high resolution, and the workflow can take these images, usually hundreds or thousands at the same time, and provide background corrected overview images to the astronomers. Processed images such as these are also commonly used by scientist outside the astronomy community, and the general public. The ADAMANT infrastructure benefits these workflows as the large amount of input data is usually hosted at some remote data service. By making the infrastructure and the workflow management system more aware of each other and each others&rsquo; requirements, smarter decisions can be made about how to best access this remote data, including creating virtual dedicated network paths from the data source to the workflow execution environment. <br /><br />The RENCI team has developed an informatics infrastructure to process and analyze full and exomic genomic sequencing data generated at UNC using a combination of cluster-based computing hardware and software. While successful in processing thousands of genomic samples, the work has exposed multiple challenges with the state-of-the-art in large-scale workflow processing that result from the complexity of the systems and software involved, the lack of cloud-based models for scaling in response to demand, and the challenges inherent in processing large-scale data sets that are distributed across institution. We believe these challenges will increase as data sets continue to grow in size, as the need to share data sets increases, and as security and privacy concerns around sharing of dat...",
  "por_txt_cntn": "\nThe project brought together teams of computer scientists from RENCI/UNC Chapel Hill, USC/ISI and Duke University with the purpose of studying the feasibility of complex scientific computational workflows running on dynamically provisioned cloud-like widely distributed cyber-infrastructure. The distributed cyber-infrastructure included a collection of institutional clouds, which included computational and storage resources linked with on-demand network connections that can all be provisioned together using ORCA control software, developed at Duke and RENCI and deployed in ExoGENI testbed (http://www.exogeni.net). The workflows were managed by the Pegasus workflow management system (http://pegasus.isi.edu/) designed at ISI. The project team developed software elements to tie these two systems together with a performance feedback mechanism called ShadowQ and also provide an easy-to-use portal for scientists to submit and monitor their computational workflows. \n\nThe main goal of the project was to prove that such a coupling between workflows and infrastructure was indeed possible, i.e. it was possible for a workflow to provision the necessary infrastructure for itself and manage it through the workflow execution process. The benefits of this approach include improved usability by domain scientists compared to existing institutional and grid resources as well better predictability and performance and security isolation, which speed up the discovery process and encourage more scientists to use existing resources. \n\nThe project focused on two science domains - astronomy and high-throughput gene sequencing. The two domains were chosen because they rely heavily on workflow management systems and have widely differing computational requirements. The ADAMANT team demonstrated support for workflows from these domains at various venues including the SuperComputing (SC) conference. \n\nThe astronomy application used was the Montage image mosaic engine from the NASA\u00c6s Infrared Processing and Analysis Center. Montage contains a suite of tools to analyze, reproject, rectify, and co-add images taken by telescopes, such as the Spitzer.  The steps in the workflow include reprojecting the input images, background correction, and adding the resulting pixels into the final output image. Telescopes takes a large number of images at high resolution, and the workflow can take these images, usually hundreds or thousands at the same time, and provide background corrected overview images to the astronomers. Processed images such as these are also commonly used by scientist outside the astronomy community, and the general public. The ADAMANT infrastructure benefits these workflows as the large amount of input data is usually hosted at some remote data service. By making the infrastructure and the workflow management system more aware of each other and each others\u00c6 requirements, smarter decisions can be made about how to best access this remote data, including creating virtual dedicated network paths from the data source to the workflow execution environment. \n\nThe RENCI team has developed an informatics infrastructure to process and analyze full and exomic genomic sequencing data generated at UNC using a combination of cluster-based computing hardware and software. While successful in processing thousands of genomic samples, the work has exposed multiple challenges with the state-of-the-art in large-scale workflow processing that result from the complexity of the systems and software involved, the lack of cloud-based models for scaling in response to demand, and the challenges inherent in processing large-scale data sets that are distributed across institution. We believe these challenges will increase as data sets continue to grow in size, as the need to share data sets increases, and as security and privacy concerns around sharing of data continue to increase. The project team successfully ported the genomic sequencing workflows to the ADAMANT infrastruc..."
 }
}