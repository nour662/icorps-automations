{
 "awd_id": "1217676",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Ontology based Perceptual Organization of Audio-Video Events using Pattern Theory",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 249843.0,
 "awd_amount": 257843.0,
 "awd_min_amd_letter_date": "2012-08-27",
 "awd_max_amd_letter_date": "2013-05-13",
 "awd_abstract_narration": "It is natural that events of interest in observed scenes manifest themselves across multiple sensing modalities - vision, hearing, smell, etc. The remarkable perceptions of audio-video signals by natural systems, such as humans, also points to superiority of inferences drawn across modalities. It, therefore, seems natural to enhance performance of automated systems by using joint, cross-modal statistical inferences. However, the detection, organization, and understanding of cues and events in real-world scenarios are difficult tasks. This project seeks to develop a pattern-theoretic framework for achieving these goals. The main research items are: (1) development of mathematical quantities to represent audio and visual events and their spatiotemporal relations, (2) use domain-specific ontologies to impose semantic structure and to incorporate prior knowledge, and (3) derive algorithms for Bayesian inferences using efficient adaptations of Markov Chain Monte Carlo sampling. \r\n\r\nThe use of pattern theory allows bridging of gaps between raw signals and high-level, domain-dependent semantics, and helps discovers large groups of audio-visual events likely to represent the same underlying event. This effort combines ideas from perceptual organization in computer vision, computational analysis of auditory signals, pattern theory, and prior developments in ontological structures. The methods developed here are applicable to many scenarios that deploy audio and video sensors, including problems of audio annotations of videos, speaker tracking in teleconferencing, and separation of multiple objects in remote surveillance. Broader impact activities involve the development of teaching modules, innovation and entrepreneurial training of the students, and communication of the findings to the community.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sudeep",
   "pi_last_name": "Sarkar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sudeep Sarkar",
   "pi_email_addr": "sarkar@usf.edu",
   "nsf_id": "000285699",
   "pi_start_date": "2012-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of South Florida",
  "inst_street_address": "4202 E FOWLER AVE",
  "inst_street_address_2": "",
  "inst_city_name": "TAMPA",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "8139742897",
  "inst_zip_code": "336205800",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "FL15",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTH FLORIDA",
  "org_prnt_uei_num": "",
  "org_uei_num": "NKAZLXLL7Z91"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Florida",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "336209951",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "FL15",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 249843.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This collaborative research project centered on applications of Grenander&rsquo;s General Pattern Theory to the representation of complex knowledge associated with human activities in video data.</p>\n<p><strong>Intellectual Merit</strong></p>\n<p>Successful automatic understanding of video content is essential for many computer vision-based applications. These applications focus on recognizing human interactions, which are typically understood as events that involve people performing actions with objects or other people. The main challenges are threefold. First, it is difficult to handle the enormous variety of interactions present in different instances of a complex event. The second problem involves rejecting clutter of extraneous objects so that only the participating ones are considered. The third problem stems from errors in classification of objects and actions. How do we leverage the success of deep-learning/machine learning based labeling methods into building a detailed semantic description of a complex event?&nbsp;We explored the&nbsp;combinatorial approach based on Grenander&rsquo;s pattern theory &ndash; the classic version. We have formulated a comprehensive and elegant formulation that can handle all these three kinds of challenges, without the need for an extensive training dataset.</p>\n<p>The main idea is to discover objects, actions, and their contextual elements, in video sequences and reach graph-theoretic representations of activities. These representations facilitate high-level tasks, such as characterizing, summarizing, and labeling of activities in video segments. Two broad research directions were developed:</p>\n<ol>\n<li><strong>Inferences on Human Activities from Videos and Sound</strong>: In pattern theory, the mathematical representation of data is a form of a configuration of basic entities, termed generators (or nodes), connected using bonds (or edges) under pre-defined rules. The optimality of a configuration is defined using a global posterior energy that has contributions from both data likelihood and prior knowledge. The choice of generators is dictated by the application context; in cooking videos one typically needs generators for representing objects (spoon, spatula, etc.), materials (bowl, egg, lettuce, etc.), actions (pickup, stir, put, etc.), that constitute a recipe. The presence of generators in a scene is based on spatiotemporal features extracted from video frames using latest learning techniques. The prior knowledge is incorporated in the form on co-occurrence frequencies of generators in the training data. Thus, the detected generators are selectively linked using bonds using a combination of prior, data support and spatiotemporal proximity. The resulting configurations denote interpretations of activities contained in the given video data. Advantages of this framework include incorporation of relevant knowledge to draw efficient scene interpretations, relative immunity to clutter and noise, and scalability to descriptions of long videos towards multiple, simultaneous activities.&nbsp; The framework is also easily generalization to include other sensory modalities such as sound.</li>\n<li><strong>Rate-Invariance Action Classification</strong>: This effort also focused on statistical analysis of activities in a manner that is invariant to the rate of performance. The basic approach is to extract characteristic features from individual frames, and then represent each observation of an activity as a time-indexed trajectory on a nonlinear feature space. This project developed novel invariant metrics that compare and quantify differences in such manifold paths, and simultaneously temporally register points across trajectories. These metrics contribute in generating statistical summaries and probabilistic classification of observed activities for performing computer vision tasks efficiently. This approach outperformed state-of-the-art results in the classification of human activities and action using 2D or 3D video and audio data.</li>\n</ol>\n<p>&nbsp;</p>\n<p><strong>Broader Impacts</strong></p>\n<p>At FSU, this project supported education and training of three Ph.D. students (including a female) in the general area of high-dimensional statistical data analysis, with placements in academia and industry. It also supported training of three undergraduate researchers during summer months. At USF, this project directly supported education and training of one Ph.D. student, and partially supported and contributed to the education of three other Ph.D. students. It also supported training of two undergraduate researchers.</p>\n<p>As research outcomes, this projected directly or partly resulted in 8 journal papers, 15+ conferences papers and several book chapters. PI Srivastava finished one graduate-level textbook on &ldquo;Functional and Shape Data Analysis&rdquo; and two edited volumes (&ldquo;3D Face Recognition&rdquo; and &ldquo;Riemannian Computing in Computer Vision&rdquo;) with partial support from this project. Current plans include the development of a textbook to disseminate further the knowledge developed in this project.</p>\n<p>PI Sarkar along with one of the Ph.D. students participated in the NSF I-Corps program to explore the commercial viability of the ideas developed in this project.</p>\n<p>The participants received several recognitions for this research, including the best paper award at ICPR 2014. They also presented a tutorial on this pattern-theoretic approach at a major conference. The PIs delivered a number of invited talks at colloquia, workshops, and working groups, to advance the use of pattern theory in applications beyond video data analysis and computer vision.</p>\n<p>As an outreach, the PIs also collaborated with researchers at two Federal agencies (NIST and NSWC), resulting in joint research efforts and publications.</p>\n<p><strong>&nbsp;</strong><em>&nbsp;</em></p>\n<p>&nbsp;</p>\n<p><span style=\"font-family: Cambria; font-size: small;\">&nbsp;</span></p>\n<p><strong>&nbsp;</strong><em>&nbsp;</em></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/17/2016<br>\n\t\t\t\t\tModified by: Sudeep&nbsp;Sarkar</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970823613_Figure4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970823613_Figure4--rgov-800width.jpg\" title=\"Semantic integration of video and sound informations\"><img src=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970823613_Figure4--rgov-66x44.jpg\" alt=\"Semantic integration of video and sound informations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Objects (nouns) and elementary actions (verbs) uncertain hypotheses generated from processing video and audio channels are integrated using pattern theory to produce top K possible semantic interpretations for the video segment.</div>\n<div class=\"imageCredit\">Fillipe de Souza, Sudeep Sarkar</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sudeep&nbsp;Sarkar</div>\n<div class=\"imageTitle\">Semantic integration of video and sound informations</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970554750_Figure2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970554750_Figure2--rgov-800width.jpg\" title=\"Handling simultaneous events using spatial contraints\"><img src=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970554750_Figure2--rgov-66x44.jpg\" alt=\"Handling simultaneous events using spatial contraints\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Each column depicts a scenario of simultaneously occurring events. The interpretations on the top were generated considered spatial constraints and the bottom row results were produced without such constraints.</div>\n<div class=\"imageCredit\">Fillipe de Souza, Sudeep Sarkar, Anuj Srivastava</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sudeep&nbsp;Sarkar</div>\n<div class=\"imageTitle\">Handling simultaneous events using spatial contraints</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970297124_Figure1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970297124_Figure1--rgov-800width.jpg\" title=\"Pattern Theory Pipeline\"><img src=\"/por/images/Reports/POR/2016/1217676/1217676_10206271_1481970297124_Figure1--rgov-66x44.jpg\" alt=\"Pattern Theory Pipeline\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Object and action generators combine using pattern-theoretic rules to produce connected generator structures that represent an interpretation of the scene. The construction of these interpretation structures are guided by the ontology of the domain.</div>\n<div class=\"imageCredit\">Fillipe de Souza, Sudeep Sarkar, Anuj Srivastava</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sudeep&nbsp;Sarkar</div>\n<div class=\"imageTitle\">Pattern Theory Pipeline</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis collaborative research project centered on applications of Grenander?s General Pattern Theory to the representation of complex knowledge associated with human activities in video data.\n\nIntellectual Merit\n\nSuccessful automatic understanding of video content is essential for many computer vision-based applications. These applications focus on recognizing human interactions, which are typically understood as events that involve people performing actions with objects or other people. The main challenges are threefold. First, it is difficult to handle the enormous variety of interactions present in different instances of a complex event. The second problem involves rejecting clutter of extraneous objects so that only the participating ones are considered. The third problem stems from errors in classification of objects and actions. How do we leverage the success of deep-learning/machine learning based labeling methods into building a detailed semantic description of a complex event? We explored the combinatorial approach based on Grenander?s pattern theory &ndash; the classic version. We have formulated a comprehensive and elegant formulation that can handle all these three kinds of challenges, without the need for an extensive training dataset.\n\nThe main idea is to discover objects, actions, and their contextual elements, in video sequences and reach graph-theoretic representations of activities. These representations facilitate high-level tasks, such as characterizing, summarizing, and labeling of activities in video segments. Two broad research directions were developed:\n\nInferences on Human Activities from Videos and Sound: In pattern theory, the mathematical representation of data is a form of a configuration of basic entities, termed generators (or nodes), connected using bonds (or edges) under pre-defined rules. The optimality of a configuration is defined using a global posterior energy that has contributions from both data likelihood and prior knowledge. The choice of generators is dictated by the application context; in cooking videos one typically needs generators for representing objects (spoon, spatula, etc.), materials (bowl, egg, lettuce, etc.), actions (pickup, stir, put, etc.), that constitute a recipe. The presence of generators in a scene is based on spatiotemporal features extracted from video frames using latest learning techniques. The prior knowledge is incorporated in the form on co-occurrence frequencies of generators in the training data. Thus, the detected generators are selectively linked using bonds using a combination of prior, data support and spatiotemporal proximity. The resulting configurations denote interpretations of activities contained in the given video data. Advantages of this framework include incorporation of relevant knowledge to draw efficient scene interpretations, relative immunity to clutter and noise, and scalability to descriptions of long videos towards multiple, simultaneous activities.  The framework is also easily generalization to include other sensory modalities such as sound.\nRate-Invariance Action Classification: This effort also focused on statistical analysis of activities in a manner that is invariant to the rate of performance. The basic approach is to extract characteristic features from individual frames, and then represent each observation of an activity as a time-indexed trajectory on a nonlinear feature space. This project developed novel invariant metrics that compare and quantify differences in such manifold paths, and simultaneously temporally register points across trajectories. These metrics contribute in generating statistical summaries and probabilistic classification of observed activities for performing computer vision tasks efficiently. This approach outperformed state-of-the-art results in the classification of human activities and action using 2D or 3D video and audio data.\n\n\n \n\nBroader Impacts\n\nAt FSU, this project supported education and training of three Ph.D. students (including a female) in the general area of high-dimensional statistical data analysis, with placements in academia and industry. It also supported training of three undergraduate researchers during summer months. At USF, this project directly supported education and training of one Ph.D. student, and partially supported and contributed to the education of three other Ph.D. students. It also supported training of two undergraduate researchers.\n\nAs research outcomes, this projected directly or partly resulted in 8 journal papers, 15+ conferences papers and several book chapters. PI Srivastava finished one graduate-level textbook on \"Functional and Shape Data Analysis\" and two edited volumes (\"3D Face Recognition\" and \"Riemannian Computing in Computer Vision\") with partial support from this project. Current plans include the development of a textbook to disseminate further the knowledge developed in this project.\n\nPI Sarkar along with one of the Ph.D. students participated in the NSF I-Corps program to explore the commercial viability of the ideas developed in this project.\n\nThe participants received several recognitions for this research, including the best paper award at ICPR 2014. They also presented a tutorial on this pattern-theoretic approach at a major conference. The PIs delivered a number of invited talks at colloquia, workshops, and working groups, to advance the use of pattern theory in applications beyond video data analysis and computer vision.\n\nAs an outreach, the PIs also collaborated with researchers at two Federal agencies (NIST and NSWC), resulting in joint research efforts and publications.\n\n  \n\n \n\n \n\n  \n\n \n\n\t\t\t\t\tLast Modified: 12/17/2016\n\n\t\t\t\t\tSubmitted by: Sudeep Sarkar"
 }
}