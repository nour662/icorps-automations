{
 "awd_id": "1247809",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Towards Modeling Human Speech Confusions in Noise",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2012-08-08",
 "awd_max_amd_letter_date": "2012-08-08",
 "awd_abstract_narration": "This EArly-concept Grant for Exploratory Research (EAGER) supports an exploratory study to evaluate model components for prediction of human speech recognition in the presence of noise. Such a model has the potential to predict confusions between fine phonetic distinctions in different levels of background noise and at different speaking rates. The study takes advantage of modern physiological results that indicate that the primary auditory cortex performs spectro-temporal filtering; that is, that there are cells that are sensitive to particular spectro-temporal modulations at each auditory frequency. In this project, perceptual experiments in the presence of both stationary and non-stationary additive noise and at different signal-to-noise ratios for a database of CVC syllables recorded at 2 different speaking rates yield confusion statistics. These statistics are then compared to those resulting from an auditory model enhanced by elements incorporating these spectro-temporal filters. \r\n\r\nSuccessful results from this study will suggest enhancements to current hearing models and ultimately, after a broader study for which this EAGER is a pilot, advance the understanding of human speech perception. Background noise presents a challenging problem for a variety of speech and hearing devices including hearing aids and automatic speech recognition (ASR) systems. Since normal-hearing human listeners are extremely adept at perceiving speech in noise, this improved understanding of human models could lead to better artificial systems for speech processing. The databases and tools developed for this study will be disseminated to the research community.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Abeer",
   "pi_last_name": "Alwan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Abeer Alwan",
   "pi_email_addr": "alwan@ee.ucla.edu",
   "nsf_id": "000090848",
   "pi_start_date": "2012-08-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jody",
   "pi_last_name": "Kreiman",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jody Kreiman",
   "pi_email_addr": "jkreiman@ucla.edu",
   "nsf_id": "000441064",
   "pi_start_date": "2012-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 2\">\n<div class=\"section\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>A collaboration between the Speech Processing and Auditory Perception Laboratory at UCLA and the Speech Group at ICSI focused on the refinement of the simple models used in Automatic Speech Recognition (ASR) with representations that have been developed from observations of mammalian auditory physiology. While no animal experiments were performed in this study, earlier research suggested that auditory systems were particularly sensitive to certain ranges of modulations over time and frequency. Processing techniques based on these observations have proved useful in many previous ASR experiments. In this study, though, the goal was to see if incorporating these insights into ASR approaches would yield a pattern of errors (for consonants in Consonant-Vowel-Consonant (CVC) syllables) that was more similar to what would be observed for human perception. </span></p>\n<p><span>More specifically, we wanted to see if correlations with the pattern of human perceptual errors for CVC syllables for noisy and rapid speech were improved by using modulation-based features for a modern ASR system. To explore this, we recorded a corpus of spoken CVC syllables, and then conducted listening tests and analyzed results to determine the pattern of errors for the listeners. These results were then passed to the ICSI team, who developed the ASR systems and compared results with the perceptual ones. </span></p>\n<p><span>On the perception side, some key results include: (a) The intelligibility of syllable-final&nbsp;consonants is more affected by noise than initial consonants. (b) Slow speech is better perceived than fast speech for /CaC/ and /CiC/ stimuli. (c) The effect of speaking rate is more pronounced in voiced syllable ?nal consonants than their unvoiced counterparts.&nbsp;</span></p>\n<p>On the ASR side, one result in particular was quite notable, mainly, that higher temporal modulations (i.e., components of the speech where the spectral content was varying quickly) were sometimes more helpful than including information from the entire range of temporal modulations. The most prominent of these results occurred for rapid speech, which could be expected since the spectral content varies more quickly in this case. But significant improvement in correlation with human consonant perception was only observed for noisy speech. But for speech recognition researchers, in an era where the efficacy of machine learning approaches is the focus of much attention, it should be interesting that restricting the observed features (to higher modulations) gives improved performance for noisy and rapid speech.&nbsp;</p>\n</div>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/03/2015<br>\n\t\t\t\t\tModified by: Abeer&nbsp;A&nbsp;Alwan</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\n\nA collaboration between the Speech Processing and Auditory Perception Laboratory at UCLA and the Speech Group at ICSI focused on the refinement of the simple models used in Automatic Speech Recognition (ASR) with representations that have been developed from observations of mammalian auditory physiology. While no animal experiments were performed in this study, earlier research suggested that auditory systems were particularly sensitive to certain ranges of modulations over time and frequency. Processing techniques based on these observations have proved useful in many previous ASR experiments. In this study, though, the goal was to see if incorporating these insights into ASR approaches would yield a pattern of errors (for consonants in Consonant-Vowel-Consonant (CVC) syllables) that was more similar to what would be observed for human perception. \n\nMore specifically, we wanted to see if correlations with the pattern of human perceptual errors for CVC syllables for noisy and rapid speech were improved by using modulation-based features for a modern ASR system. To explore this, we recorded a corpus of spoken CVC syllables, and then conducted listening tests and analyzed results to determine the pattern of errors for the listeners. These results were then passed to the ICSI team, who developed the ASR systems and compared results with the perceptual ones. \n\nOn the perception side, some key results include: (a) The intelligibility of syllable-final consonants is more affected by noise than initial consonants. (b) Slow speech is better perceived than fast speech for /CaC/ and /CiC/ stimuli. (c) The effect of speaking rate is more pronounced in voiced syllable ?nal consonants than their unvoiced counterparts. \n\nOn the ASR side, one result in particular was quite notable, mainly, that higher temporal modulations (i.e., components of the speech where the spectral content was varying quickly) were sometimes more helpful than including information from the entire range of temporal modulations. The most prominent of these results occurred for rapid speech, which could be expected since the spectral content varies more quickly in this case. But significant improvement in correlation with human consonant perception was only observed for noisy speech. But for speech recognition researchers, in an era where the efficacy of machine learning approaches is the focus of much attention, it should be interesting that restricting the observed features (to higher modulations) gives improved performance for noisy and rapid speech. \n\n\n\n\n\n\t\t\t\t\tLast Modified: 11/03/2015\n\n\t\t\t\t\tSubmitted by: Abeer A Alwan"
 }
}