{
 "awd_id": "1143373",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SBIR Phase I:  Statistical Inference for Advanced Entity Resolution",
 "cfda_num": "47.084",
 "org_code": "15030000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Juan E. Figueroa",
 "awd_eff_date": "2012-01-01",
 "awd_exp_date": "2012-12-31",
 "tot_intn_awd_amt": 149992.0,
 "awd_amount": 179992.0,
 "awd_min_amd_letter_date": "2011-11-29",
 "awd_max_amd_letter_date": "2012-06-15",
 "awd_abstract_narration": "This Small Business Innovation Research (SBIR) Phase I project addresses the problem of integrating information about named entities, such as people, companies, and products, from numerous data sources.  Integrating information about entities from multiple sources can be difficult because sources may use different formats and terminology to describe the same entity, a problem referred to as \"entity resolution\".  Most existing commercial enterprise systems rely on rule-based matching techniques for entity resolution.  This project investigates statistical learning techniques that allow a system to estimate the probability of a match, rather than computing a score based on ad-hoc rules or weights. Because the approach is based on sound statistical principles and uses evidence compiled from large datasets, it can produce more accurate results than existing methods. Moreover, these advantages are amplified when handling data that that has highly variable, missing or noisy attributes, such as data extracted from Web sites. \r\n\r\nThe broader impact/commercial potential of this project lies in enabling enterprises to perform more accurate and reliable data integration.  The are many potential target markets that need better technology for integrating information about businesses, products, people, locations, and other entities.  This capability is critical for some of the nation's largest companies and institutions, from search engines, to the U.S. Intelligence and law enforcement community, to financial institutions.  In particular, large enterprises often have difficulty utilizing data extracted from news, foreign language data sources, and social media, because the extracted data is noisy and not-well structured. The technology developed in this project will help enterprises make use of the growing amount of information on the Web, so that they can take advantage of the network of relationships that link people, companies, and other entities to serve their customers better.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "TIP",
 "org_dir_long_name": "Directorate for Technology, Innovation, and Partnerships",
 "div_abbr": "TI",
 "org_div_long_name": "Translational Impacts",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Minton",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Steven N Minton",
   "pi_email_addr": "steven.n.minton@gmail.com",
   "nsf_id": "000594499",
   "pi_start_date": "2011-11-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "InferLink Corporation",
  "inst_street_address": "2361 ROSECRANS AVE STE 348",
  "inst_street_address_2": "",
  "inst_city_name": "EL SEGUNDO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3103839234",
  "inst_zip_code": "902454929",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "INFERLINK CORP",
  "org_prnt_uei_num": "M8KCZYW54M17",
  "org_uei_num": "M8KCZYW54M17"
 },
 "perf_inst": {
  "perf_inst_name": "InferLink Corporation",
  "perf_str_addr": "326 Loma Vista Street",
  "perf_city_name": "El Segundo",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "902452901",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "537100",
   "pgm_ele_name": "SBIR Phase I"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "132E",
   "pgm_ref_txt": "CENTERS: SENSING & INFO SYS"
  },
  {
   "pgm_ref_code": "5371",
   "pgm_ref_txt": "SMALL BUSINESS PHASE I"
  },
  {
   "pgm_ref_code": "8033",
   "pgm_ref_txt": "Hardware Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 179992.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The growth of the Internet has made it much easier to aggregate information about named entities, such as companies and products, from numerous data sources.&nbsp; However, integrating information from multiple sources can be difficult due to the use of different formats and terminology to refer to the same entity.&nbsp; For example, one source may refer to &ldquo;St. John&rsquo;s Hospital in LA&rdquo; and another might refer to the same entity as &ldquo;Saint John&rsquo;s in Santa Monica&rdquo;.&nbsp;&nbsp; This matching problem, often referred to as <em>entity resolution, </em>arises in a variety of commercial applications.&nbsp; Current enterprise systems typically employ ad-hoc rules that are manually configured by trial and error.&nbsp; However, these systems tend to perform poorly on data extracted from heterogeneous sources, such as text documents.&nbsp;&nbsp; In this project, we build on recent progress in statistical machine learning algorithms to address challenging entity resolution applications beyond the capabilities of present systems.&nbsp;</p>\n<p>In particular, our work investigated a statistical learning approach that allows a system to estimate the probability of a match, rather than computing a score based on ad-hoc rules.&nbsp;&nbsp; Because the approach is based on sound statistical principles and uses evidence compiled from large datasets, it can produce more accurate results than rule-based approaches. &nbsp;Moreover, these advantages are amplified when handling data that that has highly-variable, missing or noisy attributes. Whereas rule-based approaches can be brittle due to difficulties inherent in combining ad-hoc scores for different attributes, statistical inference methods can deal with such problems gracefully.</p>\n<p>There were two primary focuses in our Phase I project.&nbsp; The first was to investigate extensions to our statistical model necessary for handling diverse types of real-world data. In particular, we addressed several challenging issues, including 1) integrating data extracted by multiple natural language text extractors, each of which may be noisy in different respects, 2) handling relational<em> </em>data, that is, data about the relations between two or more entities, rather than simple attributes, and 3) resolving entities in multiple languages, which can involve a combination of transliteration and translation. &nbsp;&nbsp;We designed and evaluated extensions to our existing statistical model to handle these challenges. &nbsp;We also investigated a joint inference approach that we can use to implement the extensions in phase II.</p>\n<p>In addition to designing extensions to our existing model, we also collected and analyzed commercially-relevant datasets to help us prioritize our design work, and to estimate the performance improvements we can expect from our technical enhancements in different domains.</p>\n<p>&nbsp;This work establishes the foundation for our phase II project, where we have proposed implementing, refining and testing the designs that we developed in phase I. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/03/2013<br>\n\t\t\t\t\tModified by: Steven&nbsp;N&nbsp;Minton</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe growth of the Internet has made it much easier to aggregate information about named entities, such as companies and products, from numerous data sources.  However, integrating information from multiple sources can be difficult due to the use of different formats and terminology to refer to the same entity.  For example, one source may refer to \"St. John\u00c6s Hospital in LA\" and another might refer to the same entity as \"Saint John\u00c6s in Santa Monica\".   This matching problem, often referred to as entity resolution, arises in a variety of commercial applications.  Current enterprise systems typically employ ad-hoc rules that are manually configured by trial and error.  However, these systems tend to perform poorly on data extracted from heterogeneous sources, such as text documents.   In this project, we build on recent progress in statistical machine learning algorithms to address challenging entity resolution applications beyond the capabilities of present systems. \n\nIn particular, our work investigated a statistical learning approach that allows a system to estimate the probability of a match, rather than computing a score based on ad-hoc rules.   Because the approach is based on sound statistical principles and uses evidence compiled from large datasets, it can produce more accurate results than rule-based approaches.  Moreover, these advantages are amplified when handling data that that has highly-variable, missing or noisy attributes. Whereas rule-based approaches can be brittle due to difficulties inherent in combining ad-hoc scores for different attributes, statistical inference methods can deal with such problems gracefully.\n\nThere were two primary focuses in our Phase I project.  The first was to investigate extensions to our statistical model necessary for handling diverse types of real-world data. In particular, we addressed several challenging issues, including 1) integrating data extracted by multiple natural language text extractors, each of which may be noisy in different respects, 2) handling relational data, that is, data about the relations between two or more entities, rather than simple attributes, and 3) resolving entities in multiple languages, which can involve a combination of transliteration and translation.   We designed and evaluated extensions to our existing statistical model to handle these challenges.  We also investigated a joint inference approach that we can use to implement the extensions in phase II.\n\nIn addition to designing extensions to our existing model, we also collected and analyzed commercially-relevant datasets to help us prioritize our design work, and to estimate the performance improvements we can expect from our technical enhancements in different domains.\n\n This work establishes the foundation for our phase II project, where we have proposed implementing, refining and testing the designs that we developed in phase I.  \n\n \n\n\t\t\t\t\tLast Modified: 03/03/2013\n\n\t\t\t\t\tSubmitted by: Steven N Minton"
 }
}