{
 "awd_id": "1149787",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Multimodal and Multialgorithm Facial Activity Understanding by Audiovisual Information Fusion",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-03-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 443803.0,
 "awd_amount": 443803.0,
 "awd_min_amd_letter_date": "2012-02-27",
 "awd_max_amd_letter_date": "2012-02-27",
 "awd_abstract_narration": "This project develops a unified multimodal and multialgorithm fusion framework to recognize facial action units, which describe complex and rich facial behaviors. The information from voice is incorporated with visual observations to effectively improve facial activity understanding since voice and facial activity are intrinsically correlated. The developed framework systematically captures the inherent interactions between the visual and audio channels in a global context of human perception of facial behavior. Advanced machine learning techniques are developed to integrate these relationships together with uncertainties associated with various visual and audio measurements in the fusion framework to achieve a robust and accurate understanding of facial activity. It is these coordinated and consistent interactions that produce a meaningful facial display.\r\n\r\nThe research work from this project fosters computer vision and machine learning technologies with applications across a wide range of fields varying from psychiatry to human-computer interaction. The new audiovisual emotional database constructed in this research facilitates benchmark evaluations and promotes new research directions, especially, in human behavior analysis. An integration of research and education promotes cutting-edge training on human-computer interactions to K-12, undergraduate, and graduate students, especially encourages the participation of women in engineering and computing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yan",
   "pi_last_name": "Tong",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yan Tong",
   "pi_email_addr": "tongy@cec.sc.edu",
   "nsf_id": "000592008",
   "pi_start_date": "2012-02-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University South Carolina Research Foundation",
  "inst_street_address": "915 BULL ST",
  "inst_street_address_2": "STE 202",
  "inst_city_name": "COLUMBIA",
  "inst_state_code": "SC",
  "inst_state_name": "South Carolina",
  "inst_phone_num": "8037777093",
  "inst_zip_code": "292084009",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "SC06",
  "org_lgl_bus_name": "SOUTH CAROLINA RESEARCH FOUNDATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "ELBVJ1KYX976"
 },
 "perf_inst": {
  "perf_inst_name": "University South Carolina",
  "perf_str_addr": "315 Main Street",
  "perf_city_name": "Columbia",
  "perf_st_code": "SC",
  "perf_st_name": "South Carolina",
  "perf_zip_code": "292080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "SC06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  },
  {
   "pgm_ele_code": "915000",
   "pgm_ele_name": "EPSCoR Co-Funding"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 443803.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Facial behavior is the most natural and powerful means of expressing affective and emotional states during human communication. To describe more complex facial activities, Facial Action Coding System developed by Ekman and Friesen defines a set of facial action units (AUs) such as \"lip corner raiser\" and \"lips apart\", each of which is anatomically related to the contraction of a set of facial muscles. An automatic facial AU recognition system is desired in many applications, such as human behavior analysis, interactive games, online learning, etc.</span><span>&nbsp;</span></p>\n<p><span>Although great progress has been achieved on posed expressions and under controlled image acquisition, facial activity recognition from spontaneous expressions has been hindered from practical applications due to subtle and complex facial deformation, frequent head movements, and large intra-class variations caused by identity-related attributes, e.g., age, race, and gender. <span>&nbsp;</span>Furthermore, it is extremely challenging to recognize AUs that are responsible for producing speech. During speech, these AUs are generally activated at a low intensity with subtle changes in facial appearance and facial geometry and more importantly, often introduce ambiguity in detecting other co-occurring AUs. The failure in recognition of speech-related AUs is because information is extracted from a single source, i.e., the visual channel.</span><span>&nbsp;</span></p>\n<p><span>In this project, information from voice has been incorporated with visual observations to effectively improve facial activity understanding since voice and facial activity are intrinsically correlated in natural human communications. Some lower-face facial AUs and voice can be <span>physiologically correlated </span>since jaw and lower-face facial muscles are highly involved in speech production. These relationships are well recognized and have been exploited in natural human communications. For instance, without looking at a face, people knows that the other person is opening his/her mouth when hearing laughter. In this project, this type of relationships was theoretically modelled and exploited for improving automatic facial activity recognition.</span><span>&nbsp;</span></p>\n<p><span>Instead of solely improving visual observations of facial activity, this project developed a unified audiovisual fusion framework to recognize facial AUs, which makes the best use of visual and acoustic cues in recognizing speech-related facial AUs. Advanced machine learning techniques have been developed to integrate these relationships together with uncertainties associated with various visual and audio measurements in the fusion framework to achieve a robust and accurate understanding of facial activity. Specifically, </span><span lang=\"EN\">the relationships between individual phonemes and AUs and the relationships between groups of phonemes and AUs have been investigated for consonants, vowels, and diphthongs, respectively. Various fusion models have been investigated and developed in this project including feature-level fusion methods and decision-level fusion methods. Among them, probabilistic graphic models such as Dynamic Bayesian Networks and Continuous Time Bayesian Networks have been developed to explicitly model and exploit the semantic and dynamic physiological relationships between AUs and phonemes as well as measurement uncertainty. </span><span>&nbsp;</span></p>\n<p><span lang=\"EN\">As far as we know, all the publicly available AU-coded datasets only provide visual information. Thus, in order to learn the dynamic and physiological relationships between AUs and phonemes, as well as to facilitate audiovisual fusion, a pilot audiovisual database was constructed in this project. Experiments on this pilot audiovisual AU-coded database have demonstrated that the proposed probabilistic audiovisual fusion framework significantly outperforms the state-of-the-art visual-based methods in terms of recognizing speech-related AUs, especially for those AUs that are activated at low intensities or \"hardly visible\" in the visual channel, and more importantly, is also superior to audio-based methods and feature-level fusion methods, which employ low-level audio features, by explicitly modeling and exploiting physiological relationships between AUs and phonemes.</span><span lang=\"EN\">&nbsp;</span></p>\n<p><span lang=\"EN\">In addition to developing audiovisual fusion models, </span><span lang=\"EN\">advanced machine learning based methods have been developed in this project to extract discriminative features for improving facial activity recognition in the visual channel. For example, an Optimized Filter Size CNN was developed to learn features across various image resolutions; </span><span>a </span><span lang=\"EN\">Feature Disentangling Machine was developed for simultaneous feature selection and disentangling; and </span><span>a Boosted Deep Belief Network and an </span><span lang=\"EN\">Incremental Boosting CNN </span><span>were developed for performing feature learning, feature selection, and classification in a unified framework.</span><span> <span>&nbsp;</span></span><span lang=\"EN\">&nbsp;</span></p>\n<p><span>The basic research in this project can foster advanced computer vision and machine learning technologies with applications across a wide range of fields varying from entertainment to psychiatry to human-computer interaction. Research results from this project were disseminated to the research community in 11 conference papers including two CVPR papers, one ECCV paper, one NIPS paper, and two FG papers, and 3 journal papers including one IEEE Trans. on Cybernetics and one IEEE Trans. on Affective Computing.</span><span>&nbsp;</span></p>\n<p><span>An integration of research and education promotes cutting-edge training on human-computer interactions to undergraduate and graduate students, especially encourages the participation of underrepresented groups and women in engineering and computing. </span></p>\n<p><strong>&nbsp;</strong><em>&nbsp;</em></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/21/2018<br>\n\t\t\t\t\tModified by: Yan&nbsp;Tong</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nFacial behavior is the most natural and powerful means of expressing affective and emotional states during human communication. To describe more complex facial activities, Facial Action Coding System developed by Ekman and Friesen defines a set of facial action units (AUs) such as \"lip corner raiser\" and \"lips apart\", each of which is anatomically related to the contraction of a set of facial muscles. An automatic facial AU recognition system is desired in many applications, such as human behavior analysis, interactive games, online learning, etc. \n\nAlthough great progress has been achieved on posed expressions and under controlled image acquisition, facial activity recognition from spontaneous expressions has been hindered from practical applications due to subtle and complex facial deformation, frequent head movements, and large intra-class variations caused by identity-related attributes, e.g., age, race, and gender.  Furthermore, it is extremely challenging to recognize AUs that are responsible for producing speech. During speech, these AUs are generally activated at a low intensity with subtle changes in facial appearance and facial geometry and more importantly, often introduce ambiguity in detecting other co-occurring AUs. The failure in recognition of speech-related AUs is because information is extracted from a single source, i.e., the visual channel. \n\nIn this project, information from voice has been incorporated with visual observations to effectively improve facial activity understanding since voice and facial activity are intrinsically correlated in natural human communications. Some lower-face facial AUs and voice can be physiologically correlated since jaw and lower-face facial muscles are highly involved in speech production. These relationships are well recognized and have been exploited in natural human communications. For instance, without looking at a face, people knows that the other person is opening his/her mouth when hearing laughter. In this project, this type of relationships was theoretically modelled and exploited for improving automatic facial activity recognition. \n\nInstead of solely improving visual observations of facial activity, this project developed a unified audiovisual fusion framework to recognize facial AUs, which makes the best use of visual and acoustic cues in recognizing speech-related facial AUs. Advanced machine learning techniques have been developed to integrate these relationships together with uncertainties associated with various visual and audio measurements in the fusion framework to achieve a robust and accurate understanding of facial activity. Specifically, the relationships between individual phonemes and AUs and the relationships between groups of phonemes and AUs have been investigated for consonants, vowels, and diphthongs, respectively. Various fusion models have been investigated and developed in this project including feature-level fusion methods and decision-level fusion methods. Among them, probabilistic graphic models such as Dynamic Bayesian Networks and Continuous Time Bayesian Networks have been developed to explicitly model and exploit the semantic and dynamic physiological relationships between AUs and phonemes as well as measurement uncertainty.  \n\nAs far as we know, all the publicly available AU-coded datasets only provide visual information. Thus, in order to learn the dynamic and physiological relationships between AUs and phonemes, as well as to facilitate audiovisual fusion, a pilot audiovisual database was constructed in this project. Experiments on this pilot audiovisual AU-coded database have demonstrated that the proposed probabilistic audiovisual fusion framework significantly outperforms the state-of-the-art visual-based methods in terms of recognizing speech-related AUs, especially for those AUs that are activated at low intensities or \"hardly visible\" in the visual channel, and more importantly, is also superior to audio-based methods and feature-level fusion methods, which employ low-level audio features, by explicitly modeling and exploiting physiological relationships between AUs and phonemes. \n\nIn addition to developing audiovisual fusion models, advanced machine learning based methods have been developed in this project to extract discriminative features for improving facial activity recognition in the visual channel. For example, an Optimized Filter Size CNN was developed to learn features across various image resolutions; a Feature Disentangling Machine was developed for simultaneous feature selection and disentangling; and a Boosted Deep Belief Network and an Incremental Boosting CNN were developed for performing feature learning, feature selection, and classification in a unified framework.   \n\nThe basic research in this project can foster advanced computer vision and machine learning technologies with applications across a wide range of fields varying from entertainment to psychiatry to human-computer interaction. Research results from this project were disseminated to the research community in 11 conference papers including two CVPR papers, one ECCV paper, one NIPS paper, and two FG papers, and 3 journal papers including one IEEE Trans. on Cybernetics and one IEEE Trans. on Affective Computing. \n\nAn integration of research and education promotes cutting-edge training on human-computer interactions to undergraduate and graduate students, especially encourages the participation of underrepresented groups and women in engineering and computing. \n\n  \n\n \n\n\t\t\t\t\tLast Modified: 12/21/2018\n\n\t\t\t\t\tSubmitted by: Yan Tong"
 }
}