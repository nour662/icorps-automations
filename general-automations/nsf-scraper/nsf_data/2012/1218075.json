{
 "awd_id": "1218075",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Ensuring Reliability and Portability of Scientific Software for Heterogeneous Architectures",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 499857.0,
 "awd_amount": 507857.0,
 "awd_min_amd_letter_date": "2012-07-03",
 "awd_max_amd_letter_date": "2013-04-15",
 "awd_abstract_narration": "Numerical results of scientific computations are stored in computers as\r\nfloating-point numbers, an approximation of real numbers that accounts for\r\nthe fact that a computer's storage is limited. This need for approximation\r\nhas the unfortunate side effect that floating-point numbers don't abide by\r\ncommon laws of arithmetic known from high school, such as the associativity\r\nof addition. As a consequence, apparently equivalent implementations of\r\nfloating-point operations on computer hardware may produce very different\r\nresults, such as when the order of operands of an addition is changed by a\r\ncompiler. Programs generically written for high-performance parallel computing\r\nplatforms are likely to be compiled using different floating-point\r\nimplementations and schedulings, as the executable resulting from the\r\ncompilation depends on the available hardware. Such parallel scientific\r\nprograms are therefore susceptible to reliability and portability issues\r\nthat can range from simple deviations in precision to drastic changes of\r\nprogram control flow when moving from one architecture to another.\r\nThe results of this research will be tools and techniques to help scientists \r\nfind bugs more effectively in such programs. This research has important implications \r\nfor the reliability of important scientific programs such as those used in biomedical \r\nimaging applications, climate modelling, and vehicle design. \r\n\r\nThis project develops rigorous methods for analyzing parallel scientific\r\ncode, specifically written using the now emerging OpenCL parallel\r\nprogramming standard. The goal is to detect potential sources of\r\nreliability and portability deficiencies in such code that are due to\r\ndependencies of the floating-point behavior on the underlying hardware,\r\nwhich may be unknown to the programmer. Traditional reliability methods\r\nsuch as program testing and debugging are ineffective for parallel OpenCL\r\nprograms, because program behavior may vary across runs, making after-test\r\nbehavior uncertain. For these reasons, the investigators will use rigorous\r\nanalysis methods that are not solely based on program execution. Instead,\r\nthe program is formally modeled as a transition system; the model is\r\nencoded symbolically, using logical formula representations that can often\r\ncompactly represent the set of executions of the program without executing\r\nit. The program model is then analyzed for portability violations and\r\nprogram errors using floating point-capable decision procedures and model\r\ncheckers. To achieve scalability, the investigators plan to exploit the\r\nhighly symmetric and parametric form of OpenCL programs, where identical\r\noperations are performed by many computational threads in Single\r\nInstruction Multiple Data (SIMD) style.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Thomas",
   "pi_last_name": "Wahl",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Thomas Wahl",
   "pi_email_addr": "wahl@ccs.neu.edu",
   "nsf_id": "000608439",
   "pi_start_date": "2012-07-03",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Miriam",
   "pi_last_name": "Leeser",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Miriam E Leeser",
   "pi_email_addr": "mel@ece.neu.edu",
   "nsf_id": "000194950",
   "pi_start_date": "2012-07-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 HUNTINGTON AVE",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "794400",
   "pgm_ele_name": "SOFTWARE ENG & FORMAL METHODS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 499857.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to investigate to what extent <strong>calculations</strong> on computers, i.e. the processing of purely numeric data such as in scientific computing, depends on the computational platform on which the calculation runs. The background is that, while mathematics defines unambiguously what the output of arithmetic operations such as + is , computers cannot implement mathematical arithmetic, for a variety of reasons. Instead, they must approximate what happens in mathematics. The most widely used approximation of real-number arithmetic used in computers is known as floating-point arithmetic.</p>\n<p>If the result of calculations depends heavily on the hardware implementation of the \"floating-point calculator\" used on the machine, we have a portability problem: moving a calculation that was programmed, say, on a desktop PC, to a high-performance graphics computer (without changing the input data) can change the result. The changes in the result can then affect the entire computation, say if the program makes <strong>decisions</strong> based on the numeric result. In extreme cases, a binary \"yes/no\" output can depend purely on non-mathematical artifacts, such as the machine, compiler flags, etc. This situation renders numeric programs usable only by computer and numerics experts, which excludes many scientists that depend on such programs for simulations.</p>\n<p>In this project we accomplished the following:</p>\n<ol>\n<li>We confirmed the frequency and severity of this problem, especially for heterogeneous platforms where hardware differences are significant. Platform dependencies are frequent, and so are situations where overall program behavior (rather than just a single numeric result) is affected by such dependencies. We experimented with numeric programs run on PCs and GPU computers, manufactured by diverse hardware companies such as Intel, NVidia, and AMD.</li>\n<li>We designed a procedure that can help (the many) non-numerics experts and non=floating-point experts detect the sensitivity of programs they plan to use to the above problem. The procedure will test whether, for a given input, the outcome depends unreasonably on the computational platform (meaning that the outcome variations due to platform differences can be large), and it can also find critical such inputs for the program.</li>\n<li>We designed a procedure that can assist users in \"fixing\" their programs. To fix such a program means to modify it such that the platform dependencies go away. This is not necessarily required for all kinds of computations. But whenever portability and reproducibility is a prime concern, it can be enforced by disabling extra features that the compiler and the platform may offer, which would render the computed result irreproducible. Those features of compilers and hardware are often offered in the name of performance: they are intended to speed up the computation. Therefore, disabling them across the entire program can have a non-trivial performance cost. Our procedure therefore identifies code segments in the program that contribute most significantly to the platform dependence of an output later in the code. The user can then disable those features only for the (short) code segment thus identified. Performance-enhancing improvements can still be applied to the rest of the code, without affecting&nbsp;reproducibility and portability of the code.</li>\n</ol>\n<p>We have published this work both in the form of scientific papers presented in leading international meetings, as well as in the form of tools that can be downloaded and used by others, academics and practitioners alike.</p>\n<ol> </ol><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/27/2016<br>\n\t\t\t\t\tModified by: Thomas&nbsp;Wahl</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to investigate to what extent calculations on computers, i.e. the processing of purely numeric data such as in scientific computing, depends on the computational platform on which the calculation runs. The background is that, while mathematics defines unambiguously what the output of arithmetic operations such as + is , computers cannot implement mathematical arithmetic, for a variety of reasons. Instead, they must approximate what happens in mathematics. The most widely used approximation of real-number arithmetic used in computers is known as floating-point arithmetic.\n\nIf the result of calculations depends heavily on the hardware implementation of the \"floating-point calculator\" used on the machine, we have a portability problem: moving a calculation that was programmed, say, on a desktop PC, to a high-performance graphics computer (without changing the input data) can change the result. The changes in the result can then affect the entire computation, say if the program makes decisions based on the numeric result. In extreme cases, a binary \"yes/no\" output can depend purely on non-mathematical artifacts, such as the machine, compiler flags, etc. This situation renders numeric programs usable only by computer and numerics experts, which excludes many scientists that depend on such programs for simulations.\n\nIn this project we accomplished the following:\n\nWe confirmed the frequency and severity of this problem, especially for heterogeneous platforms where hardware differences are significant. Platform dependencies are frequent, and so are situations where overall program behavior (rather than just a single numeric result) is affected by such dependencies. We experimented with numeric programs run on PCs and GPU computers, manufactured by diverse hardware companies such as Intel, NVidia, and AMD.\nWe designed a procedure that can help (the many) non-numerics experts and non=floating-point experts detect the sensitivity of programs they plan to use to the above problem. The procedure will test whether, for a given input, the outcome depends unreasonably on the computational platform (meaning that the outcome variations due to platform differences can be large), and it can also find critical such inputs for the program.\nWe designed a procedure that can assist users in \"fixing\" their programs. To fix such a program means to modify it such that the platform dependencies go away. This is not necessarily required for all kinds of computations. But whenever portability and reproducibility is a prime concern, it can be enforced by disabling extra features that the compiler and the platform may offer, which would render the computed result irreproducible. Those features of compilers and hardware are often offered in the name of performance: they are intended to speed up the computation. Therefore, disabling them across the entire program can have a non-trivial performance cost. Our procedure therefore identifies code segments in the program that contribute most significantly to the platform dependence of an output later in the code. The user can then disable those features only for the (short) code segment thus identified. Performance-enhancing improvements can still be applied to the rest of the code, without affecting reproducibility and portability of the code.\n\n\nWe have published this work both in the form of scientific papers presented in leading international meetings, as well as in the form of tools that can be downloaded and used by others, academics and practitioners alike.\n \n\n\t\t\t\t\tLast Modified: 08/27/2016\n\n\t\t\t\t\tSubmitted by: Thomas Wahl"
 }
}