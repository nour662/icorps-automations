{
 "awd_id": "1161564",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CGV: Medium: Collaborative Research: Understanding Translucency: Physics, Perception, and Computation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 389660.0,
 "awd_amount": 389660.0,
 "awd_min_amd_letter_date": "2012-08-29",
 "awd_max_amd_letter_date": "2012-08-29",
 "awd_abstract_narration": "People care greatly about the appearance of translucent materials such as food, skin, soap, and marble, and they are able to distinguish subtle differences in these materials based on their appearance.  The translucent appearance of these materials is caused by internal volumetric scattering, which is challenging to simulate, especially because humans are so sensitive to their subtleties.  Since in the natural world scattering materials are the norm, not the exception, it makes sense that the human visual system is so well engineered to analyze them.  However, very little is known about how this analysis is achieved because the perception of volumetric translucency is almost unstudied.\r\n\r\nThis collaborative project, involving faculty from three universities with complementary expertise in computer graphics, human vision, machine learning, and computer vision, addresses the fundamental unsolved problem of understanding translucency for graphics.  The PIs will develop a perceptually-motivated pipeline for translucency, contributing new scattering representations, perceptual dimensions, and computational algorithms to computer graphics.  The scattering representations, based on a polydispersion model, will provide analytic expressions for wavelength-dependent bulk scattering properties of translucent media; this will significantly expand the range of materials that can be simulated with high visual fidelity.  Finding perceptual knobs that relate physical scattering parameters with visual appearance will be achieved by coupling large-scale computation (using cloud computing) with controlled perceptual studies.  Novel acquisition approaches that employ hyperspectral imaging will be created, as will editing and rendering applications that use the new perceptual representations of translucency.  Low-dimensional models to represent scattering media will be developed and used to enable efficient and accurate acquisition and rendering.  A suite of test materials and scenes will be developed to evaluate the fidelity of rendered images based on the developed theory and computational applications.\r\n\r\nBroader Impacts:   Currently, the simulation of translucency presents challenges in terms of both computation and visual fidelity.  This restricts the ability of practical algorithms to predictively simulate translucent materials, thus fundamentally limiting the use of graphics in real applications. By building the computational tools to characterize, study, and use knowledge of translucency perception, this research will fundamentally change the graphics pipeline for translucent materials. and will potentially revolutionizing industrial design, interior design, skin care and cosmetics, and entertainment.\r\n\r\nThe project includes an education program that is tightly coupled to the research program.  The PIs have already been meeting twice a week for more than six months, and their graduate students already share data, code, and equipment.  During the activity, the students will make week-long and month-long visits to each other's laboratories to collaborate, and in this way the project will produce a generation of researchers who are \"T-shaped\" in the sense of being both deep in their respective fields and able to work effectively across these synergistic disciplines.  The PIs also plan to organize a workshop that will brins together researchers in vision science, computer graphics, and computer vision, so that the important ties between these fields are strengthened even further.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Todd",
   "pi_last_name": "Zickler",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Todd Zickler",
   "pi_email_addr": "zickler@eecs.harvard.edu",
   "nsf_id": "000118883",
   "pi_start_date": "2012-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Harvard University",
  "inst_street_address": "1033 MASSACHUSETTS AVE STE 3",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6174955501",
  "inst_zip_code": "021385366",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "PRESIDENT AND FELLOWS OF HARVARD COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "LN53LCFJFL45"
 },
 "perf_inst": {
  "perf_inst_name": "Harvard University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021382933",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "745300",
   "pgm_ele_name": "GRAPHICS & VISUALIZATION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 389660.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span id=\"docs-internal-guid-94243a31-15d1-2364-81e4-07747771935e\"> </span></p>\n<p dir=\"ltr\"><span>Translucent materials, like milk, wax, soap, skin and jade, are all around us. &nbsp;Humans can perceive subtle differences in translucency when distinguishing between these materials based on their appearance. Yet, little is known about what image cues are being used by humans to achieve this discrimination, or how these image cues relate to way light scatters internally in translucent media. &nbsp;The goal of this project was to develop a perceptually-motivated pipeline for translucency, contributing new light scattering representations, a better understanding of how human beings perceive translucent materials, and new material recognition algorithms to computer graphics and computer vision. The outcomes can be described in three parts.</span></p>\n<p dir=\"ltr\"><span><br /></span></p>\n<p dir=\"ltr\"><span>Measuring translucent materials</span></p>\n<p dir=\"ltr\"><span>One way to understand translucent appearance is to quantitatively measure the set of physical scattering material properties that occur in the world around us. These quantitative models can then be used to predict the appearance of translucent media by simulating light transport in different geometries, lighting and scenes.</span></p>\n<p dir=\"ltr\"><span>The key challenge of acquiring the optical scattering properties of translucent materials is that they cannot be measured directly. &nbsp;Measuring optical properties requires sending photons toward an object and using cameras to record where they arrive after interacting with the object. Photons take different paths, each undergoing a different combination of scattering, reflection, and refraction inside the object or at its surface. What makes measurement challenging is that each of a camera&rsquo;s pixels records a sum of many such photons conflated together: to recover the material properties, one must &ldquo;undo&rdquo; this conflation, which requires determining which paths were travelled by the measured photons, and what events occurred along the paths.</span></p>\n<p dir=\"ltr\"><span>In this project, we established a foundation for solving these kinds of difficult measurement problems, leading to papers in SIGGRAPH Asia 2013, SIGGRAPH 14, SIGGRAPH 2015, and ECCV 2016. These papers &nbsp;developed mathematical and computational tools for searching the space of possible material properties for those that match any acquired set of photographs. We developed &nbsp;an acquisition system that exploits the wave nature of light to produce a richer set of images than conventional photographs: images that not only record where photons arrive but also the precise length of the paths that they travel between source and camera. Together, our theory and acquisition system create opportunities for measuring optical material properties with unprecedented generality, precision, and accuracy.</span></p>\n<p>&nbsp;</p>\n<p dir=\"ltr\"><span>Perception</span></p>\n<p dir=\"ltr\"><span>Our ability to effectively design, edit, and render translucent materials like milk, soap, and wax depends on our understanding of how humans perceive translucency. While it is clear that human observers care greatly about translucent appearance in many cases -- as when they distinguish milk from cream, or marble from Formica -- even basic questions about how this occurs remain unanswered.</span></p>\n<p dir=\"ltr\"><span>During the award period we performed some of the first qualitative and quantitative analyses of the perception of translucent materials. Using a unique combination of large-scale simulations, psychophysical experiments with human subjects, and data-mining, we studied the perception of objects that are composed of spatially-homogeneous material. We were able to unveil qualitative and quantitative models of how an object&rsquo;s appearance is related to the physical properties of the material, and how both of these are related to how humans perceive translucent materials. These results were published in Transactions on Graphics 2013, Journal of Vision 2015, and Computer Vision and Pattern Recognition 2015. These projects lay the groundwork for better understanding how the human brain recognizes and understands translucent materials.</span></p>\n<p dir=\"ltr\"><span><br /></span></p>\n<p dir=\"ltr\"><span>Applications: recognition/rendering/editing</span></p>\n<p><span>Our understanding of translucency enables many applications in computer graphics and computer vision. We have developed new material recognition algorithms and material editing applications to modify translucency in images. These tools let designers intuitively &nbsp;change the appearance of materials in images using simple image-based editing operations. These results were published in Computer Vision and Pattern Recognition 2015 and Transactions on Graphics 2015.</span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 02/06/2017<br>\n\t\t\t\t\tModified by: Todd&nbsp;Zickler</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425384192_measurement_teaser--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425384192_measurement_teaser--rgov-800width.jpg\" title=\"Acquiring scattering parameters of homogeneous materials by inverse scattering\"><img src=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425384192_measurement_teaser--rgov-66x44.jpg\" alt=\"Acquiring scattering parameters of homogeneous materials by inverse scattering\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Left: Measurements of two materials (milk, blue curacao) in glass cells, illuminated by a trichromatic laser beam. The observed scattering patterns are input to an optimization algorithm that recovers precise internal scattering parameters. Right: Once the parameters are recovered we can use them to</div>\n<div class=\"imageCredit\">Ioannis Gkioulekas</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Todd&nbsp;Zickler</div>\n<div class=\"imageTitle\">Acquiring scattering parameters of homogeneous materials by inverse scattering</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425496248_TransientVideos--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425496248_TransientVideos--rgov-800width.jpg\" title=\"Measuring light in flight at a trillion frames per second: visualizations of two different scenes\"><img src=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425496248_TransientVideos--rgov-66x44.jpg\" alt=\"Measuring light in flight at a trillion frames per second: visualizations of two different scenes\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">For each row, the leftmost column shows a conventional photo of the scene, and columns two to four show representative frames that each represent contributions from photons that have taken longer to travel from the source to the camera.</div>\n<div class=\"imageCredit\">Ioannis Gkioulekas</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Todd&nbsp;Zickler</div>\n<div class=\"imageTitle\">Measuring light in flight at a trillion frames per second: visualizations of two different scenes</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425735820_perception--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425735820_perception--rgov-800width.jpg\" title=\"Perception of translucency\"><img src=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425735820_perception--rgov-66x44.jpg\" alt=\"Perception of translucency\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Our generalized model for scattering produces a 2D perceptual appearance space (left), which expands the set of appearances that can be created using standard scattering models. We can navigate this space using a small set of perceptual dimensions, to match the appearance of different materials.</div>\n<div class=\"imageCredit\">Ioannis Gkioulekas</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Todd&nbsp;Zickler</div>\n<div class=\"imageTitle\">Perception of translucency</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425941955_kb_editing--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425941955_kb_editing--rgov-800width.jpg\" title=\"Image-based material editing\"><img src=\"/por/images/Reports/POR/2017/1161564/1161564_10207521_1486425941955_kb_editing--rgov-66x44.jpg\" alt=\"Image-based material editing\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Simple image-based editing operations like boosting frequencies and/or the high amplitudes of the image signal can change the appearance of translucent skin to appear more oily or wet, or more blemished.</div>\n<div class=\"imageCredit\">Ivaylo Boyadzhiev</div>\n<div class=\"imageSubmitted\">Todd&nbsp;Zickler</div>\n<div class=\"imageTitle\">Image-based material editing</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nTranslucent materials, like milk, wax, soap, skin and jade, are all around us.  Humans can perceive subtle differences in translucency when distinguishing between these materials based on their appearance. Yet, little is known about what image cues are being used by humans to achieve this discrimination, or how these image cues relate to way light scatters internally in translucent media.  The goal of this project was to develop a perceptually-motivated pipeline for translucency, contributing new light scattering representations, a better understanding of how human beings perceive translucent materials, and new material recognition algorithms to computer graphics and computer vision. The outcomes can be described in three parts.\n\n\nMeasuring translucent materials\nOne way to understand translucent appearance is to quantitatively measure the set of physical scattering material properties that occur in the world around us. These quantitative models can then be used to predict the appearance of translucent media by simulating light transport in different geometries, lighting and scenes.\nThe key challenge of acquiring the optical scattering properties of translucent materials is that they cannot be measured directly.  Measuring optical properties requires sending photons toward an object and using cameras to record where they arrive after interacting with the object. Photons take different paths, each undergoing a different combination of scattering, reflection, and refraction inside the object or at its surface. What makes measurement challenging is that each of a camera?s pixels records a sum of many such photons conflated together: to recover the material properties, one must \"undo\" this conflation, which requires determining which paths were travelled by the measured photons, and what events occurred along the paths.\nIn this project, we established a foundation for solving these kinds of difficult measurement problems, leading to papers in SIGGRAPH Asia 2013, SIGGRAPH 14, SIGGRAPH 2015, and ECCV 2016. These papers  developed mathematical and computational tools for searching the space of possible material properties for those that match any acquired set of photographs. We developed  an acquisition system that exploits the wave nature of light to produce a richer set of images than conventional photographs: images that not only record where photons arrive but also the precise length of the paths that they travel between source and camera. Together, our theory and acquisition system create opportunities for measuring optical material properties with unprecedented generality, precision, and accuracy.\n\n \nPerception\nOur ability to effectively design, edit, and render translucent materials like milk, soap, and wax depends on our understanding of how humans perceive translucency. While it is clear that human observers care greatly about translucent appearance in many cases -- as when they distinguish milk from cream, or marble from Formica -- even basic questions about how this occurs remain unanswered.\nDuring the award period we performed some of the first qualitative and quantitative analyses of the perception of translucent materials. Using a unique combination of large-scale simulations, psychophysical experiments with human subjects, and data-mining, we studied the perception of objects that are composed of spatially-homogeneous material. We were able to unveil qualitative and quantitative models of how an object?s appearance is related to the physical properties of the material, and how both of these are related to how humans perceive translucent materials. These results were published in Transactions on Graphics 2013, Journal of Vision 2015, and Computer Vision and Pattern Recognition 2015. These projects lay the groundwork for better understanding how the human brain recognizes and understands translucent materials.\n\n\nApplications: recognition/rendering/editing\n\nOur understanding of translucency enables many applications in computer graphics and computer vision. We have developed new material recognition algorithms and material editing applications to modify translucency in images. These tools let designers intuitively  change the appearance of materials in images using simple image-based editing operations. These results were published in Computer Vision and Pattern Recognition 2015 and Transactions on Graphics 2015.\n\n\t\t\t\t\tLast Modified: 02/06/2017\n\n\t\t\t\t\tSubmitted by: Todd Zickler"
 }
}