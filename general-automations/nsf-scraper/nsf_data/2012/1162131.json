{
 "awd_id": "1162131",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC: Medium: Collaborative Research: Haptic Display of Terrain Characteristics and its Application in Virtual and Physical Worlds",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2017-09-30",
 "tot_intn_awd_amt": 147427.0,
 "awd_amount": 194681.0,
 "awd_min_amd_letter_date": "2012-09-04",
 "awd_max_amd_letter_date": "2016-05-25",
 "awd_abstract_narration": "The PIs' goal in this research is to realistically display terrain in an immersive Virtual Reality (VR) locomotion interface, based upon modification of the foot/terrain interaction coupled with graphical and auditory display of the terrain and user interaction.  Project outcomes will include novel \"smart shoe\" technology capable of sensing and modifying the terrain perceived by the wearer at each step so that terrain slope, surface stiffness, height variations, slip and balance can be actively controlled.  The approach is based upon an instrumented shoe sole with a directionally compliant structure using controllable bladders and embedded sensors to regulate terrain effects as the user walks.  The design and control of the shoe will be based upon dynamic biomechanical and terrain interaction models.  Subject data will provide a baseline for design, verification, and validation of the system.  A robotic test-bed will validate shoe response characteristics prior to subject evaluations.\r\n\r\nThe platform for this work will be the existing TreadPort Active Wind Tunnel (TPAWT), which is capable of realistically displaying locomotion environments on varying slopes as well as providing controllable wind, heat, and odor display.  The cave-like display of the TPAWT will be converted to a three dimensional stereo graphics system with seamless floor projection in order to present local terrain features such as shape, height variations, and surface texture.  Combined representation, interpretation, and coordination of the graphical and physical artifacts will be considered with the aim of creating an immersive and realistic locomotion experience with the end goal of achieving practical application. This combined system is termed the TPAWT Terrain Display System (TPAWT-TDS).\r\n\r\nThe target test group for the new technology will be patients with Parkinson's Disease, (PD), for whom VR training has already shown some promising results for improving gait characteristics and reducing the likelihood of falls.  Survey data from PD patients will motivate selection of the specific terrain.  Regular and PD users will first be evaluated on physical mockups of the terrain, which will then be recreated and evaluated in follow-up trials in the VR environment.  Once validated, the VR terrain display will be used for PD training.  Users will again be evaluated on the physical mockups to evaluate gait and balance performance, which will also be compared to untrained subjects.\r\n\r\nBroader Impacts:  The \"smart shoe\" technology to be created in this project will allow exploration of new and sophisticated methods for combining 3D graphical terrain cues with an actively changing physical terrain in a novel VR interface.  The resulting environment will have myriad potential applications as a rehabilitation and training tool, not the least of which is improved locomotion and fall prevention (since falls are the single most costly form of injury today).  Development of the new technology will be combined with rigorous human participant studies.  Research findings will be disseminated via websites and at major conferences, and also integrated into the robotics, virtual reality, ergonomics, and physical therapy curricula.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Willemsen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peter Willemsen",
   "pi_email_addr": "willemsn@d.umn.edu",
   "nsf_id": "000062928",
   "pi_start_date": "2012-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Minnesota Duluth",
  "inst_street_address": "1049 UNIVERSITY DRIVE 209 DARLAND",
  "inst_street_address_2": "",
  "inst_city_name": "DULUTH",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "2187267582",
  "inst_zip_code": "558123011",
  "inst_country_name": "United States",
  "cong_dist_code": "08",
  "st_cong_dist_code": "MN08",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MINNESOTA",
  "org_prnt_uei_num": "",
  "org_uei_num": "LPCTM8BS8NF3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Minnesota Duluth",
  "perf_str_addr": "320 HH 1114 Kirby Dr",
  "perf_city_name": "Duluth",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "558122496",
  "perf_ctry_code": "US",
  "perf_cong_dist": "08",
  "perf_st_cong_dist": "MN08",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 80015.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 75288.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 13126.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 13126.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 13126.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px Menlo; color: #000000; background-color: #ffffff} p.p2 {margin: 0.0px 0.0px 0.0px 0.0px; font: 11.0px Menlo; color: #000000; background-color: #ffffff; min-height: 13.0px} span.s1 {font-variant-ligatures: no-common-ligatures} -->\n<p class=\"p1\">The aim of this research is to realistically display terrain in an immersive virtual reality locomotion interface based upon modification of the foot/terrain interaction. This is a collaborative project conducted by the University of Utah and the University of Minnesota Duluth (UMD). Our efforts have been to display the terrain visually to a user to create an interactive simulation while also predicting when a person's foot would impact a virtual terrain surface to allow a smart shoe to simulate the feel of standing on uneven terrain. At UMD, our primary outcomes were to (1) prototype a visual terrain rendering system capable of simulating the look of realistic looking outdoor spaces to a user, and (2) prototype the communication and interaction between the visual terrain system and a smart shoe capable of providing feedback to the user to simulate the uneven terrain underfoot. UMD supported the project's broader impacts by engaging numerous undergraduate researchers through NSF REUs, supporting graduate student research and highlighting our project's results to hundreds of people at large, public outreach events at UMD, such as UMD's Astronomy Day each year.</p>\n<p class=\"p1\">The intellectual merits of this project relate to the development of smart shoe technology capable of sensing and modifying the terrain perceived by the wearer such that terrain slope, surface stiffness, height variations, slip and balance can be actively controlled during each step. The Utah team was the primary lead for the development, evaluation and user testing of the smart shoe technology. The smart shoe is a specialized shoe device that contains several flexible, yet stiff bladders in the sole of the shoe that can be controlled wirelessly by a computer each time a user steps. The bladders in the sole can be deflated proportional to the features present on the virtual terrain to give the sense of stepping on different physical features. As the virtual terrain changes under the sole of the shoe, the terrain features where the shoe will impact the virtual ground are communicated to the shoe to allow the bladders to change dynamically as the user places their foot on the actual floor. The outcome for the user is a sense of stepping on objects that exist virtually. Moreover, users' ankle angles will change, mimicking the biomechanics people might naturally experience when stepping on uneven terrain in the real world.</p>\n<p class=\"p1\">At Utah, the research team evaluated the terrain display system within the existing virtual reality device, the TreadPort Active Wind Tunnel (TPAWT), which is capable of realistically displaying locomotion environments on varying slopes as well as providing controllable wind, heat, and odor display. The virtual reality display of the TPAWT simulates three dimensional, stereo images to the user as they walk through virtual environments. While wearing the smart shoe on the TPAWT, users can also experience the unevenness of virtual, local terrain features, such as shape, height variations, and surface texture.</p>\n<p class=\"p1\">At UMD, our primary efforts were to develop the initial visual terrain rendering system and prototype the communication between the visual terrain system and the smart shoe bladder control system. We developed an L-shaped stereo projection display system using short-throw stereo projectors. The projectors were arranged such that one projector provided the frontal view to the user in a portrait mode from the floor up to about seven feet. The second projector projects directly onto the floor surrounding the user. Together, both projectors form a seamless, L-shaped view into a virtual space.&nbsp; Users make a walk-in-place, marching gesture in which they pretended to walk while staying in the same location in the physical space to navigate the virtual environment. A Microsoft Kinect v2 sensor detected the user's marching gesture and a walk-in-place locomotion algorithm moved the user through the virtual landscape. Users could turn left and right by swinging their arms away from their body up to their sides, respectively, with the rate of the turn determined by how high the arm was raised from the users side.</p>\n<p class=\"p1\">Visual terrain rendering was completed using the Unreal Engine game engine to render complex scenes. We created a large-scale terrain simulation of Mars based on data acquired from NASA's HiRISE imager. Our software can load GIS-based digital elevation models and creates a visual representation that the user is able to navigate through using virtual reality devices, such as the TPAWT and our L-shaped display. To provide motivation to walk across the terrain, the Mars simulation is interactive and contains a habitat building exercise to encourage walking on different terrain slopes. This software was provided to Utah and has been adapted and modified for further studies and investigation in potential uses of the smart shoe for virtual reality haptic interfaces as well as in rehabilitation for people with Parkinson's Disease when the shoe is used in real world conditions.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/20/2018<br>\n\t\t\t\t\tModified by: Peter&nbsp;Willemsen</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532112582217_ASTRO_0416_AstroDay(136of428)sm--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532112582217_ASTRO_0416_AstroDay(136of428)sm--rgov-800width.jpg\" title=\"L-shaped Locomotion Display\"><img src=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532112582217_ASTRO_0416_AstroDay(136of428)sm--rgov-66x44.jpg\" alt=\"L-shaped Locomotion Display\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">L-Shaped Locomotion Display at the University of Minnesota Duluth</div>\n<div class=\"imageCredit\">Valerie Coit</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Peter&nbsp;Willemsen</div>\n<div class=\"imageTitle\">L-shaped Locomotion Display</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532112892740_ScreenShot2018-03-20at9.05.29AM--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532112892740_ScreenShot2018-03-20at9.05.29AM--rgov-800width.jpg\" title=\"Rendering of Mars simulation\"><img src=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532112892740_ScreenShot2018-03-20at9.05.29AM--rgov-66x44.jpg\" alt=\"Rendering of Mars simulation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Rendering of the Noctis Labrythnus  region of Mars using our terrain rendering software within the Unreal Engine.</div>\n<div class=\"imageCredit\">Pete Willemsen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Peter&nbsp;Willemsen</div>\n<div class=\"imageTitle\">Rendering of Mars simulation</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532113517015_ASTRO_0416_AstroDay(245of428)sm--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532113517015_ASTRO_0416_AstroDay(245of428)sm--rgov-800width.jpg\" title=\"Interacting with the Mars Simulation\"><img src=\"/por/images/Reports/POR/2018/1162131/1162131_10210545_1532113517015_ASTRO_0416_AstroDay(245of428)sm--rgov-66x44.jpg\" alt=\"Interacting with the Mars Simulation\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Interacting with the Mars Simulation on the L-shaped display at the University of Minnesota Duluth's Astronomy Day.</div>\n<div class=\"imageCredit\">Valerie Coit</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Peter&nbsp;Willemsen</div>\n<div class=\"imageTitle\">Interacting with the Mars Simulation</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe aim of this research is to realistically display terrain in an immersive virtual reality locomotion interface based upon modification of the foot/terrain interaction. This is a collaborative project conducted by the University of Utah and the University of Minnesota Duluth (UMD). Our efforts have been to display the terrain visually to a user to create an interactive simulation while also predicting when a person's foot would impact a virtual terrain surface to allow a smart shoe to simulate the feel of standing on uneven terrain. At UMD, our primary outcomes were to (1) prototype a visual terrain rendering system capable of simulating the look of realistic looking outdoor spaces to a user, and (2) prototype the communication and interaction between the visual terrain system and a smart shoe capable of providing feedback to the user to simulate the uneven terrain underfoot. UMD supported the project's broader impacts by engaging numerous undergraduate researchers through NSF REUs, supporting graduate student research and highlighting our project's results to hundreds of people at large, public outreach events at UMD, such as UMD's Astronomy Day each year.\nThe intellectual merits of this project relate to the development of smart shoe technology capable of sensing and modifying the terrain perceived by the wearer such that terrain slope, surface stiffness, height variations, slip and balance can be actively controlled during each step. The Utah team was the primary lead for the development, evaluation and user testing of the smart shoe technology. The smart shoe is a specialized shoe device that contains several flexible, yet stiff bladders in the sole of the shoe that can be controlled wirelessly by a computer each time a user steps. The bladders in the sole can be deflated proportional to the features present on the virtual terrain to give the sense of stepping on different physical features. As the virtual terrain changes under the sole of the shoe, the terrain features where the shoe will impact the virtual ground are communicated to the shoe to allow the bladders to change dynamically as the user places their foot on the actual floor. The outcome for the user is a sense of stepping on objects that exist virtually. Moreover, users' ankle angles will change, mimicking the biomechanics people might naturally experience when stepping on uneven terrain in the real world.\nAt Utah, the research team evaluated the terrain display system within the existing virtual reality device, the TreadPort Active Wind Tunnel (TPAWT), which is capable of realistically displaying locomotion environments on varying slopes as well as providing controllable wind, heat, and odor display. The virtual reality display of the TPAWT simulates three dimensional, stereo images to the user as they walk through virtual environments. While wearing the smart shoe on the TPAWT, users can also experience the unevenness of virtual, local terrain features, such as shape, height variations, and surface texture.\nAt UMD, our primary efforts were to develop the initial visual terrain rendering system and prototype the communication between the visual terrain system and the smart shoe bladder control system. We developed an L-shaped stereo projection display system using short-throw stereo projectors. The projectors were arranged such that one projector provided the frontal view to the user in a portrait mode from the floor up to about seven feet. The second projector projects directly onto the floor surrounding the user. Together, both projectors form a seamless, L-shaped view into a virtual space.  Users make a walk-in-place, marching gesture in which they pretended to walk while staying in the same location in the physical space to navigate the virtual environment. A Microsoft Kinect v2 sensor detected the user's marching gesture and a walk-in-place locomotion algorithm moved the user through the virtual landscape. Users could turn left and right by swinging their arms away from their body up to their sides, respectively, with the rate of the turn determined by how high the arm was raised from the users side.\nVisual terrain rendering was completed using the Unreal Engine game engine to render complex scenes. We created a large-scale terrain simulation of Mars based on data acquired from NASA's HiRISE imager. Our software can load GIS-based digital elevation models and creates a visual representation that the user is able to navigate through using virtual reality devices, such as the TPAWT and our L-shaped display. To provide motivation to walk across the terrain, the Mars simulation is interactive and contains a habitat building exercise to encourage walking on different terrain slopes. This software was provided to Utah and has been adapted and modified for further studies and investigation in potential uses of the smart shoe for virtual reality haptic interfaces as well as in rehabilitation for people with Parkinson's Disease when the shoe is used in real world conditions.\n\n\t\t\t\t\tLast Modified: 07/20/2018\n\n\t\t\t\t\tSubmitted by: Peter Willemsen"
 }
}