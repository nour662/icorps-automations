{
 "awd_id": "1248047",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Collaborative Research: Towards Modeling Human Speech Confusions in Noise",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 100000.0,
 "awd_amount": 100000.0,
 "awd_min_amd_letter_date": "2012-08-08",
 "awd_max_amd_letter_date": "2012-08-08",
 "awd_abstract_narration": "This EArly-concept Grant for Exploratory Research (EAGER) supports an exploratory study to evaluate model components for prediction of human speech recognition in the presence of noise. Such a model has the potential to predict confusions between fine phonetic distinctions in different levels of background noise and at different speaking rates. The study takes advantage of modern physiological results that indicate that the primary auditory cortex performs spectro-temporal filtering; that is, that there are cells that are sensitive to particular spectro-temporal modulations at each auditory frequency. In this project, perceptual experiments in the presence of both stationary and non-stationary additive noise and at different signal-to-noise ratios for a database of CVC syllables recorded at 2 different speaking rates yield confusion statistics. These statistics are then compared to those resulting from an auditory model enhanced by elements incorporating these spectro-temporal filters. \r\n\r\nSuccessful results from this study will suggest enhancements to current hearing models and ultimately, after a broader study for which this EAGER is a pilot, advance the understanding of human speech perception. Background noise presents a challenging problem for a variety of speech and hearing devices including hearing aids and automatic speech recognition (ASR) systems. Since normal-hearing human listeners are extremely adept at perceiving speech in noise, this improved understanding of human models could lead to better artificial systems for speech processing. The databases and tools developed for this study will be disseminated to the research community.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Nelson",
   "pi_last_name": "Morgan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Nelson Morgan",
   "pi_email_addr": "morgan@icsi.berkeley.edu",
   "nsf_id": "000185837",
   "pi_start_date": "2012-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "International Computer Science Institute",
  "inst_street_address": "2150 SHATTUCK AVE",
  "inst_street_address_2": "SUITE 250",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106662900",
  "inst_zip_code": "947041345",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "INTERNATIONAL COMPUTER SCIENCE INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GSRMP1QCXU74"
 },
 "perf_inst": {
  "perf_inst_name": "International Computer Science Institute",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947041198",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 100000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A collaboration between the Speech Processing and Auditory Perception Laboratory at UCLA and the Speech Group at ICSI focused on the refinement of the simple models used in Automatic Speech Recognition (ASR) with representations that have been developed from observations of mammalian auditory physiology. While no animal experiments were performed in this study, earlier work reported by researchers at the University of Maryland suggested that auditory systems were particularly sensitive to particular ranges of modulations over time and frequency. Processing techniques based on these observations have proved useful in many previous ASR experiments. In this study, though, the goal was to see if incorporating these insights into ASR approaches would yield a pattern of errors (for consonants in Consonant-Vowel-Consonant&nbsp; (CVC) syllables) that was more similar to what would be observed for human perception.</p>\n<p>More specifically, we wanted to see if correlations with the pattern of human perceptual errors for CVC syllables for noisy and rapid speech were improved by using modulation-based features for a modern ASR system. To explore this, the UCLA team recorded a corpus of spoken CVC syllables, and then conducted listening tests and analyzed results to determine the pattern of errors for the listeners. These results were then passed to the ICSI team, who developed the ASR systems and compared results with the perceptual ones.</p>\n<p>One result in particular was quite notable, mainly, that higher temporalmodulations (i.e., components of the speech where the spectral content was varying quickly) were sometimes more helpful than including information from the entire range of temporal modulations. The most prominent of these results occurred for rapid speech, which could be expected since the spectral content varies more quickly in this case. But significant improvement in correlation with human consonant perception was really only observed for noisy speech. But for speech recognition researchers, in an era where the efficacy of machine learning approaches is the focus of much attention, it should be interesting that restricting the observed features (to higher modulations) gives improved performance for noisy and rapid speech.</p>\n<p>The table below summarizes these results.</p>\n<p>&nbsp;</p>\n<table border=\"1\" cellspacing=\"0\" cellpadding=\"0\">\n<tbody>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Testing condition</p>\n</td>\n<td width=\"90\" valign=\"top\">\n<p>Machine correlation with perception,   baseline model</p>\n</td>\n<td width=\"112\" valign=\"top\">\n<p>Machine correlation with perception,</p>\n<p>all modulation features</p>\n</td>\n<td width=\"120\" valign=\"top\">\n<p>Machine correlation with perception,<br /> high modulations only</p>\n</td>\n</tr>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Clean, all speech</p>\n</td>\n<td width=\"90\" valign=\"top\">\n<p>0.96</p>\n</td>\n<td width=\"112\" valign=\"top\">\n<p>0.93</p>\n</td>\n<td width=\"120\" valign=\"top\">\n<p>0.95</p>\n</td>\n</tr>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Clean, rapid speech</p>\n</td>\n<td width=\"90\" valign=\"top\">\n<p>0.94</p>\n</td>\n<td width=\"112\" valign=\"top\">\n<p>0.92</p>\n</td>\n<td width=\"120\" valign=\"top\">\n<p>0.94</p>\n</td>\n</tr>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Clean, slow speech</p>\n</td>\n<td width=\"90\" valign=\"top\">\n<p>0.94</p>\n</td>\n<td width=\"112\" valign=\"top\">\n<p>0.92</p>\n</td>\n<td width=\"120\" valign=\"top\">\n<p>0.95</p>\n</td>\n</tr>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Noisy, all speech</p>\n</td>\n<td width=\"90\" valign=\"top\">\n<p>0.24</p>\n</td>\n<td width=\"112\" valign=\"top\">\n<p>0.22</p>\n</td>\n<td width=\"120\" valign=\"top\">\n<p>0.28</p>\n</td>\n</tr>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Noisy, rapid speech</p>\n</td>\n<td width=\"90\" valign=\"top\">\n<p>0.20</p>\n</td>\n<td width=\"112\" valign=\"top\">\n<p>0.26</p>\n</td>\n<td width=\"120\" valign=\"top\">\n<p>0.31</p>\n</td>\n</tr>\n<tr>\n<td width=\"90\" valign=\"top\">\n<p>Noisy, slow speech</p>\n</td>\n<td width=\"90\" val...",
  "por_txt_cntn": "\nA collaboration between the Speech Processing and Auditory Perception Laboratory at UCLA and the Speech Group at ICSI focused on the refinement of the simple models used in Automatic Speech Recognition (ASR) with representations that have been developed from observations of mammalian auditory physiology. While no animal experiments were performed in this study, earlier work reported by researchers at the University of Maryland suggested that auditory systems were particularly sensitive to particular ranges of modulations over time and frequency. Processing techniques based on these observations have proved useful in many previous ASR experiments. In this study, though, the goal was to see if incorporating these insights into ASR approaches would yield a pattern of errors (for consonants in Consonant-Vowel-Consonant  (CVC) syllables) that was more similar to what would be observed for human perception.\n\nMore specifically, we wanted to see if correlations with the pattern of human perceptual errors for CVC syllables for noisy and rapid speech were improved by using modulation-based features for a modern ASR system. To explore this, the UCLA team recorded a corpus of spoken CVC syllables, and then conducted listening tests and analyzed results to determine the pattern of errors for the listeners. These results were then passed to the ICSI team, who developed the ASR systems and compared results with the perceptual ones.\n\nOne result in particular was quite notable, mainly, that higher temporalmodulations (i.e., components of the speech where the spectral content was varying quickly) were sometimes more helpful than including information from the entire range of temporal modulations. The most prominent of these results occurred for rapid speech, which could be expected since the spectral content varies more quickly in this case. But significant improvement in correlation with human consonant perception was really only observed for noisy speech. But for speech recognition researchers, in an era where the efficacy of machine learning approaches is the focus of much attention, it should be interesting that restricting the observed features (to higher modulations) gives improved performance for noisy and rapid speech.\n\nThe table below summarizes these results.\n\n \n\n\n\n\n\nTesting condition\n\n\n\nMachine correlation with perception,   baseline model\n\n\n\nMachine correlation with perception,\n\nall modulation features\n\n\n\nMachine correlation with perception,\n high modulations only\n\n\n\n\n\nClean, all speech\n\n\n\n0.96\n\n\n\n0.93\n\n\n\n0.95\n\n\n\n\n\nClean, rapid speech\n\n\n\n0.94\n\n\n\n0.92\n\n\n\n0.94\n\n\n\n\n\nClean, slow speech\n\n\n\n0.94\n\n\n\n0.92\n\n\n\n0.95\n\n\n\n\n\nNoisy, all speech\n\n\n\n0.24\n\n\n\n0.22\n\n\n\n0.28\n\n\n\n\n\nNoisy, rapid speech\n\n\n\n0.20\n\n\n\n0.26\n\n\n\n0.31\n\n\n\n\n\nNoisy, slow speech\n\n\n\n0.24\n\n\n\n0.19\n\n\n\n0.24\n\n\n\n\n\n\t\t\t\t\tLast Modified: 10/28/2015\n\n\t\t\t\t\tSubmitted by: Nelson Morgan"
 }
}