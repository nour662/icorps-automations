{
 "awd_id": "1155251",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Doctoral Dissertation Research: Evaluating the influence of Daubert's cross-examination safeguard on jurors', attorneys', and judges' judgments about scientific evidence",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "susan sterett",
 "awd_eff_date": "2012-04-01",
 "awd_exp_date": "2014-03-31",
 "tot_intn_awd_amt": 14934.0,
 "awd_amount": 14934.0,
 "awd_min_amd_letter_date": "2012-03-26",
 "awd_max_amd_letter_date": "2012-03-26",
 "awd_abstract_narration": "In Daubert v. Merrell Dow Pharmaceuticals, the Supreme Court clarified that trial judges were responsible for serving as evidentiary gatekeepers for scientific evidence.  When judges fail at this gatekeeping role and admit unreliable expert testimony, the task of identifying and assessing flawed scientific testimony becomes the job of attorneys during cross-examination and jurors during trial. The purpose of the proposed studies is to determine whether judges, attorneys, and jurors are sensitive to threats to scientific validity, to examine the effectiveness of scientifically informed cross-examinations for educating jurors about sophisticated validity threats, and to assess the ability of attorneys and judges to develop questions that would elicit information about scientific validity.  \r\n\r\nStudy 1 examines the utility of cross-examination to educate jurors about threats to scientific validity.  Jurors will view a videotaped trial, render a verdict, and provide evaluations of the expert's testimony.  In Study 2, the researchers will examine whether judges and attorneys are capable of fulfilling Daubert requirements in their evaluation of scientific quality and the development of questions that might highlight concepts of scientific validity or reliability.  Judges and attorneys will read a case summary and develop questions intended to assess the reliability and validity of an expert's testimony.  \r\n\r\nThis research has the potential to increase our understanding of the Court's assumptions in Daubert.  Further, this research will address a critical issue regarding the effectiveness of Daubert?s safeguards that has not been previously addressed, which is to examine whether attorneys and judges have developed the skills necessary to elicit information about scientific reliability and validity during questioning and cross-examination.  Further, knowledge of whether cross-examination can be an effective safeguard and whether attorneys and judges are capable of developing effective cross-examination questions may lead to changes in continuing legal education about scientific evidence.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Margaret",
   "pi_last_name": "Bull Kovera",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Margaret Bull Kovera",
   "pi_email_addr": "mkovera@jjay.cuny.edu",
   "nsf_id": "000250770",
   "pi_start_date": "2012-03-26",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jacqueline",
   "pi_last_name": "Austin",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Jacqueline L Austin",
   "pi_email_addr": "jacquelineaustin09@gmail.com",
   "nsf_id": "000598584",
   "pi_start_date": "2012-03-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CUNY John Jay College of Criminal Justice",
  "inst_street_address": "524 W 59TH ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2122378449",
  "inst_zip_code": "100191007",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NY12",
  "org_lgl_bus_name": "RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "NGK8GHNABTB8"
 },
 "perf_inst": {
  "perf_inst_name": "CUNY John Jay College of Criminal Justice",
  "perf_str_addr": "445 W. 59th Street",
  "perf_city_name": "New York",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100191104",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "137200",
   "pgm_ele_name": "LSS-Law And Social Sciences"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9179",
   "pgm_ref_txt": "GRADUATE INVOLVEMENT"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 14934.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Expert testimony is intended to help jurors understand scientific, technical, or otherwise complex evidence. Expert testimony can be controversial, however, because of concerns about its quality. Although the Supreme Court clarified judges&rsquo; role as gatekeepers of scientific evidence, judges may lack the skills necessary to evaluate scientific quality (Gatowski et al., 2001; Kovera &amp; McAuliff, 2000) and may admit testimony and opinions derived from invalid science into evidence (Kovera &amp; McAuliff, 2000). When judges fail to recognize flawed science, attorneys may aid the judge in determining scientific quality by filing a motion to exclude the testimony.&nbsp; For attorneys to successfully assist judges, attorneys must be sufficiently knowledgeable about scientific methodology to identify threats to validity and reliability, but attorneys may fail to identify methodological problems in scientific studies (Kovera &amp; McAuliff, 2002).&nbsp; Jurors also struggle with scientific concepts and evidence evaluation (Levett, 2008, 2009; McAuliff &amp; Duckworth, 2010; McAuliff &amp; Kovera, 2008; McAuliff et al., 2009). Although cross-examination has demonstrated promise for educating jurors about simple threats to scientific validity (Austin &amp; Kovera, under review),&nbsp; questions remain unanswered about the ability of cross-examination to increase juror sensitivity to variations in scientific methodology when the threat is more complex.</p>\n<p>The PIs conducted two studies to empirically examine whether cross-examination helps jurors evaluate scientific evidence&nbsp; In the first study, they examined whether judges and attorneys are capable of evaluating the quality of scientific evidence and of developing questions that could help jurors evaluate scientific validity or reliability.&nbsp; Ninety-five attorneys and 111 judges read a fact pattern from a civil trial that included expert testimony about the intelligence of the plaintiff. The PIs varied both the validity and reliability of the intelligence test the expert administered to the plaintiff.&nbsp; Psychological tests can vary in both validity and reliability.&nbsp; Validity refers to the how well an instrument measures what it intends to measure (e.g., I.Q. tests measure cognitive ability, personality tests measures personality characteristics, scales measures weight) and reliability refers to consistency of measurement.&nbsp; According to the trial testimony,&nbsp; the test was either valid and reliable, invalid due to experimenter bias but reliable, or the test had relatively low reliability indices on the full scale internal consistency score, the test/retest score, and the inter-observer reliability scores.&nbsp;</p>\n<p>The results suggest that judges are insensitive to variations in scientific reliability and validity.&nbsp; Judges were equally likely to admit good scientific evidence as they were bad scientific evidence and did not provide higher ratings of scientific quality for valid and reliable scientific testimony than they did for invalid or unreliable testimony.&nbsp; Thus, judges may not serve as effective gatekeepers of scientific evidence.&nbsp; Attorneys provided lower ratings of scientific quality when the testimony was unreliable but did not provide lower ratings of scientific quality when the testimony was invalid.&nbsp; Moreover, attorneys reported that they would be more likely to move to exclude unreliable testimony than they would invalid or valid testimony.&nbsp; Although attorneys provided lower ratings of scientific quality for unreliable testimony, they did not develop cross-examination questions that addressed issues of reliability to help educate jurors.&nbsp; Attorneys did, however, formulate questions about scientific validity.&nbsp; These findings suggest that attorneys may serve as an effective buffer against the admission of unreliable expert testimony but that...",
  "por_txt_cntn": "\nExpert testimony is intended to help jurors understand scientific, technical, or otherwise complex evidence. Expert testimony can be controversial, however, because of concerns about its quality. Although the Supreme Court clarified judges\u00c6 role as gatekeepers of scientific evidence, judges may lack the skills necessary to evaluate scientific quality (Gatowski et al., 2001; Kovera &amp; McAuliff, 2000) and may admit testimony and opinions derived from invalid science into evidence (Kovera &amp; McAuliff, 2000). When judges fail to recognize flawed science, attorneys may aid the judge in determining scientific quality by filing a motion to exclude the testimony.  For attorneys to successfully assist judges, attorneys must be sufficiently knowledgeable about scientific methodology to identify threats to validity and reliability, but attorneys may fail to identify methodological problems in scientific studies (Kovera &amp; McAuliff, 2002).  Jurors also struggle with scientific concepts and evidence evaluation (Levett, 2008, 2009; McAuliff &amp; Duckworth, 2010; McAuliff &amp; Kovera, 2008; McAuliff et al., 2009). Although cross-examination has demonstrated promise for educating jurors about simple threats to scientific validity (Austin &amp; Kovera, under review),  questions remain unanswered about the ability of cross-examination to increase juror sensitivity to variations in scientific methodology when the threat is more complex.\n\nThe PIs conducted two studies to empirically examine whether cross-examination helps jurors evaluate scientific evidence  In the first study, they examined whether judges and attorneys are capable of evaluating the quality of scientific evidence and of developing questions that could help jurors evaluate scientific validity or reliability.  Ninety-five attorneys and 111 judges read a fact pattern from a civil trial that included expert testimony about the intelligence of the plaintiff. The PIs varied both the validity and reliability of the intelligence test the expert administered to the plaintiff.  Psychological tests can vary in both validity and reliability.  Validity refers to the how well an instrument measures what it intends to measure (e.g., I.Q. tests measure cognitive ability, personality tests measures personality characteristics, scales measures weight) and reliability refers to consistency of measurement.  According to the trial testimony,  the test was either valid and reliable, invalid due to experimenter bias but reliable, or the test had relatively low reliability indices on the full scale internal consistency score, the test/retest score, and the inter-observer reliability scores. \n\nThe results suggest that judges are insensitive to variations in scientific reliability and validity.  Judges were equally likely to admit good scientific evidence as they were bad scientific evidence and did not provide higher ratings of scientific quality for valid and reliable scientific testimony than they did for invalid or unreliable testimony.  Thus, judges may not serve as effective gatekeepers of scientific evidence.  Attorneys provided lower ratings of scientific quality when the testimony was unreliable but did not provide lower ratings of scientific quality when the testimony was invalid.  Moreover, attorneys reported that they would be more likely to move to exclude unreliable testimony than they would invalid or valid testimony.  Although attorneys provided lower ratings of scientific quality for unreliable testimony, they did not develop cross-examination questions that addressed issues of reliability to help educate jurors.  Attorneys did, however, formulate questions about scientific validity.  These findings suggest that attorneys may serve as an effective buffer against the admission of unreliable expert testimony but that jurors may be exposed to invalid scientific testimony because neither judges nor attorneys identified these scientific flaws.  These data also suggest that although..."
 }
}