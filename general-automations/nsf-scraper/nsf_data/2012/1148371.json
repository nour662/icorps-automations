{
 "awd_id": "1148371",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: SI2-SSI: A Comprehensive Performance Tuning Framework for the MPI Stack",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rob Beverly",
 "awd_eff_date": "2012-06-01",
 "awd_exp_date": "2016-05-31",
 "tot_intn_awd_amt": 1251644.0,
 "awd_amount": 1251644.0,
 "awd_min_amd_letter_date": "2012-06-04",
 "awd_max_amd_letter_date": "2012-06-04",
 "awd_abstract_narration": "The Message Passing Interface (MPI) is a very widely used parallel programming model on modern High-End Computing (HEC) systems. Many performance aspects of MPI libraries, such as latency, bandwidth, scalability, memory footprint, cache pollution, overlap of computation and communication etc. are highly dependent on system configuration and application requirements. Additionally, modern clusters are changing rapidly with the growth of multi-core processors and commodity networking technologies such as InfiniBand and 10GigE/iWARP. They are becoming diverse and heterogeneous with varying number of processor cores, processor speed, memory speed, multi-generation network adapters/switches, I/O interface technologies, and accelerators (GPGPUs), etc.  Typically, any MPI library deals with the above kind of diversity in platforms and sensitivity of applications by employing various runtime parameters. These parameters are tuned during its release, or by\r\nsystem administrators, or by end-users.  These default parameters may or may not be optimal for all system configurations and applications.\r\n\r\nThe MPI library of a typical proprietary system goes through heavy performance tuning for a range of applications.  Since commodity clusters provide greater flexibility in their configurations (processor, memory and network), it is very hard to achieve optimal tuning using released version of any MPI library, with its default settings. This leads to the following broad challenge: \"Can a comprehensive performance tuning framework be designed for MPI library so that the next generation InfiniBand, 10GigE/iWARP and RoCE clusters and applications will be able to extract `bare-metal' performance and maximum scalability?\"  The investigators, involving computer\r\nscientists from The Ohio State University (OSU) and Ohio Supercomputer Center (OSC) as well as computational scientists from the Texas Advanced Computing Center (TACC) and San Diego Supercomputer Center (SDSC), University of California San Diego (UCSD), will be addressing the above challenge with innovative solutions.\r\n\r\nThe investigators will specifically address the following challenges: 1) Can a set of static tools be designed to optimize performance of an MPI library during installation time?  2) Can a set of dynamic tools with low overhead be designed to optimize performance on a per-user and per-application basis during production runs?  3) How to incorporate the proposed performance tuning framework with the upcoming MPIT interface?  4) How to configure MPI libraries on a given system to deliver different optimizations to a set of driving applications?  and 5) What kind of benefits (in terms of performance, scalability, memory efficiency and reduction in cache pollution) can be achieved by the proposed tuning framework?  The research will be driven by a set of applications from established NSF computational science researchers running large scale simulations on the TACC Ranger and other systems at OSC, SDSC and OSU.  The proposed designs will be integrated into the open-source MVAPICH2 library.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dhabaleswar",
   "pi_last_name": "Panda",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Dhabaleswar K Panda",
   "pi_email_addr": "panda.2@osu.edu",
   "nsf_id": "000487085",
   "pi_start_date": "2012-06-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Tomko",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Karen A Tomko",
   "pi_email_addr": "ktomko@osc.edu",
   "nsf_id": "000330142",
   "pi_start_date": "2012-06-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Ohio State University",
  "inst_street_address": "1960 KENNY RD",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBUS",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "6146888735",
  "inst_zip_code": "432101016",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "OH03",
  "org_lgl_bus_name": "OHIO STATE UNIVERSITY, THE",
  "org_prnt_uei_num": "MN4MDDMN8529",
  "org_uei_num": "DLWBSLWAJWR1"
 },
 "perf_inst": {
  "perf_inst_name": "Ohio State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "432101206",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "OH03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  },
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8009",
   "pgm_ref_txt": "Scientifc Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 1251644.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The largest computers today use thousands of computing units (referred<br />to as cores) to run complex scientific simulations. Applications<br />running on these systems predominately use the Message Passing<br />Interface (MPI) programming model to exchange data between cores. The<br />cores are organized several cores per socket, and commonly two or four<br />sockets per server, with the servers connected by a high-performance<br />network, the system's interconnect. The focus of this project is<br />performance tuning of the MVAPICH2 MPI library, a high performance,<br />open-source implementation of the MPI standard that delivers the best<br />performance, scalability, and fault tolerance for high-end computing<br />systems and servers that use one of the following interconnect network<br />technologies: InfiniBand, Intel Omni-Path, 10-40GigE/iWARP or<br />RoCE. The overarching goal of the project is to reduce the time to<br />solution for complex science simulations which use MVAPICH2 or other<br />MPI libraries on these large systems.</p>\n<p>Most large computer systems (clusters) are built up from commodity<br />components, but the technology is changing rapidly with frequent<br />introduction of new multi-core processors and networking<br />technologies. Open source MPI libraries must run well across multiple<br />generations of processors and interconnects, and additionally support<br />accelerators such as NVIDIA GPGPUS and Intel Xeon Phi<br />processors. Typically, any MPI library deals with diverse platforms<br />and a wide range of applications by employing various runtime<br />parameters. These parameters are either set during software release,<br />by system administrators, or end-users. Considering the nature of<br />commodity clusters, it is difficult to apply one set of common tuning<br />parameters that will allow an MPI library to extract the best<br />performance on all computer systems. This leads to a broad challenge:<br />Can a comprehensive performance tuning framework be designed for MPI<br />libraries for modern commodity clusters?</p>\n<p>In this project we have addressed this challenge in three areas: 1)<br />Developed architecture specific tuning for MPI communication<br />operations between pairs of cores (point-to-point operations) or<br />groups of cores (collective operations).&nbsp; 2) Developed a performance<br />profiling layer within MVAPICH2, including support for the MPI-3<br />standard's MPI_T interface.&nbsp; 3) Studied application sensitivity to MPI<br />parameters and developed applications specific tuning strategies.</p>\n<p>Developments were made in all three areas and evaluated with a range<br />of science applications including: HoomdBlue, SMG2000, Neuron, Amber,<br />MiniAMR, MILC, LULESH and HPCCG. These scientific applications and<br />mathematical libraries are from biochemistry, neuroscience, high<br />energy physics, computational fluid dynamics, and numerical linear<br />algebra.&nbsp; Some highlights of these results are that<br />architecture-specific tuning improves collective operation latency by<br />up to 58%. The GPU tuning methods were able to provide a 2X<br />improvement in the execution time of HoomdBlue application. The<br />transport protocol-based tuning shows 27% improvement for the Neuron<br />application example. With HPCG and the LULESH application kernel, the<br />collective tuning for partial subscription available on hybrid<br />MPI+OpenMP programming model improves the execution time by 24% at 512 cores.</p>\n<p>The results of this research (tuned designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />the open-source MVAPICH2 library (1.9, 2.0, 2.1, and 2.2 release<br />series including alpha, beta and RC versions). The latest version is<br />currently running on many large-scale XSEDE systems including TACC<br />Stampede, SDSC Gordon and SDSC Comet. Currently, the MVAPICH2 library is being used by more than 2,650 organizations in 81 countries. The<br />MVAPICH2 library and the enhancements are being used by a large number<br />of users of these systems.</p>\n<p>In each of these releases, information about the tuned designs for<br />various components (such as point-to-point, collectives, GPU-GPU<br />communication, etc.) has been shared with the MVAPICH2 user community<br />through mailing lists. The applications-based tuning results have been<br />made available to the community through the \"Best Practices\" link of<br />the MVAPICH project web page. In order to achieve direct face-to-face<br />discussion with MVAPICH2 users and get their feedback, in 2013 we<br />started holding an MVAPICH2 User Group Meeting (MUG) each year in<br />August in Columbus Ohio. This meeting has been continuing successfully<br />for the last four years and has helped to disseminate the results of<br />this research to a wider community.&nbsp; In addition to the software<br />distribution and the MUG events, the results have been presented at<br />various conferences and events through talks and tutorials.&nbsp; Multiple<br />Ph.D and Masters students have performed research work and received<br />their Ph.D and M.S. degrees as a part of this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/30/2016<br>\n\t\t\t\t\tModified by: Dhabaleswar&nbsp;K&nbsp;Panda</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe largest computers today use thousands of computing units (referred\nto as cores) to run complex scientific simulations. Applications\nrunning on these systems predominately use the Message Passing\nInterface (MPI) programming model to exchange data between cores. The\ncores are organized several cores per socket, and commonly two or four\nsockets per server, with the servers connected by a high-performance\nnetwork, the system's interconnect. The focus of this project is\nperformance tuning of the MVAPICH2 MPI library, a high performance,\nopen-source implementation of the MPI standard that delivers the best\nperformance, scalability, and fault tolerance for high-end computing\nsystems and servers that use one of the following interconnect network\ntechnologies: InfiniBand, Intel Omni-Path, 10-40GigE/iWARP or\nRoCE. The overarching goal of the project is to reduce the time to\nsolution for complex science simulations which use MVAPICH2 or other\nMPI libraries on these large systems.\n\nMost large computer systems (clusters) are built up from commodity\ncomponents, but the technology is changing rapidly with frequent\nintroduction of new multi-core processors and networking\ntechnologies. Open source MPI libraries must run well across multiple\ngenerations of processors and interconnects, and additionally support\naccelerators such as NVIDIA GPGPUS and Intel Xeon Phi\nprocessors. Typically, any MPI library deals with diverse platforms\nand a wide range of applications by employing various runtime\nparameters. These parameters are either set during software release,\nby system administrators, or end-users. Considering the nature of\ncommodity clusters, it is difficult to apply one set of common tuning\nparameters that will allow an MPI library to extract the best\nperformance on all computer systems. This leads to a broad challenge:\nCan a comprehensive performance tuning framework be designed for MPI\nlibraries for modern commodity clusters?\n\nIn this project we have addressed this challenge in three areas: 1)\nDeveloped architecture specific tuning for MPI communication\noperations between pairs of cores (point-to-point operations) or\ngroups of cores (collective operations).  2) Developed a performance\nprofiling layer within MVAPICH2, including support for the MPI-3\nstandard's MPI_T interface.  3) Studied application sensitivity to MPI\nparameters and developed applications specific tuning strategies.\n\nDevelopments were made in all three areas and evaluated with a range\nof science applications including: HoomdBlue, SMG2000, Neuron, Amber,\nMiniAMR, MILC, LULESH and HPCCG. These scientific applications and\nmathematical libraries are from biochemistry, neuroscience, high\nenergy physics, computational fluid dynamics, and numerical linear\nalgebra.  Some highlights of these results are that\narchitecture-specific tuning improves collective operation latency by\nup to 58%. The GPU tuning methods were able to provide a 2X\nimprovement in the execution time of HoomdBlue application. The\ntransport protocol-based tuning shows 27% improvement for the Neuron\napplication example. With HPCG and the LULESH application kernel, the\ncollective tuning for partial subscription available on hybrid\nMPI+OpenMP programming model improves the execution time by 24% at 512 cores.\n\nThe results of this research (tuned designs, performance results,\nbenchmarks, etc.) have been made available to the community through\nthe open-source MVAPICH2 library (1.9, 2.0, 2.1, and 2.2 release\nseries including alpha, beta and RC versions). The latest version is\ncurrently running on many large-scale XSEDE systems including TACC\nStampede, SDSC Gordon and SDSC Comet. Currently, the MVAPICH2 library is being used by more than 2,650 organizations in 81 countries. The\nMVAPICH2 library and the enhancements are being used by a large number\nof users of these systems.\n\nIn each of these releases, information about the tuned designs for\nvarious components (such as point-to-point, collectives, GPU-GPU\ncommunication, etc.) has been shared with the MVAPICH2 user community\nthrough mailing lists. The applications-based tuning results have been\nmade available to the community through the \"Best Practices\" link of\nthe MVAPICH project web page. In order to achieve direct face-to-face\ndiscussion with MVAPICH2 users and get their feedback, in 2013 we\nstarted holding an MVAPICH2 User Group Meeting (MUG) each year in\nAugust in Columbus Ohio. This meeting has been continuing successfully\nfor the last four years and has helped to disseminate the results of\nthis research to a wider community.  In addition to the software\ndistribution and the MUG events, the results have been presented at\nvarious conferences and events through talks and tutorials.  Multiple\nPh.D and Masters students have performed research work and received\ntheir Ph.D and M.S. degrees as a part of this project.\n\n\t\t\t\t\tLast Modified: 08/30/2016\n\n\t\t\t\t\tSubmitted by: Dhabaleswar K Panda"
 }
}