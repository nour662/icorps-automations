{
 "awd_id": "1228669",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "TWC SBES: Medium: Utility for Private Data Sharing in Social Science",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928643",
 "po_email": "skiesler@nsf.gov",
 "po_sign_block_name": "Sara Kiesler",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 1066889.0,
 "awd_amount": 1066889.0,
 "awd_min_amd_letter_date": "2012-08-20",
 "awd_max_amd_letter_date": "2013-09-10",
 "awd_abstract_narration": "One of the keys to scientific progress is the sharing of research data. When the data contain information about human subjects, the incentives not to share data are stronger. The biggest concern is privacy - specific information about individuals must be protected at all times. Recent advances in mathematical notions of privacy have raised the hope that the data can be properly sanitized and distributed to other research groups without revealing information about any individual. In order to make this effort worthwhile, the sanitized data must be useful for statistical analysis. This project addresses the research challenges in making the sanitized data useful. The first part of the project deals with the design of algorithms that produce useful sanitized data subject to privacy constraints. The second part of the project deals with the development of tools for the statistical analysis of sanitized data. Existing statistical routines are not designed for the types of complex noise patterns that are found in sanitized data; their naive use will often result in missed discoveries or false claims of statistical significance. The target application for this project is a social science dataset with geographic characteristics. The intellectual merit of this proposal is the development of a utility theory for algorithms that sanitize data and statistical tools for their analysis. The broader impact is the improved ability of research groups to share useful, but privacy-preserving, research data.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Kifer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Kifer",
   "pi_email_addr": "duk17@psu.edu",
   "nsf_id": "000519235",
   "pi_start_date": "2012-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Matthews",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen A Matthews",
   "pi_email_addr": "sxm27@psu.edu",
   "nsf_id": "000489695",
   "pi_start_date": "2012-08-20",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tse-Chuan",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tse-Chuan Yang",
   "pi_email_addr": "tyang3@albany.edu",
   "nsf_id": "000612978",
   "pi_start_date": "2012-08-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "360F IST Bldg.",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 1066889.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Differential privacy is a set of mathematical guidelines for the behavior of software. Software that follows those guidelines (called \"differentially private software\") obtains a provable mathematical guarantee on the privacy of individuals who contributed data that was input into the software. Differential privacy requires that the software use randomness is a special way that masks the contributions of data of any individual.</p>\n<p>Because of this randomness, specialized postprocessing algorithms are needed to extract the most utility from the output of differentially private software. The goal of this project was to develop such post-processing algorithms.</p>\n<p>The first part of the project was concerned with mathematical definitions of utility (i.e., \"information content\") in the output of differentially private software and the mathematical properties that utility measures need to satisfy (for example, any computation performed on the output should have measured utiltiy that is less than the output itself). Utility measures are needed for the design of differentially private software (so that the software can provide the most useful output subject to privacy constraints).</p>\n<p>Later, the project considered the design of post-processing algorithms that would unlock the information contained in the output of differentially private software. For instance, given the noisy output, one would be interested in approximately answering questions about the input. The most natural use case is statistical analysis.</p>\n<p>In classical statistics, one is interested in performing hypothesis tests on the data (e.g., did the data plausibly come from a specific distribution or are two attributes independent of each other), building models, and obtaining confidence intervals for parameters in the models. We designed new statistical techniques for taking the output of differentially private software and, using only this output, to perform hypothesis tests designed to answer questions about the true data. We also designed algorithms for obtaining confidence intervals for a class of statistical models that are trained using a framework known as empirical risk minimization.</p>\n<p>In terms of broader impact, we have developed techniques for statistical analysis over privacy preserving data that can be used by statisticians and social scientists in their applications so that they can study their data while providing strong confidentiality guarantees. We also developed software that allows construction of statistical models that use geography information while protecting the locations of individuals whose data were used to construct these models.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/06/2019<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Kifer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDifferential privacy is a set of mathematical guidelines for the behavior of software. Software that follows those guidelines (called \"differentially private software\") obtains a provable mathematical guarantee on the privacy of individuals who contributed data that was input into the software. Differential privacy requires that the software use randomness is a special way that masks the contributions of data of any individual.\n\nBecause of this randomness, specialized postprocessing algorithms are needed to extract the most utility from the output of differentially private software. The goal of this project was to develop such post-processing algorithms.\n\nThe first part of the project was concerned with mathematical definitions of utility (i.e., \"information content\") in the output of differentially private software and the mathematical properties that utility measures need to satisfy (for example, any computation performed on the output should have measured utiltiy that is less than the output itself). Utility measures are needed for the design of differentially private software (so that the software can provide the most useful output subject to privacy constraints).\n\nLater, the project considered the design of post-processing algorithms that would unlock the information contained in the output of differentially private software. For instance, given the noisy output, one would be interested in approximately answering questions about the input. The most natural use case is statistical analysis.\n\nIn classical statistics, one is interested in performing hypothesis tests on the data (e.g., did the data plausibly come from a specific distribution or are two attributes independent of each other), building models, and obtaining confidence intervals for parameters in the models. We designed new statistical techniques for taking the output of differentially private software and, using only this output, to perform hypothesis tests designed to answer questions about the true data. We also designed algorithms for obtaining confidence intervals for a class of statistical models that are trained using a framework known as empirical risk minimization.\n\nIn terms of broader impact, we have developed techniques for statistical analysis over privacy preserving data that can be used by statisticians and social scientists in their applications so that they can study their data while providing strong confidentiality guarantees. We also developed software that allows construction of statistical models that use geography information while protecting the locations of individuals whose data were used to construct these models.\n\n\t\t\t\t\tLast Modified: 03/06/2019\n\n\t\t\t\t\tSubmitted by: Daniel Kifer"
 }
}