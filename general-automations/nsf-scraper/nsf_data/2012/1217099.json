{
 "awd_id": "1217099",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Small:Tradeoffs among Measures in Computational and Proof Complexity",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jack S. Snoeyink",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 440000.0,
 "awd_amount": 440000.0,
 "awd_min_amd_letter_date": "2012-07-19",
 "awd_max_amd_letter_date": "2012-07-19",
 "awd_abstract_narration": "In this project, the PI and his team study the relationship between widely-used satisfiability algorithms (SAT solvers) and the complexity of proofs, with the goal of characterizing the proof strength of complete SAT solvers that use conflict-directed clause learning.  The project also focuses on the inherent tradeoffs between the running time and storage (space) required to derive proofs and the impact that this has on SAT solvers.  SAT solvers are among the most important and useful tools in a wide range of applications, from finding solutions to problems under constraints to checking the correctness and safety of software and hardware systems where their\r\nrole as methods of proof is required.\r\n\r\nThis project will also explore resource tradeoffs inherent in solving specific computational problems such as computing order statistics and encoding data using good quality error-correcting codes that are resilient to worst-case errors.  Such error-correcting codes are becoming increasingly important in a networked world in which adversaries may deliberately corrupt network traffic.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Paul",
   "pi_last_name": "Beame",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Paul W Beame",
   "pi_email_addr": "beame@cs.washington.edu",
   "nsf_id": "000169746",
   "pi_start_date": "2012-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "185 Stevens Way",
  "perf_city_name": "Seattle",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981952350",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7927",
   "pgm_ref_txt": "COMPLEXITY & CRYPTOGRAPHY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 440000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The research supported by this award developed a new way of measuring the complexity of computations that is appropriate for understanding the tradeoffs involved in solving problems on massively parallel computers.&nbsp; Research using these measures also yielded both new general algorithms for computing fundamental database operations, known as multiway joins, on massively parallel computers, as well proofs that many of these algorithms have optimal, or near-optimal, efficiency.&nbsp; In addition, new massively parallel algorithms for proximity search, finding al pairs of elements of a dataset that are close to each other, were developed and shown to be qualitatively optimal.</p>\n<p>This research also studied the computational complexity of fundamental problems associated with knowledge representation and inference under uncertainty, which are used in AI and for the analysis of probabilistic databases.&nbsp; In particular, it showed that a wide class of existing methods based on satisfiability (SAT) solvers can be exponentially less efficient than methods that use other features of the problems.&nbsp; In particular, the research provided the first proof of a separation between methods known as lifted and grounded inference: there are examples associated with simple database queries for which grounded inference is exponentially more costly than lifted inference.&nbsp;</p>\n<p>In addition, the supported research enhanced prior work on tradeoffs in proof complexity and showed new and tighter connections between the theory of proof complexity and the practice of SAT solving.<br /><br />This award provided extensive research support and training for four graduate students, including one who graduated with a PhD and two who have completed their Masters degrees and are continuing their PhD studies.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/20/2016<br>\n\t\t\t\t\tModified by: Paul&nbsp;W&nbsp;Beame</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe research supported by this award developed a new way of measuring the complexity of computations that is appropriate for understanding the tradeoffs involved in solving problems on massively parallel computers.  Research using these measures also yielded both new general algorithms for computing fundamental database operations, known as multiway joins, on massively parallel computers, as well proofs that many of these algorithms have optimal, or near-optimal, efficiency.  In addition, new massively parallel algorithms for proximity search, finding al pairs of elements of a dataset that are close to each other, were developed and shown to be qualitatively optimal.\n\nThis research also studied the computational complexity of fundamental problems associated with knowledge representation and inference under uncertainty, which are used in AI and for the analysis of probabilistic databases.  In particular, it showed that a wide class of existing methods based on satisfiability (SAT) solvers can be exponentially less efficient than methods that use other features of the problems.  In particular, the research provided the first proof of a separation between methods known as lifted and grounded inference: there are examples associated with simple database queries for which grounded inference is exponentially more costly than lifted inference. \n\nIn addition, the supported research enhanced prior work on tradeoffs in proof complexity and showed new and tighter connections between the theory of proof complexity and the practice of SAT solving.\n\nThis award provided extensive research support and training for four graduate students, including one who graduated with a PhD and two who have completed their Masters degrees and are continuing their PhD studies.\n\n\t\t\t\t\tLast Modified: 10/20/2016\n\n\t\t\t\t\tSubmitted by: Paul W Beame"
 }
}