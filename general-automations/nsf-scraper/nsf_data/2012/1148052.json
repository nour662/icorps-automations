{
 "awd_id": "1148052",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SI2-SSI:  Collaborative Research:   A Glass Box Approach to Enabling Open, Deep Interactions in the HPC Toolchain",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rajiv Ramnath",
 "awd_eff_date": "2012-06-01",
 "awd_exp_date": "2016-12-31",
 "tot_intn_awd_amt": 926666.0,
 "awd_amount": 926666.0,
 "awd_min_amd_letter_date": "2012-05-29",
 "awd_max_amd_letter_date": "2016-09-09",
 "awd_abstract_narration": "Parallel computing has entered the mainstream with increasingly large multicore processors and powerful accelerator devices. These compute engines, coupled with tighter integration of faster interconnection fabrics, are drivers for the next-generation high end computing (HEC) machines. However, the computing potential of HEC machines is delivered only through productive parallel program development and efficient parallel execution. This project enables application developers to improve performance on future HEC machines for their scientific and engineering processes. This project challenges the current model for parallel application development via \"black box\" tools and services. Instead, the project offers an open, transparent software infrastructure -- a Glass Box system -- for creating and tuning large-scale, parallel applications.  `Opening up' the tools and services used to create and evaluate peta- and exa-scale codes involves developing interfaces and methods that make tool-internal information and available for new performance management services that improve developer productivity and code efficiency.\r\n\r\nThe project will explore the information that can be shared 'across the software stack'.  Methods will be developed for analyzing program information, performance data and tool knowledge. The resulting Glass Box system will allow developers to better assess the performance of their parallel codes.  Tool creators can use the performance data to create new analysis and optimization techniques. System developers can also better manage multicore and machine resources at runtime, using JIT compilation and binary code editing to exploit the evolving hardware.  Working with the `Keeneland' NSF Track II machine and our industry partners, the project will create new performance monitoring tools, compiler methods and system-level resource management techniques. The effort is driven by the large-scale codes running on today's petascale machines.  Its broader impact is derived from the interactions with technology developers and application scientists as well as from its base in three universities with diverse student populations.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Barbara",
   "pi_last_name": "Chapman",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Barbara M Chapman",
   "pi_email_addr": "barbara.chapman@stonybrook.edu",
   "nsf_id": "000306370",
   "pi_start_date": "2012-05-29",
   "pi_end_date": "2016-09-09"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Edgar",
   "pi_last_name": "Gabriel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Edgar Gabriel",
   "pi_email_addr": "gabriel@cs.uh.edu",
   "nsf_id": "000316336",
   "pi_start_date": "2016-09-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Edgar",
   "pi_last_name": "Gabriel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Edgar Gabriel",
   "pi_email_addr": "gabriel@cs.uh.edu",
   "nsf_id": "000316336",
   "pi_start_date": "2012-05-29",
   "pi_end_date": "2016-09-09"
  }
 ],
 "inst": {
  "inst_name": "University of Houston",
  "inst_street_address": "4300 MARTIN LUTHER KING BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "HOUSTON",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7137435773",
  "inst_zip_code": "772043067",
  "inst_country_name": "United States",
  "cong_dist_code": "18",
  "st_cong_dist_code": "TX18",
  "org_lgl_bus_name": "UNIVERSITY OF HOUSTON SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "QKWEF8XLMTT3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Houston",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "772042015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "18",
  "perf_st_cong_dist": "TX18",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7433",
   "pgm_ref_txt": "CyberInfra Frmwrk 21st (CIF21)"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  },
  {
   "pgm_ref_code": "8009",
   "pgm_ref_txt": "Scientifc Software Integration"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 926666.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main goals of the project were to investigate methods, techniques, heuristics and software solutions for integrating information from different stages of software construction, execution, and evaluation. This integration of information is crucial to gain better performance on the exascale machines. A typical High Performance Computing (HPC) application development cycle consists of four steps. In the first step, a research problem is written as a&nbsp;<em>program</em>&nbsp;in a human-readable programming language such as C, C++, and FORTRAN. The parallelism in the program is exposed using different parallel programming models such as MPI and OpenMP. In the second step, the program is compiled using a compiler that transfers the program into the machine codes. In the third step, the machine codes are run on a single or multiple machines using one or more runtime libraries that provide support for parallel execution.&nbsp;In the fourth step, the software performance is measured, analyzed&nbsp;by one or more tools in an effort to improve performance.&nbsp;This cycle is repeated number of times to improve the performance of the program. In order to pin point the main performance bottleneck and to find a solution for that performance issue, the information at each step should be gather and integrated. This information gather will give a user a true picture of the program. In this project, we investigated ways to increase information sharing between these phases.</p>\n<p>In the project, we closely worked with our colleagues at the University of Oregon and the Georgia Institute of Technology. We developed new methods for providing interactions between the different steps of software development cycle. We developed a new strategy to help scale program instrumentation using the OpenUH compiler. We developed new OpenMP runtime APIs for OMPT profiling framework. The APIs can be used to collect runtime information for OpenMP applications. We showed that the OMPT framework has the ability to detect unique patterns that can be used to build a quality detection model for false sharing in OpenMP programs. We also showed that these patterns could be useful for many optimization problems. We developed a framework the can select the best configurations for a given OpenMP kernel at runtime. We tested the framework for power constrained OpenMP applications. We also improved the ADCL auto-tuning library for collective communication operations using the GlassBox framework, and gained important insights into impact and limitation of point-to-point performance on the performance of collective operations.</p>\n<p>We have published our results in academic journals and presented our results at related conferences.&nbsp; We have also shared our development with the scientific communities at national research labs and industrial research labs.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/15/2017<br>\n\t\t\t\t\tModified by: Edgar&nbsp;Gabriel</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe main goals of the project were to investigate methods, techniques, heuristics and software solutions for integrating information from different stages of software construction, execution, and evaluation. This integration of information is crucial to gain better performance on the exascale machines. A typical High Performance Computing (HPC) application development cycle consists of four steps. In the first step, a research problem is written as a program in a human-readable programming language such as C, C++, and FORTRAN. The parallelism in the program is exposed using different parallel programming models such as MPI and OpenMP. In the second step, the program is compiled using a compiler that transfers the program into the machine codes. In the third step, the machine codes are run on a single or multiple machines using one or more runtime libraries that provide support for parallel execution. In the fourth step, the software performance is measured, analyzed by one or more tools in an effort to improve performance. This cycle is repeated number of times to improve the performance of the program. In order to pin point the main performance bottleneck and to find a solution for that performance issue, the information at each step should be gather and integrated. This information gather will give a user a true picture of the program. In this project, we investigated ways to increase information sharing between these phases.\n\nIn the project, we closely worked with our colleagues at the University of Oregon and the Georgia Institute of Technology. We developed new methods for providing interactions between the different steps of software development cycle. We developed a new strategy to help scale program instrumentation using the OpenUH compiler. We developed new OpenMP runtime APIs for OMPT profiling framework. The APIs can be used to collect runtime information for OpenMP applications. We showed that the OMPT framework has the ability to detect unique patterns that can be used to build a quality detection model for false sharing in OpenMP programs. We also showed that these patterns could be useful for many optimization problems. We developed a framework the can select the best configurations for a given OpenMP kernel at runtime. We tested the framework for power constrained OpenMP applications. We also improved the ADCL auto-tuning library for collective communication operations using the GlassBox framework, and gained important insights into impact and limitation of point-to-point performance on the performance of collective operations.\n\nWe have published our results in academic journals and presented our results at related conferences.  We have also shared our development with the scientific communities at national research labs and industrial research labs.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 03/15/2017\n\n\t\t\t\t\tSubmitted by: Edgar Gabriel"
 }
}