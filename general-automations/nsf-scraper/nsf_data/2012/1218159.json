{
 "awd_id": "1218159",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: 'Houston, We Have a Solution': Novel Speech Processing Advancements for Analysis of Large Asynchronous Multi-Channel Audio Corpora",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 84102.0,
 "awd_amount": 84102.0,
 "awd_min_amd_letter_date": "2012-08-09",
 "awd_max_amd_letter_date": "2012-08-09",
 "awd_abstract_narration": "This project is focused on developing new speech processing techniques which will transform access to large asynchronous multi-channel and diverse collections of multimedia materials. In particular, the algorithms developed are being employed to create a novel multi-source and multi-scale event reconstruction system that brings together the massive archives of the Apollo lunar missions, to create experiential interaction with historical materials. Specific research advancements are focused on state of the art acoustic environment analysis, speech recognition including keyword spotting, speaker identification under adverse conditions, multimodal content alignment, and automated linking for events and entities from spoken content. Specifically, the research is developing: (i) new techniques for noise- and channel-robust acoustic processing, exploiting missing-features concepts with novel feature extraction and compensation techniques, (ii) a new articulatory framework for speech recognition for robustness to variations in speech production, (iii) environmental \"sniffing\" techniques to automatically characterize acoustic environments to improve robustness, and (iv) automatic detection of novel task-specific audio-events. Since the data is asynchronous, unique speech analytics techniques are being formulated to address the large number of \"local loop\" intercom circuits in the NASA Mission Control Center, audio recorded onboard the two Apollo spacecrafts during specific mission events, and space-to-ground radio circuits. The specific speech, language, and knowledge extraction advancements will be integrated into a new automated evaluation model that reflects specific challenges encountered in the event reconstruction task. This platform will be deployed and evaluated by actual users from the Science and Engineering Education Center (SEEC) of the University of Texas at Dallas. \r\n\r\nIntegration of robust speech processing algorithms with event reconstruction systems will have a direct and immediate impact on education, society, and government organizations. Working with NASA's Apollo mission data allows for the development of speech technology for challenging audio that contains severe communication channel artifacts, cross-talk/static/tones, and low signal-to-noise ratios. The software being developed in this project will be made available to any non-profit organization for use in audio/video search (download with training modules). Students working on senior design teams will also develop a Contact Science station to be deployed in Dallas, TX and overseen by the University of Texas in Dallas Science and Engineering Education Center to illustrate and assess student use of the advancements.  As a lasting legacy for this project, this project team includes eminent historians of human space flight, who will explore opportunities to deploy this event reconstruction system in a museum setting where it can support both scholarship and public engagement, and we will make the system itself available on an open-source basis to support other researchers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Douglas",
   "pi_last_name": "Oard",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Douglas W Oard",
   "pi_email_addr": "oard@glue.umd.edu",
   "nsf_id": "000161387",
   "pi_start_date": "2012-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 84102.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Research teams at the University of Maryland, College Park and the University of Texas at Dallas worked together to develop new capabilities for exploring the records of complex mission-critical activities that require simultaneous communication among dozens of experts. &nbsp;To accomplish this, the project has digitized more than two years of audio (i.e., more than 12,000 total hours) from the 60 channels of Mission Control Center coordination activity that had been recorded for historical purposes during two of the Apollo missions to the Moon (Apollo 11 and Apollo 13) and one of the earlier Gemini missions (Gemini 8). &nbsp;Performing digitization at that scale required development of new 30-track read head and a new 30-channel digitization system for the one existing system that is capable of playing the obsolete 30-track tapes on which this unique and historically significant content was originally recorded. &nbsp;</p>\n<p>Like many modern coordination activities such as global air traffic control or nuclear powerplant safety monitoring, intense periods of coordination activity in these NASA recordings are separated by long periods with relatively little activity. &nbsp;An automated system for detecting when speech was present was therefore developed and used to focus the subsequent stages of automated processing on the roughly five percent of the time in which spoken content was present on a typical channel. &nbsp;Uniquely tailored speech recognition systems were then created that could cope with the specialized technical terminology used in the Apollo Program and the challenging acoustic conditions resulting, for example, from radio communication over the quarter of a million miles between the Earth and the Moon. &nbsp;More than four billion words found in existing documents about the Apollo program were used to discover this terminology, and also to build models of the ways in which Apollo flight controllers were likely to speak that could help to improve the results of automated speech recognition. &nbsp;Transcripts from speech recognition were then used to accelerate the review of this large set of recordings for public release by allowing expert human reviewers to recognize the relatively few parts of the recording that required audio replay before a releasability determination could be made. &nbsp;</p>\n<p>New techniques were also developed for segmenting the resulting audio into conversations on specific topics. &nbsp;Automated techniques for linking conversations on related topics that occurred at different times (e.g., linking discussions during a mission to discussions of the same topic in post-mission interviews) were explored as one way of supporting non-sequential access to such large and complex collections of recorded speech. &nbsp;Research prototypes for supporting exploration of this large collection using both time-synchronization and topic-linking were developed, and a study with journalism students provided evidence for the potential value of integrating content-based access to initially find points in a mission that could then serve as starting points for time-synchronized access to the rich multimedia content that is available (which includes not just recorded speech, but also dozens of hours of video, thousands of still photographs, and associated content such as maps and planning documents).</p>\n<p>The Apollo missions are among the most ambitious achievements of science and technology in the modern era, so rich experiential access to the archival records of those missions offers the potential for citizens to experience these missions in new ways, and thus to draw new types of insights about not merely the technical aspects of the missions, but also the human experience of overcoming challenges and exploring the unknown. &nbsp;More broadly, techniques developed in this project could also be of value for providing future access to many other types of large and complex information environments. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/05/2016<br>\n\t\t\t\t\tModified by: Douglas&nbsp;W&nbsp;Oard</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nResearch teams at the University of Maryland, College Park and the University of Texas at Dallas worked together to develop new capabilities for exploring the records of complex mission-critical activities that require simultaneous communication among dozens of experts.  To accomplish this, the project has digitized more than two years of audio (i.e., more than 12,000 total hours) from the 60 channels of Mission Control Center coordination activity that had been recorded for historical purposes during two of the Apollo missions to the Moon (Apollo 11 and Apollo 13) and one of the earlier Gemini missions (Gemini 8).  Performing digitization at that scale required development of new 30-track read head and a new 30-channel digitization system for the one existing system that is capable of playing the obsolete 30-track tapes on which this unique and historically significant content was originally recorded.  \n\nLike many modern coordination activities such as global air traffic control or nuclear powerplant safety monitoring, intense periods of coordination activity in these NASA recordings are separated by long periods with relatively little activity.  An automated system for detecting when speech was present was therefore developed and used to focus the subsequent stages of automated processing on the roughly five percent of the time in which spoken content was present on a typical channel.  Uniquely tailored speech recognition systems were then created that could cope with the specialized technical terminology used in the Apollo Program and the challenging acoustic conditions resulting, for example, from radio communication over the quarter of a million miles between the Earth and the Moon.  More than four billion words found in existing documents about the Apollo program were used to discover this terminology, and also to build models of the ways in which Apollo flight controllers were likely to speak that could help to improve the results of automated speech recognition.  Transcripts from speech recognition were then used to accelerate the review of this large set of recordings for public release by allowing expert human reviewers to recognize the relatively few parts of the recording that required audio replay before a releasability determination could be made.  \n\nNew techniques were also developed for segmenting the resulting audio into conversations on specific topics.  Automated techniques for linking conversations on related topics that occurred at different times (e.g., linking discussions during a mission to discussions of the same topic in post-mission interviews) were explored as one way of supporting non-sequential access to such large and complex collections of recorded speech.  Research prototypes for supporting exploration of this large collection using both time-synchronization and topic-linking were developed, and a study with journalism students provided evidence for the potential value of integrating content-based access to initially find points in a mission that could then serve as starting points for time-synchronized access to the rich multimedia content that is available (which includes not just recorded speech, but also dozens of hours of video, thousands of still photographs, and associated content such as maps and planning documents).\n\nThe Apollo missions are among the most ambitious achievements of science and technology in the modern era, so rich experiential access to the archival records of those missions offers the potential for citizens to experience these missions in new ways, and thus to draw new types of insights about not merely the technical aspects of the missions, but also the human experience of overcoming challenges and exploring the unknown.  More broadly, techniques developed in this project could also be of value for providing future access to many other types of large and complex information environments.  \n\n \n\n\t\t\t\t\tLast Modified: 12/05/2016\n\n\t\t\t\t\tSubmitted by: Douglas W Oard"
 }
}