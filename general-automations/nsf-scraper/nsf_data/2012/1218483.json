{
 "awd_id": "1218483",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Algorithm/Architecture Co-Design of Low Power and High Performance Linear Algebra Compute Fabrics",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2012-06-01",
 "awd_exp_date": "2017-05-31",
 "tot_intn_awd_amt": 499919.0,
 "awd_amount": 499919.0,
 "awd_min_amd_letter_date": "2012-06-07",
 "awd_max_amd_letter_date": "2012-06-07",
 "awd_abstract_narration": "Until recently, the speed of a computer processors could be increased\r\nby packing more transistors into a smaller area and increasing the\r\nfrequency.  This trend is now unsustainable because of power\r\nconstraints, with only moderate gains going forward even when putting\r\nhundreds or thousands of traditional cores onto a chip.  Thus, how to\r\nreduce power consumption while increasing performance is one of the\r\ncore concerns.  It is well-accepted that specialization (designing\r\nparts of the processor for a specific task) and heterogeneity\r\n(designating different parts of the processor for different tasks) can\r\nlead to orders of magnitude improvements in both aspects.  However,\r\nthe question is whether such efficiency can be maintained while\r\nproviding enough flexibility to implement a broad class of operations.\r\nLeveraging unique domain expertise, research under this project\r\naddresses this question for the domain of matrix computations, which\r\nare at the core of many computational advances, both in scientific\r\nhigh-performance computing as well as in the embedded, mobile or\r\ncyber-physical domains.\r\n\r\nObserving that the largest benefits can be obtained through\r\nspecialization at the foundations, this project is aimed at\r\nco-designing algorithms and architectures to directly realize basic\r\nlinear algebra methods in an optimized combination of hardware and\r\nsoftware.  By designing a specialized Linear Algebra Processor (LAP),\r\nit is possible to achieve one to two orders of magnitude improved\r\nefficiencies compared to traditional or proposed computer\r\narchitectures.  The questions that the project will answer, through a\r\ncombination of analysis, simulation, and prototyping, include: (1) How\r\nto best design such LAPs that can efficiently execute the full set of\r\nlinear algebra routines; and (2) How LAPs can be scaled, networked\r\ninto clusters and integrated with application software running on one\r\nor more host processors.  The broad goal of this project is to develop\r\nnovel, integrated linear algebra compute fabrics that are co-optimized\r\nand co-designed across all layers ranging from the basic hardware\r\nfoundations all the way to the application programming support through\r\nstandard linear algebra software packages.  This project is expected\r\nto result in a leap in computational science and discovery\r\ncapabilities, thus enabling novel breakthroughs in industry, for the\r\nconsumer, at the national labs, in education and by scientists in\r\nacademia.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Gerstlauer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Gerstlauer",
   "pi_email_addr": "gerstl@ece.utexas.edu",
   "nsf_id": "000520598",
   "pi_start_date": "2012-06-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "van de Geijn",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Robert A van de Geijn",
   "pi_email_addr": "rvdg@cs.utexas.edu",
   "nsf_id": "000336892",
   "pi_start_date": "2012-06-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787137726",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 499919.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Information and semiconductor technology has had enormous social implications over the last decades. However, with semiconductor scaling reaching physical limits, power constraints have become the primary issue in continued evolution and performance growth of computing systems going forward. Fundamentally, energy savings can only be achieved by either reducing the amount of computation performed in the first place, or by reducing the amount of overhead per required computation. The latter is achieved through specializations. Traditional general-purpose computer systems incur a tremendous amount of overhead as the price for the flexibility to run any application. It is well accepted that specialization (designing parts of the computer for a specific task) and heterogeneity (designating different parts of the computer for different tasks) can lead to orders of magnitude improvements in both performance and power consumption. At the same time, exponential increasing costs of semiconductor fabrication make it impossible to tape out a new specialized chip for every new application. The question is thus whether such efficiencies can be maintained while providing enough flexibility to implement a broad class of operations.</p>\n<p>In this project, we have addressed such questions by investigating novel specialized computer system designs to reduce power consumption while increasing performance specifically for the important domain of linear algebra operations. Linear algebra or matrix operations are at the root of almost all computational problems not only in science and engineering but also in emerging application areas such as data analytics, machine learning and artificial intelligence.</p>\n<p>Observing that the largest benefits can be obtained through specialization at the foundations, this project was aimed at co-designing architectures and the algorithms running on them to directly realize core linear algebra methods in an optimized combination of hardware and software. The primary result has been a novel design of a Linear Algebra Processor (LAP) that is orders of magnitude more energy efficient (as measured by the energy per operation) than any existing processor design, all while being able to run a complete set of operations with higher performance. Beyond just the immediate goal of designing a specialized processor, we have used our LAP design as a baseline to study fundamental limits in efficiency and fundamental tradeoffs between achievable efficiency and flexibility in the context of the targeted application domain. We have developed novel approaches for fast yet accurate power and performance modeling of the LAP and other accelerators that enable rapid, early exploration of design spaces. Using developed LAP models, results have suggested minimal modifications that should make it possible to significantly improve performance and efficiency for these types of operations on existing computer system designs. Finally, we have developed necessary software stacks and software support for integration of LAP accelerators into larger systems and overall applications, showing that when integrating the LAP with existing processor and system components in a heterogeneous environment, and with corresponding support through tightly co-optimized software stacks, close to peak efficiencies can be maintained across larger applications that utilize such linear algebra kernels.</p>\n<p>All combined, results of this project have demonstrated that is possible to design domain-specific accelerators with orders-of-magnitude improved efficiency of full-custom hardware but enough flexibility to run any application across a range of related domains. In doing so, we have developed novel, integrated linear algebra compute fabrics that are co-optimized and co-designed across all layers ranging from the basic hardware foundations all the way to the application programming support through standard linear algebra software packages. Being able to provide hitherto infeasible, order of magnitude improved compute capabilities even in energy-constrained devices (such as small, embedded mobile or battery-operated systems) for this important domain of applications has the potential to solve many open problems that may significantly impact society in the future.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/27/2017<br>\n\t\t\t\t\tModified by: Andreas&nbsp;Gerstlauer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nInformation and semiconductor technology has had enormous social implications over the last decades. However, with semiconductor scaling reaching physical limits, power constraints have become the primary issue in continued evolution and performance growth of computing systems going forward. Fundamentally, energy savings can only be achieved by either reducing the amount of computation performed in the first place, or by reducing the amount of overhead per required computation. The latter is achieved through specializations. Traditional general-purpose computer systems incur a tremendous amount of overhead as the price for the flexibility to run any application. It is well accepted that specialization (designing parts of the computer for a specific task) and heterogeneity (designating different parts of the computer for different tasks) can lead to orders of magnitude improvements in both performance and power consumption. At the same time, exponential increasing costs of semiconductor fabrication make it impossible to tape out a new specialized chip for every new application. The question is thus whether such efficiencies can be maintained while providing enough flexibility to implement a broad class of operations.\n\nIn this project, we have addressed such questions by investigating novel specialized computer system designs to reduce power consumption while increasing performance specifically for the important domain of linear algebra operations. Linear algebra or matrix operations are at the root of almost all computational problems not only in science and engineering but also in emerging application areas such as data analytics, machine learning and artificial intelligence.\n\nObserving that the largest benefits can be obtained through specialization at the foundations, this project was aimed at co-designing architectures and the algorithms running on them to directly realize core linear algebra methods in an optimized combination of hardware and software. The primary result has been a novel design of a Linear Algebra Processor (LAP) that is orders of magnitude more energy efficient (as measured by the energy per operation) than any existing processor design, all while being able to run a complete set of operations with higher performance. Beyond just the immediate goal of designing a specialized processor, we have used our LAP design as a baseline to study fundamental limits in efficiency and fundamental tradeoffs between achievable efficiency and flexibility in the context of the targeted application domain. We have developed novel approaches for fast yet accurate power and performance modeling of the LAP and other accelerators that enable rapid, early exploration of design spaces. Using developed LAP models, results have suggested minimal modifications that should make it possible to significantly improve performance and efficiency for these types of operations on existing computer system designs. Finally, we have developed necessary software stacks and software support for integration of LAP accelerators into larger systems and overall applications, showing that when integrating the LAP with existing processor and system components in a heterogeneous environment, and with corresponding support through tightly co-optimized software stacks, close to peak efficiencies can be maintained across larger applications that utilize such linear algebra kernels.\n\nAll combined, results of this project have demonstrated that is possible to design domain-specific accelerators with orders-of-magnitude improved efficiency of full-custom hardware but enough flexibility to run any application across a range of related domains. In doing so, we have developed novel, integrated linear algebra compute fabrics that are co-optimized and co-designed across all layers ranging from the basic hardware foundations all the way to the application programming support through standard linear algebra software packages. Being able to provide hitherto infeasible, order of magnitude improved compute capabilities even in energy-constrained devices (such as small, embedded mobile or battery-operated systems) for this important domain of applications has the potential to solve many open problems that may significantly impact society in the future.\n\n\t\t\t\t\tLast Modified: 07/27/2017\n\n\t\t\t\t\tSubmitted by: Andreas Gerstlauer"
 }
}