{
 "awd_id": "1216318",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Extending Sparse Optimization",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "rosemary renaut",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 240999.0,
 "awd_amount": 240999.0,
 "awd_min_amd_letter_date": "2012-07-24",
 "awd_max_amd_letter_date": "2012-07-24",
 "awd_abstract_narration": "Rather than solving an optimization problem exactly, sparse optimization seeks approximate solutions that satisfy certain structural properties, such as few nonzeros in the solution vector. Sparse optimization problems and formulations are now recognized across a wide range of applications, and techniques for solving these problems draw on a large variety of algorithmic tools, old and new. This project aims to extend sparse optimization in two respects. First, work is proposed in application areas that can benefit from the sparse optimization perspective: machine learning and data mining at extreme scale, contact dynamics, object packing, medical image reconstruction, and derivative-free optimization. Algorithmic developments will target key problem formulations in these areas, paying particular attention to methods that can exploit parallel computer architectures and specialized hardware. Algorithmic techniques to be considered include stochastic approximation, randomized directions, augmented Lagrangian, and reduced-space search using higher-order information. Second, the project will use general frameworks to analyze such algorithmic ideas as manifold identification, continuation, first-order algorithms, inexactness, and convergence and complexity results. The general nature of these investigations will enable innovations to be spread across a wide range of formulations and applications.  \r\n\r\nThe field of optimization provides a vital framework for formulating, modeling, and solving problems in many application areas. In sparse optimization, we note that many applications require solutions with a special structure that is easy to specify, but hard to incorporate in traditional algorithms and models. Sparse optimization arises, for example, in reconstruction of signals and images, where we know that the signal should contain only a few frequencies, or that the image should look like a natural image rather than white noise.  Important developments of the past few years have shown that the requirement of structure in solutions, rather than being a hindrance to efficient solution, can actually lead to more efficient formulations and faster methods. Notable successes have been achieved in such areas as compressed sensing and image denoising. This project will build on these successes by developing algorithms that can be leveraged in many new and existing applications of sparse optimization. In keeping with modern optimization research, a bevy of algorithmic techniques will be considered. Theory will be developed to support the use of these techniques in a wide range of contexts.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stephen",
   "pi_last_name": "Wright",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Stephen J Wright",
   "pi_email_addr": "swright@cs.wisc.edu",
   "nsf_id": "000485636",
   "pi_start_date": "2012-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  },
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 240999.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Important new applications in data analysis, arising from machine learning, compressed sensing, speech and image processing, and other fields, have created a surge of demand for optimization algorithms. This demand is met in part by devising new methods that exploit the structure of the applciations - particularly the combination of smooth terms with nonsmooth but simple regularization terms, whose function is to avoid overfitting the data. But the demand is also being met by such traditional, elementary optimization methods as coordinate descent and stochastic gradient. The properties of these methods are being examined more closely, and new variants are being devised, particularly variants that are suited to parallel implementation.<br />This project was concerned chiefly with the mathematical analysis of optimization algorithms that arise in data analysis problems. Our goal was to understand their mathematical properties better, particularly such issues as their rate of convergence. We also aimed to extend their applicability to wider classes of problems.&nbsp;</p>\n<p><br />We mention specifics of several strands of our work.</p>\n<p>Coordinate descent is a fundamental technique in optimization in which the variables are relaxed one at a time, in a random or cyclicorder. We developed, analyzed, and applied asynchronous parallelversion of these methods, and made progress in characterizing andclosing the gap between their theoretical performance and their practical behavior.</p>\n<p><br />We developed conditional gradient approaches for atomic-norm-constrained optimization problems, applying them in a wide variety of applications.<br />We developed methods for solving relaxations of optimization problems in which the quantity to be determined is an unknown ordering of a set of objects.</p>\n<p><br />We addressed a problem arising from the question of how genetic material arranges itself in the nucleus of a cell. We posed this problem as one of packing ellopsoids into a spheroid, and solved it with a bilevel optimization problem involving trust-region methods and semidefinite programming.</p>\n<p><br />We analyzed the local convergence behavior of an algorithm for identifying an unknown subspace from incomplete observations of vectors in that subspace, showing that for sufficiently dense and random observations, linear convergence can be observed. We applied this technique to the classic problem of \"structure from motion\" in which the shape of an opaque object is learned from various planar views of that object.</p>\n<p><br />We gave a unified view of the use of optimization techniques in speechprocessing, on the eve of the deep learning revolution which has transformed that field (and which draws on optimization techniques ofa different type from those we considered).</p>\n<p><br />To summarize: The central role played by Optimization in the booming field of data analysis has continued to grow during the course of thisproject. The results obtained here have further the mathematical understanding of optimization methods in these contexts. These developments show no sign of slowing, and we anticipate doing much more work along these lines in years to come.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/18/2016<br>\n\t\t\t\t\tModified by: Stephen&nbsp;J&nbsp;Wright</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nImportant new applications in data analysis, arising from machine learning, compressed sensing, speech and image processing, and other fields, have created a surge of demand for optimization algorithms. This demand is met in part by devising new methods that exploit the structure of the applciations - particularly the combination of smooth terms with nonsmooth but simple regularization terms, whose function is to avoid overfitting the data. But the demand is also being met by such traditional, elementary optimization methods as coordinate descent and stochastic gradient. The properties of these methods are being examined more closely, and new variants are being devised, particularly variants that are suited to parallel implementation.\nThis project was concerned chiefly with the mathematical analysis of optimization algorithms that arise in data analysis problems. Our goal was to understand their mathematical properties better, particularly such issues as their rate of convergence. We also aimed to extend their applicability to wider classes of problems. \n\n\nWe mention specifics of several strands of our work.\n\nCoordinate descent is a fundamental technique in optimization in which the variables are relaxed one at a time, in a random or cyclicorder. We developed, analyzed, and applied asynchronous parallelversion of these methods, and made progress in characterizing andclosing the gap between their theoretical performance and their practical behavior.\n\n\nWe developed conditional gradient approaches for atomic-norm-constrained optimization problems, applying them in a wide variety of applications.\nWe developed methods for solving relaxations of optimization problems in which the quantity to be determined is an unknown ordering of a set of objects.\n\n\nWe addressed a problem arising from the question of how genetic material arranges itself in the nucleus of a cell. We posed this problem as one of packing ellopsoids into a spheroid, and solved it with a bilevel optimization problem involving trust-region methods and semidefinite programming.\n\n\nWe analyzed the local convergence behavior of an algorithm for identifying an unknown subspace from incomplete observations of vectors in that subspace, showing that for sufficiently dense and random observations, linear convergence can be observed. We applied this technique to the classic problem of \"structure from motion\" in which the shape of an opaque object is learned from various planar views of that object.\n\n\nWe gave a unified view of the use of optimization techniques in speechprocessing, on the eve of the deep learning revolution which has transformed that field (and which draws on optimization techniques ofa different type from those we considered).\n\n\nTo summarize: The central role played by Optimization in the booming field of data analysis has continued to grow during the course of thisproject. The results obtained here have further the mathematical understanding of optimization methods in these contexts. These developments show no sign of slowing, and we anticipate doing much more work along these lines in years to come.\n\n\t\t\t\t\tLast Modified: 12/18/2016\n\n\t\t\t\t\tSubmitted by: Stephen J Wright"
 }
}