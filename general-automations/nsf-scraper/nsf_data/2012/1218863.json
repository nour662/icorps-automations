{
 "awd_id": "1218863",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Developing Large Scale Distributed Syntactic, Semantic and Lexical Language Models for Machine Translation and Speech Recognition",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 460000.0,
 "awd_amount": 460000.0,
 "awd_min_amd_letter_date": "2012-07-19",
 "awd_max_amd_letter_date": "2013-07-23",
 "awd_abstract_narration": "This project aims to build large scale distributed syntactic, semantic, and lexical language models that are trained by corpora with up to Web-scale data on a supercomputer in order to substantially improve the performance of machine translation and speech recognition systems.  It is conducted under the directed Markov random field paradigm to integrate both topics and syntax to form complex distributions for natural language, and uses hierarchical Pitman-Yor processes to model long-tail properties of natural language.  By exploiting this particular structure, the complex statistical estimation and inference algorithms are decomposed and performed in a distributed environment.  The language models are put into one-pass decoders of machine translation systems, and the lattice rescoring decoder into a speech recognition system.  In addition, a principled solution to a long-standing open problem, smoothing fractional counts due to latent variables in Kneser-Ney's sense, might be found. \r\n\r\nThis project fits into the NSF's strategic long term vision of a Cyber-infrastructure Framework for 21st Century Science and Engineering (CIF21).  The project integrates various kinds of known language models and provides a way to overcome the limitations of existing combination methods for language models and to deploy algorithmically interesting methodologies that are scalable to data sets available on the Web.  The project provides an environment for interdisciplinary education in information technology that bridges areas of language and speech processing, machine learning, and data-intensive science and engineering to benefit students at several levels.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Shaojun",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Shaojun Wang",
   "pi_email_addr": "swang.usa@gmail.com",
   "nsf_id": "000297009",
   "pi_start_date": "2012-07-19",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Yunxin",
   "pi_last_name": "Zhao",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yunxin Zhao",
   "pi_email_addr": "zhaoy@missouri.edu",
   "nsf_id": "000428350",
   "pi_start_date": "2012-07-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Wright State University",
  "inst_street_address": "3640 COLONEL GLENN HWY",
  "inst_street_address_2": "",
  "inst_city_name": "DAYTON",
  "inst_state_code": "OH",
  "inst_state_name": "Ohio",
  "inst_phone_num": "9377752425",
  "inst_zip_code": "454350002",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "OH10",
  "org_lgl_bus_name": "WRIGHT STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPT2UNTNHJZ1"
 },
 "perf_inst": {
  "perf_inst_name": "Wright State University",
  "perf_str_addr": "3640 Colonel Glenn Hwy",
  "perf_city_name": "Dayton",
  "perf_st_code": "OH",
  "perf_st_name": "Ohio",
  "perf_zip_code": "454350001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "OH10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 155827.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 304173.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We have proposed composite statistical language models and compact neural language models where in order to train neural language models on large corpus, space efficient model compression method is developed. Experiments on several data sets show that the new method achieves perplexity and BLEU score results comparable to the best existing methods, while only using a tiny fraction of the parameters required by other approaches. We investigated using a distributive composite statistical language model (CLM) in speech recognition to reduce word error rate through re-scoring word lattices, confusion networks, and n&shy;-best lists. Finally we developed a novel approach of ensemble neural language modeling to integrate lexical, syntactical, and semantic language phenomena through multiple word/class factorizations and direct word error rate minimization.</p>\n<p>The proposed models are scalable and improve the performance of the state-of-the-art speech recognition and machine translation systems. The techniques developed in this project might not only lead to effective, robust and intelligent language technology applications, but also might be extended and applied to solve problems in computational biology and computer vision. The project provides an excellent environment for interdisciplinary education in information technology that bridges areas of language and speech processing, machine learning and computational statistics, and theoretical computer science to benefit students of all levels.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/03/2018<br>\n\t\t\t\t\tModified by: Shaojun&nbsp;Wang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe have proposed composite statistical language models and compact neural language models where in order to train neural language models on large corpus, space efficient model compression method is developed. Experiments on several data sets show that the new method achieves perplexity and BLEU score results comparable to the best existing methods, while only using a tiny fraction of the parameters required by other approaches. We investigated using a distributive composite statistical language model (CLM) in speech recognition to reduce word error rate through re-scoring word lattices, confusion networks, and n&shy;-best lists. Finally we developed a novel approach of ensemble neural language modeling to integrate lexical, syntactical, and semantic language phenomena through multiple word/class factorizations and direct word error rate minimization.\n\nThe proposed models are scalable and improve the performance of the state-of-the-art speech recognition and machine translation systems. The techniques developed in this project might not only lead to effective, robust and intelligent language technology applications, but also might be extended and applied to solve problems in computational biology and computer vision. The project provides an excellent environment for interdisciplinary education in information technology that bridges areas of language and speech processing, machine learning and computational statistics, and theoretical computer science to benefit students of all levels.\n\n\t\t\t\t\tLast Modified: 09/03/2018\n\n\t\t\t\t\tSubmitted by: Shaojun Wang"
 }
}