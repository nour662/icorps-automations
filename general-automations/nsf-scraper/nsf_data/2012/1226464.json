{
 "awd_id": "1226464",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Inference in Non-Standard Econometric Problems",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Georgia Kosmopoulou",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 199116.0,
 "awd_amount": 199116.0,
 "awd_min_amd_letter_date": "2012-09-10",
 "awd_max_amd_letter_date": "2012-09-10",
 "awd_abstract_narration": "Empirical questions in economics are studied via econometric models. These models contain unknown parameters, which are typically the object of interest in empirical applications. For instance, one might be interested in learning about the average propensity to consume, or the marginal effect of a tax increase on labor supply, and researchers use data and econometrics analysis to estimate that parameter. \r\n\r\nClearly, one would like to use methods of data analysis that allow one to learn as much as possible about these unknown parameters. And for a large set of models, it is well understood how to do so, at least approximately. With many observations from a well-behaved model, the problem of not knowing the parameter is effectively equivalent to the \"standard problem\" of observing a single Gaussian (bell-shaped) observation with known spread and a mean that corresponds to the unknown parameter. For a non-negligible subset of models, however, there is only an approximate equivalence to a different, \"non-standard\" problem. These unknown correlation patterns make it impossible to perfectly learn about the spread of the parameter estimator, which leads to a non-standard approximately equivalent problem where the spread of the single Gaussian vector observation is unknown. \r\n\r\nThe proposal studies such non-standard problems, and seeks to develop ways of learning about the unknown parameters. The derivation of nearly efficient methods involves the development of appropriate analytical results, such accurate upper bounds on the potential quality of inference in non-standard problems, as well as numerical methods that determine a nearly efficient procedure, that is one with an inference quality close to the upper bound.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ulrich",
   "pi_last_name": "Mueller",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Ulrich K Mueller",
   "pi_email_addr": "umueller@princeton.edu",
   "nsf_id": "000488827",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Department of Economics",
  "perf_str_addr": "311 Fisher Hall",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085441045",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132000",
   "pgm_ele_name": "Economics"
  },
  {
   "pgm_ele_code": "133300",
   "pgm_ele_name": "Methodology, Measuremt & Stats"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 199116.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;&nbsp;&nbsp; Econometrics is the statistical evaluation of data with the goal of learning about economic phenomena. In a variety of contexts, economic data is such that well-studied types of inference do not apply. For instance, economic time series are often very persistent, which potentially invalidates textbook statistical approaches. In this project, I study such \"non-standard\" statistical problems, and devise ways of obtaining valid and in some sense optimal new procedures.<br />&nbsp;&nbsp;&nbsp; The statistical questions in econometrics are typically formulated through models and their parameters. Inference about the value of the parameters, or at least a subset of them, is typically the primary aim of the research, as they relate to economic structures and behavior of interest (\"the marginal propensity to consume\", \"the supply elasticity\", etc.). The most common approach to learning about parameters is to perform a hypothesis test, that is the researcher checks wether a given realization of the data is plausibly compatible with a prespecified value of a parameter of interest. In this context, this project develops a numerical approach to determine nearly optimal hypothesis tests in nonstandard problems that involve nuisance parameters (that is, parameters that are not of interest per se, but that further describe the distribution of the data). Moreover, I also develop a related approach to numerically obtain nearly optimal unbiased estimators of parameter values. These numerical approaches are quite generic and potentially useful in a variety of contexts.<br />&nbsp;&nbsp;&nbsp; To provide one practical illustration, consider the problem of making a very long-run forecast of an economic time series, such as gross domestic production (GDP). Say we have 60 years worth of data to learn about the statistical properties of the series, and seek to forecast the level of GDP in 30 years. This becomes a non-standard problem if one makes the plausible assumption that mean growth of GDP is not necessarily perfectly constant through time, or that GDP potentially has other non-trivial long-run dynamics, such a slow reversal to a long-run trend. The parameters that describe these long-run phenomena are important determinants of long-run forecast uncertainty, but they cannot be determined from the data with great accuracy, and the available sample information is of a non-standard form. In one paper supported by this grant, we applied the general approach to determine a rule for forecast intervals that are as informative as possible while incorporating this crucial parameter uncertainty. These results are potentially useful to inform policy makers about the degree of uncertainty of long-run predictions, such as the long-term solvency of the social security trust fund.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/06/2015<br>\n\t\t\t\t\tModified by: Ulrich&nbsp;K&nbsp;Mueller</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n    Econometrics is the statistical evaluation of data with the goal of learning about economic phenomena. In a variety of contexts, economic data is such that well-studied types of inference do not apply. For instance, economic time series are often very persistent, which potentially invalidates textbook statistical approaches. In this project, I study such \"non-standard\" statistical problems, and devise ways of obtaining valid and in some sense optimal new procedures.\n    The statistical questions in econometrics are typically formulated through models and their parameters. Inference about the value of the parameters, or at least a subset of them, is typically the primary aim of the research, as they relate to economic structures and behavior of interest (\"the marginal propensity to consume\", \"the supply elasticity\", etc.). The most common approach to learning about parameters is to perform a hypothesis test, that is the researcher checks wether a given realization of the data is plausibly compatible with a prespecified value of a parameter of interest. In this context, this project develops a numerical approach to determine nearly optimal hypothesis tests in nonstandard problems that involve nuisance parameters (that is, parameters that are not of interest per se, but that further describe the distribution of the data). Moreover, I also develop a related approach to numerically obtain nearly optimal unbiased estimators of parameter values. These numerical approaches are quite generic and potentially useful in a variety of contexts.\n    To provide one practical illustration, consider the problem of making a very long-run forecast of an economic time series, such as gross domestic production (GDP). Say we have 60 years worth of data to learn about the statistical properties of the series, and seek to forecast the level of GDP in 30 years. This becomes a non-standard problem if one makes the plausible assumption that mean growth of GDP is not necessarily perfectly constant through time, or that GDP potentially has other non-trivial long-run dynamics, such a slow reversal to a long-run trend. The parameters that describe these long-run phenomena are important determinants of long-run forecast uncertainty, but they cannot be determined from the data with great accuracy, and the available sample information is of a non-standard form. In one paper supported by this grant, we applied the general approach to determine a rule for forecast intervals that are as informative as possible while incorporating this crucial parameter uncertainty. These results are potentially useful to inform policy makers about the degree of uncertainty of long-run predictions, such as the long-term solvency of the social security trust fund.\n\n\t\t\t\t\tLast Modified: 09/06/2015\n\n\t\t\t\t\tSubmitted by: Ulrich K Mueller"
 }
}