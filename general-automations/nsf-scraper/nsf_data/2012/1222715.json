{
 "awd_id": "1222715",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "ATD Collaborative Research:  Theory and Algorithms for High Dimensional Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 248870.0,
 "awd_amount": 248870.0,
 "awd_min_amd_letter_date": "2012-08-28",
 "awd_max_amd_letter_date": "2012-08-28",
 "awd_abstract_narration": "The investigators and their collaborators study how to organize and query high dimensional data in order to extract relevant content while avoiding the so-called curse of dimensionality.   The team is developing new analytical and numerical methods based on sparsity, adaptivity, and variable reduction. The focus is placed on developing a coherent theory that results in sophisticated state-of-the-art numerical algorithms that can be applied in a variety of settings. This activity is a  critical component of many scientific problems since it  complements and supports the scientific methods of theory, experimentation, and simulation. A setting of particular interest  to this project is learning tasks such as regression and classification. The research team is developing quantifiable frameworks and algorithms for learning that systematically break down the high dimensional barriers and exploit empirical data collections. \r\n\r\nMany scientic problems, vital to the security, economy, and health of our nation, are so complex that they challenge this nation's most sophisticated computational resources. Examples occur in modeling physical and biological systems, e.g. in atmospheric modeling; in optimal design (optimal control and shape optimization);  and also in understanding social networks such as those that occur in threat detection. The complexity of these problems prohibits the use of traditional off-the-shelf computational techniques for their solution. This research team develops new computational tools that lead to state of the art algorithms for detecting and the capturing critical information held in the solution of such complex systems. An emphasis in  this project is the  processing of  data that arise in threat detection,  damage assessment, and containment.  This requires the simultaneous analysis of  data obtained from different modalities and \r\na variety of sensors. The new algorithms are applied, for example,  to identify and track the migration of airborne biological and chemical contaminants. Another application area is  the development of new approaches to high dimensional problems related to gene sequencing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ronald",
   "pi_last_name": "DeVore",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Ronald A DeVore",
   "pi_email_addr": "rdevore@math.tamu.edu",
   "nsf_id": "000329812",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Guergana",
   "pi_last_name": "Petrova",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Guergana Petrova",
   "pi_email_addr": "gpetrova@math.tamu.edu",
   "nsf_id": "000256375",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M University",
  "inst_street_address": "400 HARVEY MITCHELL PKY S STE 300",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778454375",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A & M UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "JF6XLNB4CDJ5"
 },
 "perf_inst": {
  "perf_inst_name": "Texas A&M University Main Campus",
  "perf_str_addr": "TAMU 3368",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778433368",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "755200",
   "pgm_ele_name": "COFFES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6877",
   "pgm_ref_txt": "ALGORITHMS IN THREAT DETECTION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 248870.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Many scientific problems of vital interest to the security and economy of the United States are intrinsically high dimensional in that they depend on a large number of parameters, variables, or features. While high dimensionality has long been a challenge to scientists who model complex physical, chemical, and biological systems, it is also increasingly significant in data processing. The fusion and analysis of data drawn from a variety of sensors and other information sources, including social networks, involves a large number of features, which are typically unknown. Finding where to query this data in order to answer questions of interest is challenged by this high dimensionality. It is not merely an issue of where to query but also whether the queries carry enough information to answer the underlying questions and if so, how to compute these answers in an efficient manner.<br /> High space dimensionality presents severe challenges to the research community because traditional computational procedures suffer from the so-called `curse of dimensionality'. This means that when employed on high dimensional problems, these traditional methods cannot guarantee that the output of the computation is accurate. This research project put forth new concepts and computational algorithms to remediate the high dimensional impediments. Namely, the project discovered new computational methods based on sparsity, compressibility, as well as model and variable reduction, and then proved the accuracy of computation with these new algorithms. The new algorithms are being applied to a range of problems in simulation of complex processes and in analysis of big data.</p>\n<p style=\"text-align: left;\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/05/2015<br>\n\t\t\t\t\tModified by: Ronald&nbsp;A&nbsp;Devore</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMany scientific problems of vital interest to the security and economy of the United States are intrinsically high dimensional in that they depend on a large number of parameters, variables, or features. While high dimensionality has long been a challenge to scientists who model complex physical, chemical, and biological systems, it is also increasingly significant in data processing. The fusion and analysis of data drawn from a variety of sensors and other information sources, including social networks, involves a large number of features, which are typically unknown. Finding where to query this data in order to answer questions of interest is challenged by this high dimensionality. It is not merely an issue of where to query but also whether the queries carry enough information to answer the underlying questions and if so, how to compute these answers in an efficient manner.\n High space dimensionality presents severe challenges to the research community because traditional computational procedures suffer from the so-called `curse of dimensionality'. This means that when employed on high dimensional problems, these traditional methods cannot guarantee that the output of the computation is accurate. This research project put forth new concepts and computational algorithms to remediate the high dimensional impediments. Namely, the project discovered new computational methods based on sparsity, compressibility, as well as model and variable reduction, and then proved the accuracy of computation with these new algorithms. The new algorithms are being applied to a range of problems in simulation of complex processes and in analysis of big data.\n \n\n\t\t\t\t\tLast Modified: 10/05/2015\n\n\t\t\t\t\tSubmitted by: Ronald A Devore"
 }
}