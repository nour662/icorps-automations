{
 "awd_id": "1208497",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NRI-Small: Collaborative Research: Multiple Task Learning from Unstructured Demonstrations",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Reid Simmons",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 499199.0,
 "awd_amount": 499199.0,
 "awd_min_amd_letter_date": "2012-09-06",
 "awd_max_amd_letter_date": "2012-09-06",
 "awd_abstract_narration": "This project develops techniques for the efficient, incremental learning of complex robotic tasks by breaking unstructured demonstrations into reusable component skills.  A Bayesian model segments task demonstrations into simpler components and recognizes instances of repeated skills across demonstrations.  Established methods from control engineering and reinforcement learning are leveraged and extended to allow for skill improvement from practice, in addition to learning from demonstration.  The project aims to unify existing research on each of these ideas into a principled, integrated approach that addresses all of these problems jointly, with the goal of creating a deployment-ready, open-source system that transforms the way experts and novices alike interact with robots.\r\n\r\nA simple interface that allows end-users to intuitively program robots is a key step to getting robots out of the laboratory and into human-cooperative settings in the home and workplace.   Although it is often possible for an expert to program a robot to perform complex tasks, this programming is often very time-consuming and requires a great deal of knowledge. In response to this, much recent research is focusing on robot learning-from-demonstration, where non-expert users can teach a robot how to perform a task by example.  Unfortunately, much of this work is limited to the artificially-structured demonstration of a single task with a well-defined beginning and end.  By contrast, human-cooperative robots will be required to efficiently and incrementally learn many different, but often related, tasks from complex, unstructured demonstrations that are easy for non-experts to produce.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Andrew",
   "pi_last_name": "Barto",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Andrew G Barto",
   "pi_email_addr": "barto@cs.umass.edu",
   "nsf_id": "000201472",
   "pi_start_date": "2012-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "Computer Sci, 140 Governors Dr",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801300",
   "pgm_ele_name": "NRI-National Robotics Initiati"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "8086",
   "pgm_ref_txt": "Natl Robotics Initiative (NRI)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 499199.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<div>This work addressed an issue that accounts for much of the gap between present-day robots and future human-cooperative robots that will be as ubiquitous as the personal computer: the difficulty of robot programming. Robotic hardware that has the ability to perform tasks requiring significant precision and dexterity has become commonplace in industry and research, and it is on the verge of becoming affordable to consumers through mass production. &nbsp;However, highly trained experts are required to program these robots to perform even the simplest of tasks. &nbsp;This has inspired research on robot learning from demonstration, where non-expert users can teach a robot how to perform a task by example. &nbsp;Unfortunately, much of this work has been limited to the artificially-structured demonstration of a single task with a well-defined beginning and end. &nbsp;By contrast, human-cooperative robots will be required to efficiently and incrementally learn many different, but possibly related, tasks from complex, unstructured demonstrations.&nbsp;</div>\n<div>\n<div>Toward this, we developed techniques for the efficient learning of multi-step tasks by breaking unstructured demonstrations into reusable component skills. &nbsp;These algorithms were then extended to learn more complex tasks incrementally via interactive corrections from users, culminating in a robotic system that was able to learn a challenging IKEA furniture assembly task from a small number of demonstrations. &nbsp;Additionally, we developed novel state-of- the-art algorithms for learning common-sense physics based reasoning for manipulation tasks via both demonstrations and active robot interaction with objects. &nbsp;Using these techniques, the robot was able to improve performance on manipulation tasks, such as IKEA furniture assembly, by predicting and verifying the consequences of its actions. &nbsp;Finally, we developed active viewpoint selection and visual classification algorithms that allowed the robot to perform task verification visually, rather than through its sense of touch alone.</div>\n<div>This project integrated cutting-edge methods from Bayesian statistics, active learning, deep learning, and control theory to address fundamental problems in robotics. &nbsp;The end product of this work---a series of algorithms that allow end-users to program robots via unstructured demonstrations&mdash;is absolutely essential if cooperative robots are to be deployed in real-world situations in the home and workplace. &nbsp;The partnership between UMass and Bosch combined the resources of academia and industry to attack this problem and also resulted in a fruitful graduate student internship program that provided mentorship opportunities and produced valuable data and results. The developed learning from demonstration algorithms were ideal for educational K-12 programs to encourage student interest in STEM areas, and were exhibited as part of the First Bytes and Code Longhorn camps at UT Austin. Furthermore, grant funds were used to partially support a postdoctoral researcher, leading to mentoring opportunities, as well as graduate research assistants and summer interns that included underrepresented groups in computer science.&nbsp;</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2016<br>\n\t\t\t\t\tModified by: Andrew&nbsp;G&nbsp;Barto</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1208497/1208497_10211821_1478796824662_demo2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1208497/1208497_10211821_1478796824662_demo2--rgov-800width.jpg\" title=\"Kinesthetic Task Demonstration\"><img src=\"/por/images/Reports/POR/2016/1208497/1208497_10211821_1478796824662_demo2--rgov-66x44.jpg\" alt=\"Kinesthetic Task Demonstration\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A user providing the robot with a kinesthetic task demonstration for an IKEA furniture assembly task.</div>\n<div class=\"imageCredit\">Scott Niekum</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;G&nbsp;Barto</div>\n<div class=\"imageTitle\">Kinesthetic Task Demonstration</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1208497/1208497_10211821_1478796957853_demo2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1208497/1208497_10211821_1478796957853_demo2--rgov-800width.jpg\" title=\"Autonomous IKEA Furniture Assembly\"><img src=\"/por/images/Reports/POR/2016/1208497/1208497_10211821_1478796957853_demo2--rgov-66x44.jpg\" alt=\"Autonomous IKEA Furniture Assembly\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The robot autonomously executes skill controllers learned entirely from unstructured demonstrations for an IKEA furniture assembly task.</div>\n<div class=\"imageCredit\">Scott Niekum</div>\n<div class=\"imageSubmitted\">Andrew&nbsp;G&nbsp;Barto</div>\n<div class=\"imageTitle\">Autonomous IKEA Furniture Assembly</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\n \nThis work addressed an issue that accounts for much of the gap between present-day robots and future human-cooperative robots that will be as ubiquitous as the personal computer: the difficulty of robot programming. Robotic hardware that has the ability to perform tasks requiring significant precision and dexterity has become commonplace in industry and research, and it is on the verge of becoming affordable to consumers through mass production.  However, highly trained experts are required to program these robots to perform even the simplest of tasks.  This has inspired research on robot learning from demonstration, where non-expert users can teach a robot how to perform a task by example.  Unfortunately, much of this work has been limited to the artificially-structured demonstration of a single task with a well-defined beginning and end.  By contrast, human-cooperative robots will be required to efficiently and incrementally learn many different, but possibly related, tasks from complex, unstructured demonstrations. \n\nToward this, we developed techniques for the efficient learning of multi-step tasks by breaking unstructured demonstrations into reusable component skills.  These algorithms were then extended to learn more complex tasks incrementally via interactive corrections from users, culminating in a robotic system that was able to learn a challenging IKEA furniture assembly task from a small number of demonstrations.  Additionally, we developed novel state-of- the-art algorithms for learning common-sense physics based reasoning for manipulation tasks via both demonstrations and active robot interaction with objects.  Using these techniques, the robot was able to improve performance on manipulation tasks, such as IKEA furniture assembly, by predicting and verifying the consequences of its actions.  Finally, we developed active viewpoint selection and visual classification algorithms that allowed the robot to perform task verification visually, rather than through its sense of touch alone.\nThis project integrated cutting-edge methods from Bayesian statistics, active learning, deep learning, and control theory to address fundamental problems in robotics.  The end product of this work---a series of algorithms that allow end-users to program robots via unstructured demonstrations&mdash;is absolutely essential if cooperative robots are to be deployed in real-world situations in the home and workplace.  The partnership between UMass and Bosch combined the resources of academia and industry to attack this problem and also resulted in a fruitful graduate student internship program that provided mentorship opportunities and produced valuable data and results. The developed learning from demonstration algorithms were ideal for educational K-12 programs to encourage student interest in STEM areas, and were exhibited as part of the First Bytes and Code Longhorn camps at UT Austin. Furthermore, grant funds were used to partially support a postdoctoral researcher, leading to mentoring opportunities, as well as graduate research assistants and summer interns that included underrepresented groups in computer science. \n\n\n \n\n\t\t\t\t\tLast Modified: 11/30/2016\n\n\t\t\t\t\tSubmitted by: Andrew G Barto"
 }
}