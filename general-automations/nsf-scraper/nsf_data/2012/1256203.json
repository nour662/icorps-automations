{
 "awd_id": "1256203",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER:Scaling On-Chip Interconnects for Exascale Systems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Hong Jiang",
 "awd_eff_date": "2012-09-15",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 89948.0,
 "awd_amount": 89948.0,
 "awd_min_amd_letter_date": "2012-09-10",
 "awd_max_amd_letter_date": "2012-09-10",
 "awd_abstract_narration": "This research aims to overcome the extreme challenges that need to be solved to realize a 1000-core (kilocore) processor. Processors with tens of cores are already in commercial products today. A kilocore processor could take us into the era of Server-on-Chip and Supercomputer-on-Chip. On-chip network is the medium through which two nodes in a processor can communicate, and therefore constitutes the backbone of a kilocore processor. Unfortunately, current on-chip network solutions are inadequate as they do not scale in terms of both power and performance beyond a few tens of cores. To reach the ambitious design goal of 1000+ cores with realistic power budgets, the interconnect technology needs to be at least 15 times more power efficient while providing at least the same level of throughput-per-core as today. \r\n\r\nThis project investigates three interrelated solutions to meet the above challenge in an evolutionary manner: (1) Developing a low-power and energy-proportional interconnect architecture that employs a larger number of narrower networks, (2) Using high-radix Swizzle-Switches as the building blocks for interconnecting the multiple networks, and (3) Re-designing network architecture with multiple networks and Swizzle-Switches using 3D integration with Through-Silicon-Vias to achieve scalability beyond 1000 cores. This project will demonstrate the feasibility of kilocore processors. If such processors can be built, they could have a tremendous impact on future exascale systems such as cloud computing servers and HPC systems that have many applications including drug discovery, defense, information analysis, and social networking.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Reetuparna",
   "pi_last_name": "Das",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Reetuparna Das",
   "pi_email_addr": "reetudas@umich.edu",
   "nsf_id": "000750892",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Trevor",
   "pi_last_name": "Mudge",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Trevor N Mudge",
   "pi_email_addr": "tnm@eecs.umich.edu",
   "nsf_id": "000295695",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "2260 Hayward, Room 3828 Beyster",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 89948.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>Advancements in processor technology have enabled diverse applications of computing which have significantly influenced the progress of various facets of society such as health care and medical science, information technology, large scale scientific research, improvements in human quality of life, finance and commerce, transportation and manufacturing industries. A number of societal projections and industrial roadmaps are driven by the expectation that these rates of improvement will continue. &nbsp;</span>For the past three decades, Moores Law (the doubling of transistors on chip every 18 months) has been the cardinal driver of advancement of processor technology. However, voltage scaling began to stagnate after the 180nm technology node and as a result, accelerating single core performance has become prohibitive from a power perspective. Designers have instead resorted to increasing the number of cores per die as a power-efficient approach to throughput scaling, leading to the dawn of multicore era. &nbsp;</p>\n<p><span>The paradigm shift towards manycore designs has led to a renewed interest in interconnect design. Interconnects play a dominant role in shaping the power and performance profiles of manycore processors designed using deep submicron technologies. The lack of a good interconnect fabric can be envisioned to result in problems similar to traffic chaos in a large city without a proper roadway infrastructure. The trend towards integrating 100&rsquo;s of cores onto the same chip is further accentuating the importance of on-chip interconnect design.&nbsp;</span></p>\n<p><span>In this project we focus on scaling networks for on-chip communication in manycore systems with up to 1000 cores, that will be the critical enabler for exascale supercomputers and cloud computing platforms of the future. &nbsp;T<span>o accomplish our ambitious goal, we must overcome several challenges. The largest of these barriers is related to energy and power. As the number of cores increases, it is going to become a significant challenge to sustain the current per-core bandwidth given the super-linear increase in network power consumption.&nbsp;The projected interconnect power by scaling up existing network designs far exceeds a practical power budget. A low power interconnect design is imperative to the realization of a 1000 core processor. While meeting the power constraint is important, it is equally necessary to meet the performance goals in terms of network bandwidth and latency. We adopt a power-driven approach towards scaling on-chip interconnects to meet the bandwidth and latency targets necessary in high-performance processors. &nbsp;Our research investigated two interrelated thrusts to scale the interconnect: (1) high-radix swizzle switches and (2) fine grained power-gating to achieve energy proportionality.&nbsp;</span></span></p>\n<p><span>Existing on-chip network topologies require large number of on-chip routers. The performance and power cost of these routers becomes prohibitive as we scale up to a manycore system with 100&rsquo;s of cores. Consolidating these routers into a few large but </span><span>efficient </span><span>high-radix switches</span><span>&nbsp;</span><span>is a potential solution to this problem. Each high-radix switch should be able to provide connectivity for 32 to 64 cores and connectivity to other high-radix switches. Typically, such a design is generally considered impractical due to power and area complexity of these large switches. Along with our colleagues at Michigan, we developed a novel high-radix switch design called </span><span>Swizzle-Switch </span><span>which challenges this conventional notion and can readily scale up to a radix of 64. Using </span><span>Swizzle-Switches </span><span>as building blocks we architected </span><span>asymmetric high-radix topologies&nbsp;</span><span>that can scale on-chip networks to 1000 cores. ...",
  "por_txt_cntn": "\nAdvancements in processor technology have enabled diverse applications of computing which have significantly influenced the progress of various facets of society such as health care and medical science, information technology, large scale scientific research, improvements in human quality of life, finance and commerce, transportation and manufacturing industries. A number of societal projections and industrial roadmaps are driven by the expectation that these rates of improvement will continue.  For the past three decades, Moores Law (the doubling of transistors on chip every 18 months) has been the cardinal driver of advancement of processor technology. However, voltage scaling began to stagnate after the 180nm technology node and as a result, accelerating single core performance has become prohibitive from a power perspective. Designers have instead resorted to increasing the number of cores per die as a power-efficient approach to throughput scaling, leading to the dawn of multicore era.  \n\nThe paradigm shift towards manycore designs has led to a renewed interest in interconnect design. Interconnects play a dominant role in shaping the power and performance profiles of manycore processors designed using deep submicron technologies. The lack of a good interconnect fabric can be envisioned to result in problems similar to traffic chaos in a large city without a proper roadway infrastructure. The trend towards integrating 100\u00c6s of cores onto the same chip is further accentuating the importance of on-chip interconnect design. \n\nIn this project we focus on scaling networks for on-chip communication in manycore systems with up to 1000 cores, that will be the critical enabler for exascale supercomputers and cloud computing platforms of the future.  To accomplish our ambitious goal, we must overcome several challenges. The largest of these barriers is related to energy and power. As the number of cores increases, it is going to become a significant challenge to sustain the current per-core bandwidth given the super-linear increase in network power consumption. The projected interconnect power by scaling up existing network designs far exceeds a practical power budget. A low power interconnect design is imperative to the realization of a 1000 core processor. While meeting the power constraint is important, it is equally necessary to meet the performance goals in terms of network bandwidth and latency. We adopt a power-driven approach towards scaling on-chip interconnects to meet the bandwidth and latency targets necessary in high-performance processors.  Our research investigated two interrelated thrusts to scale the interconnect: (1) high-radix swizzle switches and (2) fine grained power-gating to achieve energy proportionality. \n\nExisting on-chip network topologies require large number of on-chip routers. The performance and power cost of these routers becomes prohibitive as we scale up to a manycore system with 100\u00c6s of cores. Consolidating these routers into a few large but efficient high-radix switches is a potential solution to this problem. Each high-radix switch should be able to provide connectivity for 32 to 64 cores and connectivity to other high-radix switches. Typically, such a design is generally considered impractical due to power and area complexity of these large switches. Along with our colleagues at Michigan, we developed a novel high-radix switch design called Swizzle-Switch which challenges this conventional notion and can readily scale up to a radix of 64. Using Swizzle-Switches as building blocks we architected asymmetric high-radix topologies that can scale on-chip networks to 1000 cores. As a natural next step we designed Hi-Rise 3D high-radix switch which extends scalability to radix 96 from that of the 64 radix supported by 2D switches at the same operating frequency. 3D integration is a promising technology to enable performance scaling for future systems and Hi-Rise can serve as an efficient communicati..."
 }
}