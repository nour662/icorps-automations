{
 "awd_id": "1252987",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Memory-based learning of effective actions",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 80000.0,
 "awd_amount": 80000.0,
 "awd_min_amd_letter_date": "2012-09-05",
 "awd_max_amd_letter_date": "2012-09-05",
 "awd_abstract_narration": "This project addresses the foundational question in Robust Intelligence of how an autonomous agent can learn use low-level sub-symbolic (pixel-level) sensorimotor experiences with its environment to learn higher level effective concepts, ranging from learning to use a hand to manipulate objects on a tabletop, to learning to balance and walk, to learning to move through a complex environment without collisions with walls or pedestrians. This project will develop computational models of how this learning process could take place and will implement and test these computational models on an actual robot. Understanding such autonomous concept learning has the potential to impact a range of disciplines including Cognitive Science, Psychology, AI in general, and robotics, computer vision, and machine learning in particular. Understanding how concepts come into being and evolve in the specific domain of robot navigation also has the potential to contribute to advances in systems that help persons with physical and learning disabilities.\r\n\r\nThe project draws on insights from two different approaches from the PI's lab that have complementary strengths: (1) QLAP (Qualitative Learner of Action and Perception), and (2) MPEPC system (Model Predictive Equilibrium Point Control). The QLAP system exploits a qualitative abstraction of continuous sensor input in order to learn causal contingencies, DBN (Dynamic Belief Network) and MDP models of the causal world, and to build a hierarchy of action models. It uses perception with laser rangefinders and correlation peaks between changes to the motor vector and events in the sense vector -- so-called contingencies -- to discern motor signals that produce resulting perceptual events that may be more than random variation. Reliable episodes can be remembered as cases and used in learning. The MPEPC system factors the continuous navigation problem for a mobile robot into a local unconstrained control and a global optimization process that balances constraints such as progress and collision avoidance. Both methods have a local phase (learning contingencies and local control laws), and a global phase (learning a hierarchy of actions and finding extended routes that balance constraints). These two approaches will be augmented by learning methods from Case-Based Reasoning (CBR) that use features of the presenting case to retrieve related cases from case memory. Two levels of case representation will be employed. The lowest level case representation is a simple feature vector: in the case of local motion control, it specifies the target pose location in the egocentric frame of reference, along with the parameters of the motion control law that attempts to reach it, and the quality of the resulting trajectory. Retrieval will be done using Nearest Neighbor, combining information from the retrieved cases by Locally Weighted Regression or Locally Weighted Projection Regression. At the higher level of action learning, a case is to be described by identifying the critical environmental constraints that determine the global structure of the action.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Kuipers",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin J Kuipers",
   "pi_email_addr": "kuipers@umich.edu",
   "nsf_id": "000324244",
   "pi_start_date": "2012-09-05",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481092121",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 80000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>An intelligent robot must learn to take effective actions in a world that it only partially understands.&nbsp; We take as our primary example the task of an intelligent robotic wheelchair moving around a residence, office building, or campus, learning a map of the environment while avoiding pedestrians and other hazards.&nbsp; A secondary example is moving a manipulator arm past objects where collisions should be avoided, to grasp a desired object and move it to a target location while avoiding collisions.&nbsp;</p>\n<p>Our initial hypothesis was that memory-based reasoning methods for motion planning would make it possible to exploit previous experience --- memories of previous trajectories around configurations of nearby obstacles.&nbsp;&nbsp; By retrieving an experience similar to the current problem, and adapting it to the current circumstance, we would reduce the cost of planning collision-free motions through configuration space.&nbsp; Unfortunately, the nearby space for motion planning is highly variable, particularly when the uncertain motions of dynamic obstacles such as pedestrians are taken into account.&nbsp; Furthermore, the viability of a particular trajectory can depend sensitively on the detailed configuration of static and dynamic obstacles.&nbsp; In the end, we concluded that similarity matching of features of proposed trajectories, to retrieve candidate trajectories that might serve as solutions to the motion planning problem, was almost certain to be intractable.&nbsp;</p>\n<p>We have developed a sample-based search that generates smooth trajectories that make progress toward a desired destination.&nbsp; Each sample trajectory is characterized by its progress, its cost, and its probability of collision with static and dynamic obstacles.&nbsp; With this information, an off-the-shelf optimizer can select the current best trajectory within a model-predictive control framework.&nbsp; This is proving to be tractable, efficient, and effective.</p>\n<p>This method depends on maintaining and updating an accurate model of the response of the wheelchair to its motor commands.&nbsp; In future work under other funding, we plan to continue to explore the use of memory-based methods, specifically for identifying larger-granularity characteristics of the robot and its environment, such as changes in passenger weight or distribution, or changes in floor surface and friction.&nbsp; We conjecture that this problem will be a better fit to the strengths of memory-based methods than motion planning in the presence of static and dynamic obstacles has proved to be.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/21/2014<br>\n\t\t\t\t\tModified by: Benjamin&nbsp;J&nbsp;Kuipers</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAn intelligent robot must learn to take effective actions in a world that it only partially understands.  We take as our primary example the task of an intelligent robotic wheelchair moving around a residence, office building, or campus, learning a map of the environment while avoiding pedestrians and other hazards.  A secondary example is moving a manipulator arm past objects where collisions should be avoided, to grasp a desired object and move it to a target location while avoiding collisions. \n\nOur initial hypothesis was that memory-based reasoning methods for motion planning would make it possible to exploit previous experience --- memories of previous trajectories around configurations of nearby obstacles.   By retrieving an experience similar to the current problem, and adapting it to the current circumstance, we would reduce the cost of planning collision-free motions through configuration space.  Unfortunately, the nearby space for motion planning is highly variable, particularly when the uncertain motions of dynamic obstacles such as pedestrians are taken into account.  Furthermore, the viability of a particular trajectory can depend sensitively on the detailed configuration of static and dynamic obstacles.  In the end, we concluded that similarity matching of features of proposed trajectories, to retrieve candidate trajectories that might serve as solutions to the motion planning problem, was almost certain to be intractable. \n\nWe have developed a sample-based search that generates smooth trajectories that make progress toward a desired destination.  Each sample trajectory is characterized by its progress, its cost, and its probability of collision with static and dynamic obstacles.  With this information, an off-the-shelf optimizer can select the current best trajectory within a model-predictive control framework.  This is proving to be tractable, efficient, and effective.\n\nThis method depends on maintaining and updating an accurate model of the response of the wheelchair to its motor commands.  In future work under other funding, we plan to continue to explore the use of memory-based methods, specifically for identifying larger-granularity characteristics of the robot and its environment, such as changes in passenger weight or distribution, or changes in floor surface and friction.  We conjecture that this problem will be a better fit to the strengths of memory-based methods than motion planning in the presence of static and dynamic obstacles has proved to be.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/21/2014\n\n\t\t\t\t\tSubmitted by: Benjamin J Kuipers"
 }
}