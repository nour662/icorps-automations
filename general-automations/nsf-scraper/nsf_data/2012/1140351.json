{
 "awd_id": "1140351",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Immediate Feedback Assessment in Chemistry Courses",
 "cfda_num": "47.076",
 "org_code": "11040200",
 "po_phone": "7032924674",
 "po_email": "drickey@nsf.gov",
 "po_sign_block_name": "Dawn Rickey",
 "awd_eff_date": "2012-05-01",
 "awd_exp_date": "2017-04-30",
 "tot_intn_awd_amt": 50849.0,
 "awd_amount": 50849.0,
 "awd_min_amd_letter_date": "2012-04-22",
 "awd_max_amd_letter_date": "2012-04-22",
 "awd_abstract_narration": "This collaborative project between the University of Wisconsin-River Falls and Winona State University is gathering evidence on the role timing and type of feedback has on student learning when multiple choice exams are used in large-enrollment general chemistry and organic chemistry lecture courses.  This systematic chemical education research project is using both traditional Scantron and Immediate Feedback Assessment Technique forms (also known as \"Answer Until Correct\" in which answer spaces are covered with a coating that is scratched off like a lottery ticket).  The variables being investigated include immediate versus delayed feedback, corrective versus non-corrective feedback, test retaking, deliberate increases in cognitive complexity demands, and student confidence in their answers.  The results are providing evidence about student cognitive and metacognitive growth and how different sub-populations vary within the testing regimes.  Volunteer student populations from one doctoral institution, three comprehensive institutions, and two community colleges (Anoka-Ramsey Community College and South Suburban College) are participating.  The intellectual merit of this project includes valid and reliable General Chemistry 1 and Organic Chemistry 1 multiple-choice tests that encompass a spectrum of thinking skills, including higher order cognitive skills.  In addition, the project is documenting how multiple choice exams are used both for real-time student learning as well as assessing a student's knowledge.  The broader impacts of this project are the knowledge that is being created about how the structure of multiple choice questions, coupled with type and timing of feedback, influences student learning for the whole, as well as for different sub-groups.  Given the wide-spread use of multiple-choice exams, this project also is establishing best practices for the construction of online practice and homework problems such that they now can now be formulated better to maximize student learning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DUE",
 "org_div_long_name": "Division Of Undergraduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sara",
   "pi_last_name": "Hein",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Sara M Hein",
   "pi_email_addr": "shein@winona.edu",
   "nsf_id": "000336061",
   "pi_start_date": "2012-04-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Winona State University",
  "inst_street_address": "175 W MARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "WINONA",
  "inst_state_code": "MN",
  "inst_state_name": "Minnesota",
  "inst_phone_num": "5074575519",
  "inst_zip_code": "559873384",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "MN01",
  "org_lgl_bus_name": "WINONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "MQBGMELWH9M3"
 },
 "perf_inst": {
  "perf_inst_name": "Winona State University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MN",
  "perf_st_name": "Minnesota",
  "perf_zip_code": "559875838",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "MN01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "153600",
   "pgm_ele_name": "S-STEM-Schlr Sci Tech Eng&Math"
  },
  {
   "pgm_ele_code": "751300",
   "pgm_ele_name": "TUES-Type 1 Project"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0412",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001213DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  },
  {
   "app_code": "13XX",
   "app_name": "H-1B FUND, EHR, NSF",
   "app_symb_id": "045176",
   "fund_code": "1300XXXXDB",
   "fund_name": "H-1B FUND, EDU, NSF",
   "fund_symb_id": "045176"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 50849.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Throughout higher education, high impact teaching and learning strategies have been honed to cultivate the best outcomes for student learning. Research efforts in non-STEM courses have shown that different delivery and feedback methods in multiple-choice testing can affect student learning in the classroom. However, there has been very little research on the use of multiple-choice testing techniques regarding classroom reform efforts to improve student learning and retention in the STEM fields. This study sought to determine if different approaches to multiple-choice testing development- using different delivery methods and feedback mechanisms- would influence student learning and retention in first-semester organic chemistry.</p>\n<p>To study delivery and feedback mechanisms, two valid and reliable tests, covering first-semester organic chemistry topics, were developed to collect repetitive testing data. These tests provided baseline data to compare both control (scores only) and two different experimental groups using immediate answer-until-correct feedback. In one set of experiments, students were provided immediate, corrective feedback for all questions on the first test only. In another set of experiments, students were provided immediate, corrective feedback on the first half of questions in both the first and second tests. These experiments were carried out with populations of students from four different PUI institutions, which provided a diverse pool of students.</p>\n<p>We found that offering immediate, corrective feedback improved student mean scores in both sets of experiments. Students in the second set of experiments (first half of tests only) exhibited the largest gains. The learning gains for the second set of experiments were significant. In this set of experiments, students were provided a physical instrument to draw the feedback or used computers to take the tests. The extent of the learning gains varied depending on institution and incoming student proficiency. Variance in gains seems to be a result of the complexity of organic chemistry questions posed, as well as the context in which they are presented. This research provided valuable information about how the construction of multiple-choice questions and the feedback mechanism chosen can influence student learning in the testing environment.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/29/2017<br>\n\t\t\t\t\tModified by: Sara&nbsp;M&nbsp;Hein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThroughout higher education, high impact teaching and learning strategies have been honed to cultivate the best outcomes for student learning. Research efforts in non-STEM courses have shown that different delivery and feedback methods in multiple-choice testing can affect student learning in the classroom. However, there has been very little research on the use of multiple-choice testing techniques regarding classroom reform efforts to improve student learning and retention in the STEM fields. This study sought to determine if different approaches to multiple-choice testing development- using different delivery methods and feedback mechanisms- would influence student learning and retention in first-semester organic chemistry.\n\nTo study delivery and feedback mechanisms, two valid and reliable tests, covering first-semester organic chemistry topics, were developed to collect repetitive testing data. These tests provided baseline data to compare both control (scores only) and two different experimental groups using immediate answer-until-correct feedback. In one set of experiments, students were provided immediate, corrective feedback for all questions on the first test only. In another set of experiments, students were provided immediate, corrective feedback on the first half of questions in both the first and second tests. These experiments were carried out with populations of students from four different PUI institutions, which provided a diverse pool of students.\n\nWe found that offering immediate, corrective feedback improved student mean scores in both sets of experiments. Students in the second set of experiments (first half of tests only) exhibited the largest gains. The learning gains for the second set of experiments were significant. In this set of experiments, students were provided a physical instrument to draw the feedback or used computers to take the tests. The extent of the learning gains varied depending on institution and incoming student proficiency. Variance in gains seems to be a result of the complexity of organic chemistry questions posed, as well as the context in which they are presented. This research provided valuable information about how the construction of multiple-choice questions and the feedback mechanism chosen can influence student learning in the testing environment.\n\n\t\t\t\t\tLast Modified: 07/29/2017\n\n\t\t\t\t\tSubmitted by: Sara M Hein"
 }
}