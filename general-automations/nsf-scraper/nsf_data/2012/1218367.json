{
 "awd_id": "1218367",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Small: Collaborative Proposal: Towards Robust Uncertain Data Management",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Nan Zhang",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 248627.0,
 "awd_amount": 248627.0,
 "awd_min_amd_letter_date": "2012-09-04",
 "awd_max_amd_letter_date": "2013-07-05",
 "awd_abstract_narration": "The goal of this project is to develop a systematic framework to enable \"robust\" query processing in presence of data uncertainties that arise naturally in a wide variety of application domains. Data uncertainties may take the form of missing or incomplete data, inherent noise in the data, trust or reputation scores assigned to data based on their sources of origin, or confidences in predictions made using automated modeling tools. The input uncertainties naturally lead to uncertainties in the results of any queries or analyses performed on such data. To enable robust and systematic reasoning over such uncertain query results, efficient algorithms and practical tools are developed to: (a) identify the input uncertainties to which query results are most sensitive, (b) decide how to use scarce resources like subject matter experts to resolve uncertainties in query results, and (c) incorporate user feedback to improve the robustness of the input uncertainty parameters themselves.  The tools have the potential to make it easy and intuitive to process and analyze uncertain data and extract useful information from it in a wide range of real-world application domains including social media analysis, scientific and biological data management, sensor data management, web data integration, and information extraction. This project provides research opportunities for graduate and undergraduate students, and is aligned with several advanced graduate courses offered by the PIs.  The prototype implementation of the framework, publications, and experimental data, will be disseminated via the project web site: http://www.cs.umd.edu/~amol/RPrDB.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Amol",
   "pi_last_name": "Deshpande",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Amol V Deshpande",
   "pi_email_addr": "amol@cs.umd.edu",
   "nsf_id": "000486255",
   "pi_start_date": "2012-09-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland College Park",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 84494.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 164133.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This NSF-funded project was motivated by the observation that, much of the data that needs to be processed and analyzed in today's \"big data\" world is inherently uncertain, imprecise, and probabilistic in nature, and the impacts of such uncertainty on the results or the outcomes of an analysis are hard to predict or reason about. Uncertainty is often inherent in the raw data itself, especially in data that is autonomously generated by IoT devices or scientific instruments, or that is collected from a variety of data sources with unknown provenance on the Internet; it is often also introduced as a result of running statistical or machine learning tools to extract higher-level events and knowledge from data. This abundance of uncertain data has led to a vast body of literature in the database community within the last decade, focused on developing techniques for managing and querying such data. Although the prior work on \"probabilistic databases\" shows us how to annotate the data with such uncertainties, and how to propagate those uncertainties through different types of analyses or querying, this in itself is not sufficient to make probabilistic databases useful as a tool in practice. In particular, we need to understand how to: (a) resolve uncertainties in query results by gathering more information so that the decision-makers can have more confidence in their decisions, (b) analyze sensitivity of the output of a specific query to the uncertainties in the input, and (c) meaningfully interpret the numeric uncertainty scores as probabilities. The goal of this project was to develop a systematic framework to address these challenges to enable \"robust\" probabilistic data management.<br /><br />The key outcomes of the project can be summarized as follows. It resulted in the development of a collection of algorithms, in many cases with provable approximation guarantees, for query-driven uncertainty resolution; here the goal is to minimize the expected cost of fully determining the value of a given Boolean function on uncertain inputs. Such algorithms were developed for a variety of different classes of Boolean functions, including linear threshold functions where a linear function over the uncertain inputs is compared to a threshold value, and Boolean formulas containing conjunctions and/or disjunctions. Several of the algorithms are designed for the case where a \"sample\" of the input space is provided, which does not require making strict independence assumptions about how the uncertainties of different inputs relate to each other. We also designed a general framework to resolve uncertainty for a collection of ranking and partitioning problems, where the goal is to rank or partition a set of objects using incomplete information about user preferences. Finally, we also demonstrated that the effects of \"uncalibrated\" probabilities can be significant, by showing that simple transformations of numeric uncertainty scores can result in complex transformations on the results of simple queries. Key results from this project were published in conference and journal papers in Computer Science. This project also trained several graduate and undergraduate students to do research.<br /><br />Overall this project has made significant advances in our ability to query and reason about uncertain data in a robust manner, and has thus paved the way for incorporating uncertainty in commercial database systems. More details about the project, and the publications that resulted from it, can be found at: http://www.cs.umd.edu/~amol/DBGroup/RobustPrDB.html</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/06/2017<br>\n\t\t\t\t\tModified by: Amol&nbsp;V&nbsp;Deshpande</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis NSF-funded project was motivated by the observation that, much of the data that needs to be processed and analyzed in today's \"big data\" world is inherently uncertain, imprecise, and probabilistic in nature, and the impacts of such uncertainty on the results or the outcomes of an analysis are hard to predict or reason about. Uncertainty is often inherent in the raw data itself, especially in data that is autonomously generated by IoT devices or scientific instruments, or that is collected from a variety of data sources with unknown provenance on the Internet; it is often also introduced as a result of running statistical or machine learning tools to extract higher-level events and knowledge from data. This abundance of uncertain data has led to a vast body of literature in the database community within the last decade, focused on developing techniques for managing and querying such data. Although the prior work on \"probabilistic databases\" shows us how to annotate the data with such uncertainties, and how to propagate those uncertainties through different types of analyses or querying, this in itself is not sufficient to make probabilistic databases useful as a tool in practice. In particular, we need to understand how to: (a) resolve uncertainties in query results by gathering more information so that the decision-makers can have more confidence in their decisions, (b) analyze sensitivity of the output of a specific query to the uncertainties in the input, and (c) meaningfully interpret the numeric uncertainty scores as probabilities. The goal of this project was to develop a systematic framework to address these challenges to enable \"robust\" probabilistic data management.\n\nThe key outcomes of the project can be summarized as follows. It resulted in the development of a collection of algorithms, in many cases with provable approximation guarantees, for query-driven uncertainty resolution; here the goal is to minimize the expected cost of fully determining the value of a given Boolean function on uncertain inputs. Such algorithms were developed for a variety of different classes of Boolean functions, including linear threshold functions where a linear function over the uncertain inputs is compared to a threshold value, and Boolean formulas containing conjunctions and/or disjunctions. Several of the algorithms are designed for the case where a \"sample\" of the input space is provided, which does not require making strict independence assumptions about how the uncertainties of different inputs relate to each other. We also designed a general framework to resolve uncertainty for a collection of ranking and partitioning problems, where the goal is to rank or partition a set of objects using incomplete information about user preferences. Finally, we also demonstrated that the effects of \"uncalibrated\" probabilities can be significant, by showing that simple transformations of numeric uncertainty scores can result in complex transformations on the results of simple queries. Key results from this project were published in conference and journal papers in Computer Science. This project also trained several graduate and undergraduate students to do research.\n\nOverall this project has made significant advances in our ability to query and reason about uncertain data in a robust manner, and has thus paved the way for incorporating uncertainty in commercial database systems. More details about the project, and the publications that resulted from it, can be found at: http://www.cs.umd.edu/~amol/DBGroup/RobustPrDB.html\n\n\t\t\t\t\tLast Modified: 01/06/2017\n\n\t\t\t\t\tSubmitted by: Amol V Deshpande"
 }
}