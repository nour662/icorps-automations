{
 "awd_id": "1217023",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Fast Rate-Efficient Codes for Data Compression and Transmission via Sparse Regression",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Richard Brown",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 499498.0,
 "awd_amount": 499498.0,
 "awd_min_amd_letter_date": "2012-08-27",
 "awd_max_amd_letter_date": "2012-08-27",
 "awd_abstract_narration": "Modern communication networks are constantly growing in size and sophistication. New applications require these networks to be reliable, computationally efficient, and have small latency. To meet these demands it is critical to have low-complexity, rate-efficient codes for communication and compression. Since Shannon's fundamental results, there has been a flurry of activity to design capacity achieving, computationally efficient coding schemes. For the channel coding problem, it was not until the early 1990s that capacity achieving codes were implemented. Similarly, many good quantizer designs were developed for lossy compression, but unfortunately none of these low-complexity codes provably attain the rate-distortion bound. The divergence between information-theoretic results and code construction is even more pronounced in network communication problems. Despite a sharp characterization of information-theoretic limits for several network models, the best practical codes for these problems fall short of the capacity limits.\r\n\r\nThis research involves the development of computationally efficient codes for Gaussian sources and channels. To this end we leverage recent advances in high-dimensional sparse regression. These are the first low-complexity codes that provably attain the information-theoretic limits codes for Gaussian sources and channels. The main objectives of this research project are: (1) Determining the fundamental limits of sparse regression codes in a variety of communication theoretic settings; (2) Developing low-complexity encoding and decoding schemes for our sparse regression codes. This part of the project draws on ideas from function approximation and sparse signal recovery. The project also provides an opportunity for training graduate students and postdoctoral researchers in the disciplines of communication theory, data compression, statistics and networks.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sekhar",
   "pi_last_name": "Tatikonda",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Sekhar C Tatikonda",
   "pi_email_addr": "sekhar.tatikonda@yale.edu",
   "nsf_id": "000359845",
   "pi_start_date": "2012-08-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "10 Hillhouse Av",
  "perf_city_name": "New Haven",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208284",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 499498.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major goal of this project is to design and analyze sparse regression codes for a variety of communication settings including point-to-point compression and channel coding as well as multi-terminal settings.&nbsp; The project involves&nbsp; the design of sparse regression codes, the empirical evaluation of these codes, and the theoretical analysis of these codes.&nbsp; Specifically:&nbsp; 1. Theoretical analysis of maximum likelihood (ML) encoding for the Gaussian lossy compression problem.&nbsp;&nbsp; We have shown that under the optimal ML decoder we can achieve all rates up to Shannon's rate distortion bound.&nbsp; Thus there is no loss in rate when using the sparse regression code ensemble.&nbsp; 2. Development of practical encoding schemes for the Gaussian lossy compression problem.&nbsp; We developed a greedy algorithm and an approximate&nbsp; message passing algorithm that work well in practice.&nbsp; 3. Extension of SPARC codes to settings with side-information.&nbsp; We have developed a sparse regression approach to binning.&nbsp; This allows us to treat&nbsp; the Gelfand-Pinsker and Wyner-Ziv problems.&nbsp; One can also combine SPARC codes via superposition thus&nbsp; we can treat multiple access channels and broadcast channels.&nbsp; Broader impact:&nbsp; This research has been disseminated via archival journal publications, conference presentations, and tutorial sessions.&nbsp; Graduate students, undergraduate students, and postdoctoral fellows get exposure to both real-world communication problems and cutting edge theory.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/23/2017<br>\n\t\t\t\t\tModified by: Sekhar&nbsp;C&nbsp;Tatikonda</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe major goal of this project is to design and analyze sparse regression codes for a variety of communication settings including point-to-point compression and channel coding as well as multi-terminal settings.  The project involves  the design of sparse regression codes, the empirical evaluation of these codes, and the theoretical analysis of these codes.  Specifically:  1. Theoretical analysis of maximum likelihood (ML) encoding for the Gaussian lossy compression problem.   We have shown that under the optimal ML decoder we can achieve all rates up to Shannon's rate distortion bound.  Thus there is no loss in rate when using the sparse regression code ensemble.  2. Development of practical encoding schemes for the Gaussian lossy compression problem.  We developed a greedy algorithm and an approximate  message passing algorithm that work well in practice.  3. Extension of SPARC codes to settings with side-information.  We have developed a sparse regression approach to binning.  This allows us to treat  the Gelfand-Pinsker and Wyner-Ziv problems.  One can also combine SPARC codes via superposition thus  we can treat multiple access channels and broadcast channels.  Broader impact:  This research has been disseminated via archival journal publications, conference presentations, and tutorial sessions.  Graduate students, undergraduate students, and postdoctoral fellows get exposure to both real-world communication problems and cutting edge theory.\n\n\t\t\t\t\tLast Modified: 01/23/2017\n\n\t\t\t\t\tSubmitted by: Sekhar C Tatikonda"
 }
}