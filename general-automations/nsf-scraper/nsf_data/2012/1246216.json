{
 "awd_id": "1246216",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CC-NIE Network Infrastructure: Advancing Network Capacity, Efficiency, and Security for Wisconsin Big Data Research Through Improvement of campus research DMZ",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 491328.0,
 "awd_amount": 491328.0,
 "awd_min_amd_letter_date": "2012-09-12",
 "awd_max_amd_letter_date": "2012-09-12",
 "awd_abstract_narration": "Powerful networks are the thread that holds together the computing and data resources that underpin modern scientific discovery. With the ever-increasing demands on bandwidth to support the analysis of the ever-growing volumes of scientific data, this project will transition the University of Wisconsin - Madison (UW-Madison) research community to a new era of networking technologies and capabilities.\r\n\r\nSoftware Defined Network (SDN) technologies will be employed to establish the UW-Madison science DMZ that will support up to 100Gb/sec data transfer rate to universities and research institutions across the US through Internet2.  Integral to this new campus wide resource will be the ability to use emerging networking protocols like OpenFlow to dynamically provision communication bandwidth between departmental resources and the campus DMZ.  \r\n\r\nA new campus wide team of both central IT and research staff will facilitate the deployment and operation of these new capabilities. This Advanced Network Team (ANT) will be responsible for the integration of the new networking and software capabilities with the UW CyberInfrastructure. An important element of the ANT activity will be to monitor networking activities on the campus and guide research and development of new distributed computing technologies.\r\n\r\nBroader impacts: This will benefit all campus scholarly activities including other NSF sponsored projects, activities funded by other federal agencies, and other campus scholarship including the social sciences, humanities, and arts. The expertise gained will be disseminated throughout the UW System, the Committee for Institutional Cooperation, and the WiscNet regional network, which serves higher education, K-12, and public libraries.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bruce",
   "pi_last_name": "Maas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bruce Maas",
   "pi_email_addr": "bruce.maas@cio.wisc.edu",
   "nsf_id": "000620171",
   "pi_start_date": "2012-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Miron",
   "pi_last_name": "Livny",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Miron Livny",
   "pi_email_addr": "miron@cs.wisc.edu",
   "nsf_id": "000340383",
   "pi_start_date": "2012-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "1210 W Dayton St",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537061613",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "808000",
   "pgm_ele_name": "Campus Cyberinfrastructure"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1640",
   "pgm_ref_txt": "INFORMATION TECHNOLOGY RESEARC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 491328.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The UW-Madison campus network is designed to support data exchanges between our scientists and their off campus collaborators and accommodate data movement for computationally-intensive analysis on campus. Improvements to the network facilitated by the &ldquo;CCNIE Network Infrastructure: Advancing Network Capacity, Efficiency, and Security for Wisconsin Big Data Research Through Improvement of the Campus Research DMZ&rdquo; project enabled sustained network bandwidth utilization of up to 90 Gbps and seamless data movement between departments and the computing resources offered by the UW Center for High Throughput Computing (CHTC), where over a million compute hours are delivered daily to researchers on and off campus.&nbsp;</p>\n<p><br />UW-Madison is a Tier 2 institution in the Large Hadron Collider (LHC) international collaboration. The CCNIE project established a high-speed connection between the campus and ESnet, FermiLab, and institutions in the Open Science Grid (OSG), resulting in more efficient processing and analysis of data from particle collisions in the LHC&rsquo;s Run 2 by UW-Madison investigators in the Compact Muon Solenoid and ATLAS experiments. The upgraded network supported transfer of 3.2 Petabytes of data to storage in UW-Madison during 2016, producing sustained peaks of around 60-90 Gbps. These upgrades provided the performance required for UW-Madison to join the LHC Open Network Environment to improve routing arrangements between UW-Madison and other institutions involved in LHC analysis and further support data flows between Tier 2 institutions.</p>\n<p><br />UW-Madison is the lead institution for the South Pole IceCube neutrino observatory. IceCube simulations rely on distributed computing resources. Input and output data of those distributed jobs is stored at UW-Madison. Use of pooled resources across the Open Science Grid (OSG) has been growing, making high bandwidth connections enabled by Science DMZ improvements increasingly critical for running jobs and accessing data. A distributed infrastructure is also used for storing IceCube data. The high bandwidth campus backbone has made possible large data flows between UW-Madison and centers at Lawrence Berkeley National Laboratory/National Energy Research Scientific Computing Center and the Deutsches Elektronen-Synchrotron (DESY). As data is processed at UW-Madison, data products are transferred to these locations for long term storage and preservation. This data replication service started operating in September 2016 and will keep a steady data flow between UW-Madison and these centers in the future. In addition, the hundreds of Terabytes of raw data recorded by the detector and stored at UW-Madison can now be effectively transferred to these archival locations over the Science DMZ.</p>\n<p><br />Network bandwidth improvements were noticed by scientists in Space Science and Engineering who provide and distribute near real time processed data and products from the Visible Infrared Imager Radiometer Suites (VIIRS) via network links that had been exhibiting high error rates due to insufficient capacity. With 100 Gb connectivity to the Internet came significant improvements that allowed NASA scientists to experience much lower error rates and more efficient transfers of processed satellite data from UW-Madison for use in the Suomi NPP Mission. During Hurricane Matthew in fall 2016, enhanced network reliability gave weather forecasters and the public uninterrupted access to real-time imagery, analysis products, and intensity estimates provided by the Tropic Cyclones Tracking group at the Cooperative Institute for Meteorological Satellite Studies. A new NASA project, the Science Investigator-led Processing System, is underway. it also relies heavily on network performance.</p>\n<p><br />The CHTC operates compute and storage resources distributed throughout campus. This CCNIE project upgraded the connections between these resources and the rest of campus to 10 Gb and resulted in doubling of throughput and sustained outbound spikes of 14 Gbps on this internal network. The network is used annually by more than 150 research groups to move data to and from the CHTC processing resources that have delivered more than 330M compute hours in the past 12 months. More than 10% of these cycles were provided by off campus OSG sites. Because many researchers submit jobs to CHTC resources from their home departments, departmental firewalls present an impediment to throughput improvements. &nbsp;Simulations of job submissions using a software defined network controller provided evidence that multiple concurrent computational jobs on CHTC resources using data hosted behind firewalls can be accelerated using paths that bypass firewalls.</p>\n<p><br />The UW-Madison Science DMZ traffic typically has flows around 40-60 Gbps, with peaks around 95Gbps in a single direction. Because scientific computing workflows are susceptible to cyberattacks, (https://sciencenode.org/feature/what-does-security-mean-in-science-today.php?clicked=title), this project included monitoring Science DMZ traffic for intrusions using Bro infrastructure. Campus investments made in 2016 provided the capability for monitoring nearly 30 Gbps of traffic in Bro and up to 80Gbps of traffic for flows. Monitoring of the science DMZ is a component of a larger Advanced Threat Protection project which will have the capability for shunting scientific data traffic through the Gigamon architecture using its automated packet broker appliance.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/19/2016<br>\n\t\t\t\t\tModified by: Bruce&nbsp;Maas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe UW-Madison campus network is designed to support data exchanges between our scientists and their off campus collaborators and accommodate data movement for computationally-intensive analysis on campus. Improvements to the network facilitated by the \"CCNIE Network Infrastructure: Advancing Network Capacity, Efficiency, and Security for Wisconsin Big Data Research Through Improvement of the Campus Research DMZ\" project enabled sustained network bandwidth utilization of up to 90 Gbps and seamless data movement between departments and the computing resources offered by the UW Center for High Throughput Computing (CHTC), where over a million compute hours are delivered daily to researchers on and off campus. \n\n\nUW-Madison is a Tier 2 institution in the Large Hadron Collider (LHC) international collaboration. The CCNIE project established a high-speed connection between the campus and ESnet, FermiLab, and institutions in the Open Science Grid (OSG), resulting in more efficient processing and analysis of data from particle collisions in the LHC?s Run 2 by UW-Madison investigators in the Compact Muon Solenoid and ATLAS experiments. The upgraded network supported transfer of 3.2 Petabytes of data to storage in UW-Madison during 2016, producing sustained peaks of around 60-90 Gbps. These upgrades provided the performance required for UW-Madison to join the LHC Open Network Environment to improve routing arrangements between UW-Madison and other institutions involved in LHC analysis and further support data flows between Tier 2 institutions.\n\n\nUW-Madison is the lead institution for the South Pole IceCube neutrino observatory. IceCube simulations rely on distributed computing resources. Input and output data of those distributed jobs is stored at UW-Madison. Use of pooled resources across the Open Science Grid (OSG) has been growing, making high bandwidth connections enabled by Science DMZ improvements increasingly critical for running jobs and accessing data. A distributed infrastructure is also used for storing IceCube data. The high bandwidth campus backbone has made possible large data flows between UW-Madison and centers at Lawrence Berkeley National Laboratory/National Energy Research Scientific Computing Center and the Deutsches Elektronen-Synchrotron (DESY). As data is processed at UW-Madison, data products are transferred to these locations for long term storage and preservation. This data replication service started operating in September 2016 and will keep a steady data flow between UW-Madison and these centers in the future. In addition, the hundreds of Terabytes of raw data recorded by the detector and stored at UW-Madison can now be effectively transferred to these archival locations over the Science DMZ.\n\n\nNetwork bandwidth improvements were noticed by scientists in Space Science and Engineering who provide and distribute near real time processed data and products from the Visible Infrared Imager Radiometer Suites (VIIRS) via network links that had been exhibiting high error rates due to insufficient capacity. With 100 Gb connectivity to the Internet came significant improvements that allowed NASA scientists to experience much lower error rates and more efficient transfers of processed satellite data from UW-Madison for use in the Suomi NPP Mission. During Hurricane Matthew in fall 2016, enhanced network reliability gave weather forecasters and the public uninterrupted access to real-time imagery, analysis products, and intensity estimates provided by the Tropic Cyclones Tracking group at the Cooperative Institute for Meteorological Satellite Studies. A new NASA project, the Science Investigator-led Processing System, is underway. it also relies heavily on network performance.\n\n\nThe CHTC operates compute and storage resources distributed throughout campus. This CCNIE project upgraded the connections between these resources and the rest of campus to 10 Gb and resulted in doubling of throughput and sustained outbound spikes of 14 Gbps on this internal network. The network is used annually by more than 150 research groups to move data to and from the CHTC processing resources that have delivered more than 330M compute hours in the past 12 months. More than 10% of these cycles were provided by off campus OSG sites. Because many researchers submit jobs to CHTC resources from their home departments, departmental firewalls present an impediment to throughput improvements.  Simulations of job submissions using a software defined network controller provided evidence that multiple concurrent computational jobs on CHTC resources using data hosted behind firewalls can be accelerated using paths that bypass firewalls.\n\n\nThe UW-Madison Science DMZ traffic typically has flows around 40-60 Gbps, with peaks around 95Gbps in a single direction. Because scientific computing workflows are susceptible to cyberattacks, (https://sciencenode.org/feature/what-does-security-mean-in-science-today.php?clicked=title), this project included monitoring Science DMZ traffic for intrusions using Bro infrastructure. Campus investments made in 2016 provided the capability for monitoring nearly 30 Gbps of traffic in Bro and up to 80Gbps of traffic for flows. Monitoring of the science DMZ is a component of a larger Advanced Threat Protection project which will have the capability for shunting scientific data traffic through the Gigamon architecture using its automated packet broker appliance. \n\n \n\n\t\t\t\t\tLast Modified: 12/19/2016\n\n\t\t\t\t\tSubmitted by: Bruce Maas"
 }
}