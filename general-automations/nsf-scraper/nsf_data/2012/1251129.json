{
 "awd_id": "1251129",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: EAGER: An Integrated Framework for Performance and Reliability in Large-scaled Computing Systems",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 272351.0,
 "awd_amount": 272351.0,
 "awd_min_amd_letter_date": "2012-08-24",
 "awd_max_amd_letter_date": "2012-08-24",
 "awd_abstract_narration": "Large-scale computing environments such as data centers and cloud computing are becoming the core computing infrastructure, making the availability of such services extremely critical. However, these environments are increasingly vulnerable to both hardware and software failures. This project designs failure-aware techniques for modeling, prediction, and resource management in large-scale computing environments with the presence of hardware and software failures at various levels. Intellectually, this project develops fundamental understanding of workload and reliability characteristics, and investigates how improved capacity planning models and prediction techniques can obtain useful information for system design and maintenance. This project further provides insights of the impact of software/hardware component failures in the area of resource management. \r\n\r\nThe results of this project will include new capacity planning models that evaluate both reliability and performance of a given system and new prediction techniques that forecast the future failure occurrences by taking advantage of temporal dependence in failure events. Based on the modeling and prediction techniques, this project will develop new failure-aware runtime strategies for job scheduling, node allocation, and system maintenance, aiming to achieve high system performance and reliability in complex large scale systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ningfang",
   "pi_last_name": "Mi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ningfang Mi",
   "pi_email_addr": "ningfang@ece.neu.edu",
   "nsf_id": "000548527",
   "pi_start_date": "2012-08-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Ave.",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 272351.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The main goal of this research work is to address the fundamental challenges (with respect to performance and reliability) in large-scaled cluster systems, like data centers and cloud computing. To accomplish this goal, we design and implement new techniques and schemes for resource management in such cluster systems that have huge positive impacts on different industry sectors and our everyday lives.</p>\n<p>&nbsp;</p>\n<p><strong>Intellectual Merit:</strong> The key contributions of this proposal include developing fundamental understanding of workload and reliability characteristics in large-scaled cluster systems, learning how improved performance models and prediction techniques can obtain useful information for system design and maintenance, and providing new runtime schemes for resource allocation and job scheduling in cluster systems.</p>\n<p>&nbsp;</p>\n<p><strong>Broader Impact:</strong> In this research project, we devote the outreach agenda to developing an education module centered on applications that have large-scale computing challenges. As a woman, the PI naturally attracts women undergraduates and graduate students interested in computer engineering. We aggressively motivate young female students towards science and engineering. We continue the participation in the Boston Area Girls STEM Collaborative through the College of Engineering at NEU and extend our influence to middle school girls. As part of this project, we incorporate the concepts of failure-aware resource management into the existing courses of computer architecture, capacity planning, and performance evaluation at both the graduate and undergraduate levels. This project offers a natural vehicle in place for technology transfer by building collaborations with a number of companies (e.g., HP, IBM, VMware) on this project.</p>\n<p><strong>Project Outcomes:</strong> &nbsp;During the project years, we have achieved the following outcomes on both research and education. First, we focus the development of new load balancing algorithms to efficiently balance the computational load among servers in a large-scaled cluster system. Large-scaled cluster systems have been employed in various areas by offering pools of fundamental resources. Efficient allocation of the shared resources in a cluster system is a critical but challenging issue, which has been extensively studied in the past few years. However, we found that performance benefits of the existing policies (e.g., Join Shortest Queue and size-based polices) diminish when workloads are highly variable and temporally correlated. Thus, we designed a new load balancing policy, which attempts to partition jobs according to their present sizes and further rank the servers based on their loads. By dispatching jobs of similar sizes to the corresponding ranked servers, this scheduler can adaptively balance user traffic and system load in a cluster and thus achieve significant performance benefits.</p>\n<p>Second, we investigate new approaches for data placement and data migration in data centers with tiered storage systems. One popular approach of leveraging Flash technology in the virtual machine environment today is using it as a secondary-level host-side cache. Although this approach delivers I/O acceleration for a single VM workload, it might not be able to fully exploit the outstanding performance of Flash and justify the high cost-per-GB of Flash resources. We designed a new VMware Flash Resource Manager, which aims to maximize the utilization of Flash resources with minimal CPU, memory and I/O cost for managing and operating Flash. It borrows the ideas of heating and cooling from thermodynamics to identify the data blocks that benefit most from being put on Flash, and lazily and asynchronously migrates data blocks between Flash and spinning disks.</p>\n<p>Third, we focus on the scheduling problem for parallel data processing applications in a Mapre...",
  "por_txt_cntn": "\nThe main goal of this research work is to address the fundamental challenges (with respect to performance and reliability) in large-scaled cluster systems, like data centers and cloud computing. To accomplish this goal, we design and implement new techniques and schemes for resource management in such cluster systems that have huge positive impacts on different industry sectors and our everyday lives.\n\n \n\nIntellectual Merit: The key contributions of this proposal include developing fundamental understanding of workload and reliability characteristics in large-scaled cluster systems, learning how improved performance models and prediction techniques can obtain useful information for system design and maintenance, and providing new runtime schemes for resource allocation and job scheduling in cluster systems.\n\n \n\nBroader Impact: In this research project, we devote the outreach agenda to developing an education module centered on applications that have large-scale computing challenges. As a woman, the PI naturally attracts women undergraduates and graduate students interested in computer engineering. We aggressively motivate young female students towards science and engineering. We continue the participation in the Boston Area Girls STEM Collaborative through the College of Engineering at NEU and extend our influence to middle school girls. As part of this project, we incorporate the concepts of failure-aware resource management into the existing courses of computer architecture, capacity planning, and performance evaluation at both the graduate and undergraduate levels. This project offers a natural vehicle in place for technology transfer by building collaborations with a number of companies (e.g., HP, IBM, VMware) on this project.\n\nProject Outcomes:  During the project years, we have achieved the following outcomes on both research and education. First, we focus the development of new load balancing algorithms to efficiently balance the computational load among servers in a large-scaled cluster system. Large-scaled cluster systems have been employed in various areas by offering pools of fundamental resources. Efficient allocation of the shared resources in a cluster system is a critical but challenging issue, which has been extensively studied in the past few years. However, we found that performance benefits of the existing policies (e.g., Join Shortest Queue and size-based polices) diminish when workloads are highly variable and temporally correlated. Thus, we designed a new load balancing policy, which attempts to partition jobs according to their present sizes and further rank the servers based on their loads. By dispatching jobs of similar sizes to the corresponding ranked servers, this scheduler can adaptively balance user traffic and system load in a cluster and thus achieve significant performance benefits.\n\nSecond, we investigate new approaches for data placement and data migration in data centers with tiered storage systems. One popular approach of leveraging Flash technology in the virtual machine environment today is using it as a secondary-level host-side cache. Although this approach delivers I/O acceleration for a single VM workload, it might not be able to fully exploit the outstanding performance of Flash and justify the high cost-per-GB of Flash resources. We designed a new VMware Flash Resource Manager, which aims to maximize the utilization of Flash resources with minimal CPU, memory and I/O cost for managing and operating Flash. It borrows the ideas of heating and cooling from thermodynamics to identify the data blocks that benefit most from being put on Flash, and lazily and asynchronously migrates data blocks between Flash and spinning disks.\n\nThird, we focus on the scheduling problem for parallel data processing applications in a Mapreduce cluster. The MapReduce framework has become the de facto scheme for scalable semi-structured and un-structured data processing in recent years. The Hadoop ecosystem ..."
 }
}