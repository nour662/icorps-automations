{
 "awd_id": "1239031",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CPS: Synergy: Collaborative Research: SensEye: An Architecture for Ubiquitous, Real-Time Visual Context Sensing and Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2016-09-30",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2012-09-10",
 "awd_max_amd_letter_date": "2012-09-10",
 "awd_abstract_narration": "Continuous real-time tracking of the eye and field-of-view of an individual is profoundly important to understanding how humans perceive and interact with the physical world.  This work advances both the technology and engineering of cyber-physical systems by designing an innovative paradigm involving next-generation computational eyeglasses that interact with a user's mobile phone to provide the capability for real-time visual context sensing and inference. This research integrates novel research into low-power embedded systems, image representation, image processing and machine learning, and mobile sensing and inference, to advance the state-of-art in continuous sensing for CPS applications. The activity addresses several fundamental research challenges including: 1) design of novel, highly integrated, computational eyeglasses for tracking eye movements, the visual field of a user, and head movement patterns, all in real-time; 2) a unified compressive signal processing framework that optimizes sensing and estimation, while enabling re-targeting of the device to perform a broad range of tasks depending on the needs of an application; 3) design of a novel real-time visual context sensing system that extracts high-level contexts of interest from compressed data representations; and 4) a layer of intelligence that combines contexts extracted from the computational eyeglass together with contexts obtained from the mobile phone to improve energy-efficiency and sensing accuracy.\r\n\r\nThis technology can revolutionize a range of disciplines including transportation, healthcare, behavioral science and market research. Continuous monitoring of the eye and field-of-view of an individual can enable detection of hazardous behaviors such as drowsiness while driving, mental health issues such as schizophrenia, addictive behavior and substance abuse, neurological disease progression, head injuries, and others. The research provides the foundations for such applications through the design of a prototype platform together with real-time sensor processing algorithms, and making these systems available through open source venues for broader use. Outreach for this project includes demonstrations of the device at science fairs for high-school students, and integration of the platform into undergraduate and graduate courses.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Prabal",
   "pi_last_name": "Dutta",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Prabal Dutta",
   "pi_email_addr": "prabal@berkeley.edu",
   "nsf_id": "000542043",
   "pi_start_date": "2012-09-10",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Michigan Ann Arbor",
  "perf_str_addr": "3003 S. State St.",
  "perf_city_name": "Ann Arbor",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091271",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "791800",
   "pgm_ele_name": "CPS-Cyber-Physical Systems"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7918",
   "pgm_ref_txt": "CYBER-PHYSICAL SYSTEMS (CPS)"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 300000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>It has been written that the eyes are the window to the soul. &nbsp;But the eyes also reflect the outside world, and visual attention, as captured through one's gaze. &nbsp;In this project, we explore the viability and potential of continuously tracking pupil position (and other eye parameters) and projecting it onto the scene as seen by one's eyes. Such an ability to track one's gaze could be revolutionary by letting our computers see what we actually see. &nbsp;But intellectual challenges abound: power, portability, perception, and performance.</p>\n<p>One of the most challenging aspects of this project is how to operate two cameras, a computing system, and communications on a power budget befitting a set of eyeglasses. &nbsp;Fitting all of these components into a portable form factor is another challenge. &nbsp;Balancing these challenges is the need for effective perception and the computational performanceit requires. &nbsp;We collectively address these interlocking problems with an integrated approach that co-optimizes the entire system. &nbsp;We choose a pair of low resolution cameras, which drives down power draw. &nbsp;We eschew a traditional microprocessor for an FPGA accelerator for image acquisition which offers higher performance at a lower power for the given workload. &nbsp;We use compressive sensing to discard 90% of the image pixels which reduces gaze tracking accuracy by less than 10% but provides substantial power savings by reducing data throughput by nearly 90%. &nbsp;And we multiplex the hardware to better amortize fixed overhead costs like static power dissipation. &nbsp;Overall, this project demonstrates the viability of eyeglass-integrated gaze tracking, although miniaturization, attractive packaging, and greater processing remain future exercises.</p>\n<p><br />Real-time tracking of the eye can help detect a range of unsafe behaviors, including drowsiness while driving, lack of attention whileusing a medical device, and distractions while crossing a road. The benefits extend to personal health, where the state of the eye provides a continuous window into Parkinson&rsquo;s disease progression, psychiatric disorders such as schizophrenia, drug use, head injuries and concussions, and others. Real-time gaze direction tracking can also introduce new \"hands-free\" ways of interacting with computers ordisplays, and provide an accurate method of gauging user intent to drive context-aware advertising. &nbsp;Looking ahead, we believe that our platform has substantial commercial appeal. One area of interest is public safety while driving while monitoring driver behavior and hazardous driving patterns (whether due to alcohol, sleepiness, inattention, or other). &nbsp;Beyond science and technology, many wearable technologies such as activity monitors and physiological sensors are widely available in the consumer space and we believe that this workcan lead to a similar technology being available for eye and gazetracking.<br /><br /></p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/30/2017<br>\n\t\t\t\t\tModified by: Prabal&nbsp;Dutta</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160752622_1_wearable--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160752622_1_wearable--rgov-800width.jpg\" title=\"Wearable Gaze Tracker\"><img src=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160752622_1_wearable--rgov-66x44.jpg\" alt=\"Wearable Gaze Tracker\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Wearable. The SensEye device being worn and shown in perspective.</div>\n<div class=\"imageCredit\">Russ Bielawski</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Prabal&nbsp;Dutta</div>\n<div class=\"imageTitle\">Wearable Gaze Tracker</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160842436_2_processor--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160842436_2_processor--rgov-800width.jpg\" title=\"Image Acquisition Processor\"><img src=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160842436_2_processor--rgov-66x44.jpg\" alt=\"Image Acquisition Processor\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Processor.  The initial microcontroller based computer board.</div>\n<div class=\"imageCredit\">Russ Bielawski</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Prabal&nbsp;Dutta</div>\n<div class=\"imageTitle\">Image Acquisition Processor</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160962412_3_breadboard--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160962412_3_breadboard--rgov-800width.jpg\" title=\"Breadboard\"><img src=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496160962412_3_breadboard--rgov-66x44.jpg\" alt=\"Breadboard\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Breadboard.  A prototype breadboard with FPGA image processing and integrated wireless communications.</div>\n<div class=\"imageCredit\">Prabal Dutta</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Prabal&nbsp;Dutta</div>\n<div class=\"imageTitle\">Breadboard</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496161048249_4_frames--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496161048249_4_frames--rgov-800width.jpg\" title=\"Eyeglass Frames Build Using PCBs\"><img src=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496161048249_4_frames--rgov-66x44.jpg\" alt=\"Eyeglass Frames Build Using PCBs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Frames. A printed circuit board version of the wearable frames.</div>\n<div class=\"imageCredit\">Prabal Dutta</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Prabal&nbsp;Dutta</div>\n<div class=\"imageTitle\">Eyeglass Frames Build Using PCBs</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496161154513_5_tracking--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496161154513_5_tracking--rgov-800width.jpg\" title=\"Gaze Tracking\"><img src=\"/por/images/Reports/POR/2017/1239031/1239031_10212727_1496161154513_5_tracking--rgov-66x44.jpg\" alt=\"Gaze Tracking\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Tracking. Actual (green diamond) and estimates (blue circle) gaze.</div>\n<div class=\"imageCredit\">Russ Bielawski</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">Prabal&nbsp;Dutta</div>\n<div class=\"imageTitle\">Gaze Tracking</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIt has been written that the eyes are the window to the soul.  But the eyes also reflect the outside world, and visual attention, as captured through one's gaze.  In this project, we explore the viability and potential of continuously tracking pupil position (and other eye parameters) and projecting it onto the scene as seen by one's eyes. Such an ability to track one's gaze could be revolutionary by letting our computers see what we actually see.  But intellectual challenges abound: power, portability, perception, and performance.\n\nOne of the most challenging aspects of this project is how to operate two cameras, a computing system, and communications on a power budget befitting a set of eyeglasses.  Fitting all of these components into a portable form factor is another challenge.  Balancing these challenges is the need for effective perception and the computational performanceit requires.  We collectively address these interlocking problems with an integrated approach that co-optimizes the entire system.  We choose a pair of low resolution cameras, which drives down power draw.  We eschew a traditional microprocessor for an FPGA accelerator for image acquisition which offers higher performance at a lower power for the given workload.  We use compressive sensing to discard 90% of the image pixels which reduces gaze tracking accuracy by less than 10% but provides substantial power savings by reducing data throughput by nearly 90%.  And we multiplex the hardware to better amortize fixed overhead costs like static power dissipation.  Overall, this project demonstrates the viability of eyeglass-integrated gaze tracking, although miniaturization, attractive packaging, and greater processing remain future exercises.\n\n\nReal-time tracking of the eye can help detect a range of unsafe behaviors, including drowsiness while driving, lack of attention whileusing a medical device, and distractions while crossing a road. The benefits extend to personal health, where the state of the eye provides a continuous window into Parkinson?s disease progression, psychiatric disorders such as schizophrenia, drug use, head injuries and concussions, and others. Real-time gaze direction tracking can also introduce new \"hands-free\" ways of interacting with computers ordisplays, and provide an accurate method of gauging user intent to drive context-aware advertising.  Looking ahead, we believe that our platform has substantial commercial appeal. One area of interest is public safety while driving while monitoring driver behavior and hazardous driving patterns (whether due to alcohol, sleepiness, inattention, or other).  Beyond science and technology, many wearable technologies such as activity monitors and physiological sensors are widely available in the consumer space and we believe that this workcan lead to a similar technology being available for eye and gazetracking.\n\n\n\n \n\n\t\t\t\t\tLast Modified: 05/30/2017\n\n\t\t\t\t\tSubmitted by: Prabal Dutta"
 }
}