{
 "awd_id": "1201790",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Super-Turing Computation and Brain-Like Intelligence",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Radhakisan Baheti",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2016-06-30",
 "tot_intn_awd_amt": 449999.0,
 "awd_amount": 449999.0,
 "awd_min_amd_letter_date": "2012-04-22",
 "awd_max_amd_letter_date": "2014-04-30",
 "awd_abstract_narration": "The objective of this research is to advance significantly Artificial Intelligence by developing the first ever, brain-like Super-Turing system. \r\nThe approach is to build a computational system that is much more powerful (Super) than digital computation.  Alan Turing, a WWII code breaker, developed a conceptual (Turing) machine that has been basic to the digital computation revolution of the last 70 years.  This project will develop a Super-Turing mathematical model and implement it on an Optical Analog Neural Network Computer.  The project will focus on understanding system abilities and differences between Super-Turing and Turing systems.  Natural and artificial time-series will be compared to those of Super-Turing models and modeled Super-Turing processes will be compared with those of the brain. \r\nIntellectual Merit \r\nSuper-Turing Computation is a novel, transformative technology that will rewrite computational limits and lay a new foundation for Artificial Intelligence.\r\nBroader Impact\r\nIncreasing the capabilities of machine intelligence will profoundly influence society; Intelligent Super-Turing systems are expected to be capable of improving quality of life, e.g. providing intelligent robotic assistants, improving individualized education, creating new industries and medical technologies producing great economic benefit.  Further, a deepened understanding of Super-Turing computation as it relates to living systems is likely to bring new insights into brain function - fundamentally impacting neuroscience and brain-related medical technologies. Graduate and undergraduates will be trained in the interdisciplinary areas of advanced mathematics, physics, and computer science.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "A. Steven",
   "pi_last_name": "Younger",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "A. Steven Younger",
   "pi_email_addr": "SteveYounger@missouristate.edu",
   "nsf_id": "000398281",
   "pi_start_date": "2012-04-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Emmett",
   "pi_last_name": "Redd",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Emmett R Redd",
   "pi_email_addr": "EmmettRedd@MissouriState.edu",
   "nsf_id": "000394150",
   "pi_start_date": "2012-04-22",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Hava",
   "pi_last_name": "Siegelmann",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Hava Siegelmann",
   "pi_email_addr": "hava@cs.umass.edu",
   "nsf_id": "000144363",
   "pi_start_date": "2012-04-22",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Missouri State University",
  "inst_street_address": "901 S NATIONAL AVE",
  "inst_street_address_2": "",
  "inst_city_name": "SPRINGFIELD",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "4178365972",
  "inst_zip_code": "658970001",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MO07",
  "org_lgl_bus_name": "MISSOURI STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "CL7KJX2VFHS9"
 },
 "perf_inst": {
  "perf_inst_name": "Missouri State University",
  "perf_str_addr": "901 SOUTH NATIONAL",
  "perf_city_name": "SPRINGFIELD",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "658970027",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MO07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 177668.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 139134.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 133197.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In 1936, Alan Turing introduced is famous Computational model which we now call a Turing Machine TM[1]. The TM proved to be one of the most powerful mathematical models in the history of science. It is the mathematical basis of Digital Computers, and was a major enabling factor for the Digital Economy that we have today.</p>\n<p>It has been shown that many types of computing are equivalent to the Turing Model. The TM is so successful, it led many in the field to adopt the so-called <strong>Physical Church&ndash;Turing thesis</strong> as a fact: \"All physically computable functions are Turing-computable.&rdquo; &nbsp;This is despite strong indications that Turing himself did not believe this to be true [2].</p>\n<p>In 1995, Siegelmann and others have developed a Super-Turing Computational model based on Analog Recurrent Neural Networks (ARNNs) [3-5].&nbsp; They proved that ARNNs are able to solve classes of problems that are beyond the Turing Limit. Turing Machines can solve problems of complexity class P. While ARNNs with deterministic, real-valued signals can solve problems of class <strong>P/poly</strong>, which is a strict superset of class <strong>P</strong>. Moreover, they showed that ARNNs with signals with certain kinds of stochastic signals can solve problems of complexity class BPP/log*.&nbsp; Where:</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>&nbsp; P</strong>&nbsp;&nbsp; subset:of&nbsp;&nbsp; <strong>BPP/log*</strong>&nbsp; subset:of <strong>P/poly</strong></p>\n<p>That is, the<strong> BPP/log* </strong>is also Super-Turing, but perhaps lower computational complexity than<strong> P/poly</strong>. One condition is that the stochastic signal must have the possibility of having an irrational value.</p>\n<p>The significance of this is: while a machine that computes with deterministic real-valued signals cannot be constructed, a ARNN with stochastic signals can be. There is no reason that the stochasticity cannot be irrational.&nbsp; While any given measurement is rational (there are a finite number of significant bits or digits) a long sequence of measurements allows indirect access to the probability.&nbsp; More accurately, it facilitates its approximation to high probability.&nbsp;</p>\n<p>We designed, constructed, and tested a ARNN based on an electro-optic design, with theoretical guidance from the work by Siegelmann and others [6]. Figure 1 shows a schematic of our Optical Analog Recurrent Neural Network (OpticARNN).</p>\n<p><strong>Compatibility with Chaos.</strong></p>\n<p>Because chaotic systems are based on continous domain, Turning Machines cannot full characterize their behavior. <strong><br /></strong></p>\n<p>In their foundational book, Kaplan and Glass presented methods of testing whether or not a given time-series is compatible with chaos.&nbsp; It cannot be proven that a candidate time series is chaotic. The series can fail a test and prove that it is incompatible with chaos.&nbsp; Alternatively, the series can pass a several tests and confidence accumulated (but not proven) that is indeed chaotic.</p>\n<p>We compared five neural neural networks (four digital and one analog) on the Logistics Chaos Time-Series to verify that the analog neural network results are compatible with chaos, while some digital results are not compatible with chaos.</p>\n<p>Table I. Correlation Dimension: Does Time-Series differ than Linearized Surrogate Data?</p>\n<table border=\"0\" cellpadding=\"0\">\n<tbody>\n<tr>\n<td>\n<p><strong>Time-Series Data</strong></p>\n</td>\n<td>\n<p>Precision</p>\n</td>\n<td>\n<p>&nbsp;Mean</p>\n</td>\n<td>\n<p>Std Dev</p>\n</td>\n<td>\n<p>Actual</p>\n</td>\n<td>\n<p>Z score</p>\n</td>\n<td>\n<p>Consistent w/ Chaos?</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Logistics Map Training</p>\n</td>\n<td>\n<p>double</p>\n</td>\n<td>\n<p>8.0889</p>\n</td>\n<td>\n<p>0.0919</p>\n</td>\n<td>\n<p>1.9194</p>\n</td>\n<td>\n<p>67.096</p>\n</td>\n<td>\n<p>Yes</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>DRNN double</p>\n</td>\n<td>\n<p>double</p>\n</td>\n<td>\n<p>5.1057</p>\n</td>\n<td>\n<p>0.0530</p>\n</td>\n<td>\n<p>0.8733</p>\n</td>\n<td>\n<p>79.836</p>\n</td>\n<td>\n<p>Yes</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>DRNN9</p>\n</td>\n<td>\n<p>9 bit</p>\n</td>\n<td>\n<p>3.4129</p>\n</td>\n<td>\n<p>0.4162</p>\n</td>\n<td>\n<p>1.5719</p>\n</td>\n<td>\n<p>4.4238</p>\n</td>\n<td>\n<p>Yes</p>\n</td>\n</tr>\n<tr>\n<td>\n<p>Random</p>\n</td>\n<td>\n<p>double</p>\n</td>\n<td>\n<p>7.8194</p>\n</td>\n<td>\n<p>0.0752</p>\n</td>\n<td>\n<p>7.7673</p>\n</td>\n<td>\n<p>0.6935</p>\n</td>\n<td>\n<p><strong>No</strong></p>\n</td>\n</tr>\n<tr>\n<td>\n<p>OpticARNN</p>\n</td>\n<td>\n<p>&nbsp;7 bit</p>\n</td>\n<td>\n<p>0.8672</p>\n</td>\n<td>\n<p>0.0577</p>\n</td>\n<td>\n<p>1.0269</p>\n</td>\n<td>\n<p>2.7688</p>\n</td>\n<td>\n<p>Yes</p>\n</td>\n</tr>\n</tbody>\n</table>\n<p>&nbsp;</p>\n<p>Figures 3-6 show the Autocorrelation spectra for all the time-series under consideration. Note that the DRNN 9-bit has a strikingliy different spectrum than the others, showing it fails this test for chaos.</p>\n<p>Figure 6 is the autocorrelation for the (1:3:1) logsig-logsig network which was additionally trained the OpticARNN hardware-in-the-loop.&nbsp; l. We conclude that this network is consistent with chaos. In particular, it is much more consistent than its digital &ldquo;equivalent.&rdquo;</p>\n<p>&nbsp;Conclusions:</p>\n<p>We physically realized a machine(OpticARNN) based on the Siegelmann BPP/log* model -- an analog neural network with rational synaptic weights and signals with real-valued stochastic noise, and showed that this network is&nbsp; capable of Super-Turing computations</p>\n<p>&nbsp;1. Turing, Alan: On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, Series 2, 42, 230&ndash;265 (1936)</p>\n<p>2. Turing, A.M.: Intelligent Machinery, report for National Physical Laboratory, in Machine Intelligence 7, B. Meltzer and D. Michie (eds.) (1969)</p>\n<p>3. Siegelmann, H.T.: Computation Beyond the Turing Limit. Science 238(28), 632-637 (April 1995)</p>\n<p>4. &nbsp;Siegelmann H. T.: Neural Networks and Analog Computation: Beyond the Turing Limit, Birkhauser, Boston (December 1998)</p>\n<p>5. Siegelmann, H. T. : Stochastic Analog Networks and Computational Complexity, Journal of Complexity, 15(4) 451-475 (1999)</p>\n<div id=\"gs_cit0\" class=\"gs_citr\">6. Younger, A. Steven, Emmett Redd, and Hava Siegelmann. \"Development of Physical Super-Turing Analog Hardware.\" <em>International Conference on Unconventional Computation and Natural Computation</em>. Springer International Publishing, 2014.</div>\n<div class=\"gs_citr\"></div>\n<p>7. Kaplan, Daniel, and Leon Glass. <em>Understanding nonlinear dynamics</em>. Springer Science &amp; Business Media, 2012.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/07/2016<br>\n\t\t\t\t\tModified by: A. Steven&nbsp;Younger</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262122772_Outcomes.Slide1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262122772_Outcomes.Slide1--rgov-800width.jpg\" title=\"Figure 1. Optical Analog Recurrent Neural Network\"><img src=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262122772_Outcomes.Slide1--rgov-66x44.jpg\" alt=\"Figure 1. Optical Analog Recurrent Neural Network\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Schematic of Optical Analog Neural Network developed for this project.</div>\n<div class=\"imageCredit\">A. Steven Younger and Emmett Redd</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">A. Steven&nbsp;Younger</div>\n<div class=\"imageTitle\">Figure 1. Optical Analog Recurrent Neural Network</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262783114_Outcomes.Slide2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262783114_Outcomes.Slide2--rgov-800width.jpg\" title=\"Figure 2. Periodicity artifact of Turing Machines.\"><img src=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262783114_Outcomes.Slide2--rgov-66x44.jpg\" alt=\"Figure 2. Periodicity artifact of Turing Machines.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">In simulating Chaotic Systems, Digital computers (Turing Machines) eventually produce a periodic (non-chaotic) time-series due to finite precision and deterministic computation. More precision reduces, but not eliminates, this artifact.</div>\n<div class=\"imageCredit\">A. Steven Younger and Emmett Redd</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">A. Steven&nbsp;Younger</div>\n<div class=\"imageTitle\">Figure 2. Periodicity artifact of Turing Machines.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262987080_Outcomes.Slide3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262987080_Outcomes.Slide3--rgov-800width.jpg\" title=\"Autocorrelation of Double Precision Logistics Map Training Data\"><img src=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473262987080_Outcomes.Slide3--rgov-66x44.jpg\" alt=\"Autocorrelation of Double Precision Logistics Map Training Data\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The artifact is beyond the resolution of this data, but is still present.</div>\n<div class=\"imageCredit\">A. Steven Younger and Emmett Redd</div>\n<div class=\"imageSubmitted\">A. Steven&nbsp;Younger</div>\n<div class=\"imageTitle\">Autocorrelation of Double Precision Logistics Map Training Data</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264099195_Outcomes.Slide4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264099195_Outcomes.Slide4--rgov-800width.jpg\" title=\"Figure 4. Autocorrelation of 1:3:1 Digital RNN with Double Precision Arithmetic.\"><img src=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264099195_Outcomes.Slide4--rgov-66x44.jpg\" alt=\"Figure 4. Autocorrelation of 1:3:1 Digital RNN with Double Precision Arithmetic.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Digital NN trained and executed with double precision arithmetic.</div>\n<div class=\"imageCredit\">A. Steven Younger and Emmett Redd</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">A. Steven&nbsp;Younger</div>\n<div class=\"imageTitle\">Figure 4. Autocorrelation of 1:3:1 Digital RNN with Double Precision Arithmetic.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264220585_Outcomes.Slide5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264220585_Outcomes.Slide5--rgov-800width.jpg\" title=\"Figure 5. Autocorrelation for DRNN above with output rounded to 9 significant bits.\"><img src=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264220585_Outcomes.Slide5--rgov-66x44.jpg\" alt=\"Figure 5. Autocorrelation for DRNN above with output rounded to 9 significant bits.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Note strikingly different character when compared with other Time-Series, due to the digital periodicity artifact.</div>\n<div class=\"imageCredit\">A. Steven Younger and Emmett Redd</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">A. Steven&nbsp;Younger</div>\n<div class=\"imageTitle\">Figure 5. Autocorrelation for DRNN above with output rounded to 9 significant bits.</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264423001_Outcomes.Slide6--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264423001_Outcomes.Slide6--rgov-800width.jpg\" title=\"Figure 6: Autocorrelation for Super-Turing OpticARNN Time-Series.\"><img src=\"/por/images/Reports/POR/2016/1201790/1201790_10167427_1473264423001_Outcomes.Slide6--rgov-66x44.jpg\" alt=\"Figure 6: Autocorrelation for Super-Turing OpticARNN Time-Series.\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The ARNN time-series. Note that the Turing Machine Artifact is totally absent.</div>\n<div class=\"imageCredit\">A. Steven Younger and Emmett Redd</div>\n<div class=\"imagePermisssions\">Public Domain</div>\n<div class=\"imageSubmitted\">A. Steven&nbsp;Younger</div>\n<div class=\"imageTitle\">Figure 6: Autocorrelation for Super-Turing OpticARNN Time-Series.</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nIn 1936, Alan Turing introduced is famous Computational model which we now call a Turing Machine TM[1]. The TM proved to be one of the most powerful mathematical models in the history of science. It is the mathematical basis of Digital Computers, and was a major enabling factor for the Digital Economy that we have today.\n\nIt has been shown that many types of computing are equivalent to the Turing Model. The TM is so successful, it led many in the field to adopt the so-called Physical Church&ndash;Turing thesis as a fact: \"All physically computable functions are Turing-computable.\"  This is despite strong indications that Turing himself did not believe this to be true [2].\n\nIn 1995, Siegelmann and others have developed a Super-Turing Computational model based on Analog Recurrent Neural Networks (ARNNs) [3-5].  They proved that ARNNs are able to solve classes of problems that are beyond the Turing Limit. Turing Machines can solve problems of complexity class P. While ARNNs with deterministic, real-valued signals can solve problems of class P/poly, which is a strict superset of class P. Moreover, they showed that ARNNs with signals with certain kinds of stochastic signals can solve problems of complexity class BPP/log*.  Where:\n\n                               P   subset:of   BPP/log*  subset:of P/poly\n\nThat is, the BPP/log* is also Super-Turing, but perhaps lower computational complexity than P/poly. One condition is that the stochastic signal must have the possibility of having an irrational value.\n\nThe significance of this is: while a machine that computes with deterministic real-valued signals cannot be constructed, a ARNN with stochastic signals can be. There is no reason that the stochasticity cannot be irrational.  While any given measurement is rational (there are a finite number of significant bits or digits) a long sequence of measurements allows indirect access to the probability.  More accurately, it facilitates its approximation to high probability. \n\nWe designed, constructed, and tested a ARNN based on an electro-optic design, with theoretical guidance from the work by Siegelmann and others [6]. Figure 1 shows a schematic of our Optical Analog Recurrent Neural Network (OpticARNN).\n\nCompatibility with Chaos.\n\nBecause chaotic systems are based on continous domain, Turning Machines cannot full characterize their behavior. \n\n\nIn their foundational book, Kaplan and Glass presented methods of testing whether or not a given time-series is compatible with chaos.  It cannot be proven that a candidate time series is chaotic. The series can fail a test and prove that it is incompatible with chaos.  Alternatively, the series can pass a several tests and confidence accumulated (but not proven) that is indeed chaotic.\n\nWe compared five neural neural networks (four digital and one analog) on the Logistics Chaos Time-Series to verify that the analog neural network results are compatible with chaos, while some digital results are not compatible with chaos.\n\nTable I. Correlation Dimension: Does Time-Series differ than Linearized Surrogate Data?\n\n\n\n\n\nTime-Series Data\n\n\n\nPrecision\n\n\n\n Mean\n\n\n\nStd Dev\n\n\n\nActual\n\n\n\nZ score\n\n\n\nConsistent w/ Chaos?\n\n\n\n\n\nLogistics Map Training\n\n\n\ndouble\n\n\n\n8.0889\n\n\n\n0.0919\n\n\n\n1.9194\n\n\n\n67.096\n\n\n\nYes\n\n\n\n\n\nDRNN double\n\n\n\ndouble\n\n\n\n5.1057\n\n\n\n0.0530\n\n\n\n0.8733\n\n\n\n79.836\n\n\n\nYes\n\n\n\n\n\nDRNN9\n\n\n\n9 bit\n\n\n\n3.4129\n\n\n\n0.4162\n\n\n\n1.5719\n\n\n\n4.4238\n\n\n\nYes\n\n\n\n\n\nRandom\n\n\n\ndouble\n\n\n\n7.8194\n\n\n\n0.0752\n\n\n\n7.7673\n\n\n\n0.6935\n\n\n\nNo\n\n\n\n\n\nOpticARNN\n\n\n\n 7 bit\n\n\n\n0.8672\n\n\n\n0.0577\n\n\n\n1.0269\n\n\n\n2.7688\n\n\n\nYes\n\n\n\n\n\n \n\nFigures 3-6 show the Autocorrelation spectra for all the time-series under consideration. Note that the DRNN 9-bit has a strikingliy different spectrum than the others, showing it fails this test for chaos.\n\nFigure 6 is the autocorrelation for the (1:3:1) logsig-logsig network which was additionally trained the OpticARNN hardware-in-the-loop.  l. We conclude that this network is consistent with chaos. In particular, it is much more consistent than its digital \"equivalent.\"\n\n Conclusions:\n\nWe physically realized a machine(OpticARNN) based on the Siegelmann BPP/log* model -- an analog neural network with rational synaptic weights and signals with real-valued stochastic noise, and showed that this network is  capable of Super-Turing computations\n\n 1. Turing, Alan: On computable numbers, with an application to the Entscheidungsproblem. Proceedings of the London Mathematical Society, Series 2, 42, 230&ndash;265 (1936)\n\n2. Turing, A.M.: Intelligent Machinery, report for National Physical Laboratory, in Machine Intelligence 7, B. Meltzer and D. Michie (eds.) (1969)\n\n3. Siegelmann, H.T.: Computation Beyond the Turing Limit. Science 238(28), 632-637 (April 1995)\n\n4.  Siegelmann H. T.: Neural Networks and Analog Computation: Beyond the Turing Limit, Birkhauser, Boston (December 1998)\n\n5. Siegelmann, H. T. : Stochastic Analog Networks and Computational Complexity, Journal of Complexity, 15(4) 451-475 (1999)\n6. Younger, A. Steven, Emmett Redd, and Hava Siegelmann. \"Development of Physical Super-Turing Analog Hardware.\" International Conference on Unconventional Computation and Natural Computation. Springer International Publishing, 2014.\n\n\n7. Kaplan, Daniel, and Leon Glass. Understanding nonlinear dynamics. Springer Science &amp; Business Media, 2012.\n\n\t\t\t\t\tLast Modified: 09/07/2016\n\n\t\t\t\t\tSubmitted by: A. Steven Younger"
 }
}