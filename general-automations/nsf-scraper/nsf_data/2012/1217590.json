{
 "awd_id": "1217590",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF:Small: Solving the Problem of Scalable Multi-Precision Matrix Arithmetic on GPUs",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2012-06-01",
 "awd_exp_date": "2016-05-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2012-05-16",
 "awd_max_amd_letter_date": "2012-06-22",
 "awd_abstract_narration": "Computers directly support arithmetic that is typically limited to 64 bits (about 19 decimal digits) of precision. Applications that need more precision must implement arithmetic through computationally expensive software. Beyond about 256 bits of precision, such calculations become quite costly. The RSA encryption algorithm, for example, can require arithmetic with up to 4096 bits of precision. Applications in areas such as experimental mathematics and number theory can require millions of bits of precision. One multiplication with 10 million bits of precision can take a tenth of a second to compute on a modern processor, which means that matrix arithmetic using such large values can take days to weeks to execute. In previous work the investigators have shown that it is possible to obtain a factor of 20 improvement in performance by utilizing the parallel processing capabilities of a commodity graphics processing unit (GPU) in place of the traditional CPU. However, programming a GPU to achieve this level of performance is quite difficult, and the resulting code requires considerable hand-tuning to move it to new generations of GPU and gain the advantage of their performance, which is scaling up at a rate that exceeds CPU performance scaling.\r\n\r\nThis project is working to develop a framework that automatically generates and tunes multi-precision arithmetic libraries to execute on successive generations of GPUs. The libraries include both scalar and basic matrix arithmetic routines. They support scaling in precision as well as matrix size. The problem is challenging because different parallel algorithms must be automatically selected for different levels of precision, which must be balanced with the exploitation of the alternate dimension of parallelism inherent in matrix arithmetic. In addition, the work seeks to employ distributed parallelism across a cluster of computers enhanced with GPUs, so that the libraries can be used on a new generation of GPU-based supercomputers that is beginning to be deployed at national laboratories. \r\n\r\nThe work is significant because it enables easier exploitation of low-cost commodity graphics processors to achieve more than an order of magnitude increase in performance for multi-precision scalar and matrix arithmetic. One important application is enhancing performance of RSA encryption to support longer, more secure keys, at greater data rates, so that it becomes feasible to encrypt greater volumes of internet traffic. Another important use is experimental mathematics, where computationally expensive functions (e.g., integrals, infinite series) are computed at high precision and compared to other functions and high precision constants to help identify more efficient closed-form solutions. Results from experimental mathematics have found applications in particle physics, chaos theory, and calculation of fundamental constants. The resulting software framework offers a significant performance enhancement for multi-precision arithmetic to systems that range from individual researcher workstations to large supercomputers.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Weems",
   "pi_mid_init": "C",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Charles C Weems",
   "pi_email_addr": "weems@cs.umass.edu",
   "nsf_id": "000434762",
   "pi_start_date": "2012-05-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Amherst",
  "inst_street_address": "101 COMMONWEALTH AVE",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "4135450698",
  "inst_zip_code": "010039252",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "MA02",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS",
  "org_prnt_uei_num": "VGJHK59NMPK9",
  "org_uei_num": "VGJHK59NMPK9"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Amherst",
  "perf_str_addr": "140 Governors Drive",
  "perf_city_name": "Amherst",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "010039264",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "MA02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7329",
   "pgm_ref_txt": "COMPILERS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Multiprecision (MP) integer arithmetic is widely used in internet cryptography, science, engineering, and experimental mathematics. Modern graphics processors (GPUs) offer the potential to accelerate MP calculations by two orders of magnitude beyond traditional sequential processors. However, their designs are evolving rapidly, and extracting maximum performance from them is very challenging.&nbsp;</p>\n<p>In this project we developed tools and algorithms to obtain the best MP arithmetic performance possible for six generations of NVIDIA GPU. In most cases, the code we generate achieves 90 to 95 percent utilization of the available computational capabilities of the GPU, with no means remaining to further increase utilization. All of our implementations are the fastest known in the world.</p>\n<p>A practical example of how our work can be used is that it makes it possible for a single GPU to offload all of the cryptographic processing for a rack of servers. Doing so would save power and make it cost effective to use stronger encryption in large server installations, thereby increasing internet security.&nbsp;</p>\n<p>We first formed a relationship with NVIDIA by showing them how to significantly increase the performance of the integer matrix multiply operation in their Trove library. That result opened an opportunity to work more closely with them on the MP library they were developing (called XMP). Our research made it possible to increase the performance of XMP by a factor of ten over the approach they had planned to use. That made it of particular interest to a large government agency for use in a national security application.&nbsp;</p>\n<p>We worked with NVIDIA to implement compiler improvements, new optimizations, and new algorithms in support of XMP. In part because we had already published much of our work, NVIDIA released XMP publicly as open source. Thus the community now has access to a highly optimized, vendor-supported MP arithmetic package, and the ability to build on our work.&nbsp;</p>\n<p>Because general purpose GPUs are relatively new, their generations vary considerably in their designs. Optimizing performance depends on maximizing the number of parallel threads of computation. Computation happens in register memory, while shared memory provides overflow space. When the number of threads is large, there are very few registers and very little shared memory available for each one. Thus, an algorithm must be tuned to work within this limited memory. With each generation, the ratio of registers to shared memory shifts, necessitating different algorithms. Different generations also change the arithmetic operations they offer. For example, integer arithmetic may be provided for 24 bit values on one, 32 bit values on another, or 16 bit values on a third. In MP arithmetic, handling carries between these values is a critical operation, and different generations provide different forms of support for carry handling.&nbsp;</p>\n<p>We began with a simple technique using code templates to generate low level instructions for the different operators. But after seeing the changes in two successive generations of GPU, we realized a much different approach would be needed. We developed a set of models of the register-memory ratio, arithmetic algorithms, and thread organization that enabled us to search a multidimensional space of implementation options to find the best performance. Many of the optimal configurations were counterintuitive and took considerable analysis to understand why they were the fastest. The subtle interactions that were exploited would not have been obvious to someone who was trying to hand-code a fast implementation. The resulting insights were what we brought to bear in helping NVIDIA optimize XMP.&nbsp;</p>\n<p>Although we focused mainly on sizes that are used in cryptography (256 to 2048 bits), we also developed GPU algorthms for much larger numbers (up to millions of bits). The larger sizes are important for experimental mathematics, as we demonstrated by finding the least eigenvalues for a particular class of ill-conditioned Hankel matrices with connections to computational physics. We were able to solve significantly larger instances of these matrices than had ever been attempted, providing mathematicians with new insights into their seemingly unpredictable behaviors.&nbsp;</p>\n<p>One other outcome of the project was discovering a new algorithm for parallel short division (dividing an MP value by a single precision value). We were able to establish and prove a new strong lower bound on the complexity of parallel short division and show that our algorithm achieves it efficiently. We also showed that the algorithm is practical to implement on all common classes of parallel systems.&nbsp;</p>\n<p>The project produced eight journal or conference papers, and contributed technology to two of NVIDIA's libraries. It supported 3 MS students and one PhD student. Two of the MS students were women, one of whom was also from an underrepresented group. Insights gained from this work have also been taught in the PI's computer architecture classes.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/24/2016<br>\n\t\t\t\t\tModified by: Charles&nbsp;C&nbsp;Weems</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMultiprecision (MP) integer arithmetic is widely used in internet cryptography, science, engineering, and experimental mathematics. Modern graphics processors (GPUs) offer the potential to accelerate MP calculations by two orders of magnitude beyond traditional sequential processors. However, their designs are evolving rapidly, and extracting maximum performance from them is very challenging. \n\nIn this project we developed tools and algorithms to obtain the best MP arithmetic performance possible for six generations of NVIDIA GPU. In most cases, the code we generate achieves 90 to 95 percent utilization of the available computational capabilities of the GPU, with no means remaining to further increase utilization. All of our implementations are the fastest known in the world.\n\nA practical example of how our work can be used is that it makes it possible for a single GPU to offload all of the cryptographic processing for a rack of servers. Doing so would save power and make it cost effective to use stronger encryption in large server installations, thereby increasing internet security. \n\nWe first formed a relationship with NVIDIA by showing them how to significantly increase the performance of the integer matrix multiply operation in their Trove library. That result opened an opportunity to work more closely with them on the MP library they were developing (called XMP). Our research made it possible to increase the performance of XMP by a factor of ten over the approach they had planned to use. That made it of particular interest to a large government agency for use in a national security application. \n\nWe worked with NVIDIA to implement compiler improvements, new optimizations, and new algorithms in support of XMP. In part because we had already published much of our work, NVIDIA released XMP publicly as open source. Thus the community now has access to a highly optimized, vendor-supported MP arithmetic package, and the ability to build on our work. \n\nBecause general purpose GPUs are relatively new, their generations vary considerably in their designs. Optimizing performance depends on maximizing the number of parallel threads of computation. Computation happens in register memory, while shared memory provides overflow space. When the number of threads is large, there are very few registers and very little shared memory available for each one. Thus, an algorithm must be tuned to work within this limited memory. With each generation, the ratio of registers to shared memory shifts, necessitating different algorithms. Different generations also change the arithmetic operations they offer. For example, integer arithmetic may be provided for 24 bit values on one, 32 bit values on another, or 16 bit values on a third. In MP arithmetic, handling carries between these values is a critical operation, and different generations provide different forms of support for carry handling. \n\nWe began with a simple technique using code templates to generate low level instructions for the different operators. But after seeing the changes in two successive generations of GPU, we realized a much different approach would be needed. We developed a set of models of the register-memory ratio, arithmetic algorithms, and thread organization that enabled us to search a multidimensional space of implementation options to find the best performance. Many of the optimal configurations were counterintuitive and took considerable analysis to understand why they were the fastest. The subtle interactions that were exploited would not have been obvious to someone who was trying to hand-code a fast implementation. The resulting insights were what we brought to bear in helping NVIDIA optimize XMP. \n\nAlthough we focused mainly on sizes that are used in cryptography (256 to 2048 bits), we also developed GPU algorthms for much larger numbers (up to millions of bits). The larger sizes are important for experimental mathematics, as we demonstrated by finding the least eigenvalues for a particular class of ill-conditioned Hankel matrices with connections to computational physics. We were able to solve significantly larger instances of these matrices than had ever been attempted, providing mathematicians with new insights into their seemingly unpredictable behaviors. \n\nOne other outcome of the project was discovering a new algorithm for parallel short division (dividing an MP value by a single precision value). We were able to establish and prove a new strong lower bound on the complexity of parallel short division and show that our algorithm achieves it efficiently. We also showed that the algorithm is practical to implement on all common classes of parallel systems. \n\nThe project produced eight journal or conference papers, and contributed technology to two of NVIDIA's libraries. It supported 3 MS students and one PhD student. Two of the MS students were women, one of whom was also from an underrepresented group. Insights gained from this work have also been taught in the PI's computer architecture classes. \n\n\t\t\t\t\tLast Modified: 10/24/2016\n\n\t\t\t\t\tSubmitted by: Charles C Weems"
 }
}