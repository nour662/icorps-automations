{
 "awd_id": "1162076",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "SHF: Medium: Collaborative Research: Marrying Program Analysis and Numerical Search",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927991",
 "po_email": "namla@nsf.gov",
 "po_sign_block_name": "Nina Amla",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2018-08-31",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 600000.0,
 "awd_min_amd_letter_date": "2012-04-04",
 "awd_max_amd_letter_date": "2015-08-23",
 "awd_abstract_narration": "This research project explores ways to solve optimization problems where the targets of optimization are programs containing general-purpose control and data constructs. Such optimization questions arise often in the everyday practice of software engineering. While it may seem that standard optimization packages could solve these problems, it is often not so. White-box optimization approaches like linear programming are ruled out here because they only permit very restricted classes of objective functions. Black-box optimization techniques like gradient descent and Nelder-Mead search are applicable in principle, but they work well only in relatively smooth search spaces, and due to arbitrarily nested branches and loops, even simple programs can have highly irregular, ill-conditioned behavior.\r\n\r\nThe central insight guiding this project is that program analysis techniques from the field of formal reasoning about programs can work together with blackbox optimization toolkits, and make it possible to solve many more problems of the above sort than are currently possible. Ultimately, the project will produce a unified system for optimizing programs that can leverage flexible combinations of optimization techniques and program analysis strategies. As numerous real-world problems faced in the development of everyday software are optimization problems, this system will offer a new range of capabilities to the end programmer. In addition, the research will foster synergy between two different research areas customarily housed in different academic departments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Swarat",
   "pi_last_name": "Chaudhuri",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Swarat Chaudhuri",
   "pi_email_addr": "swarat@cs.utexas.edu",
   "nsf_id": "000504208",
   "pi_start_date": "2012-04-04",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Illya",
   "pi_last_name": "Hicks",
   "pi_mid_init": "V",
   "pi_sufx_name": "",
   "pi_full_name": "Illya V Hicks",
   "pi_email_addr": "ivhicks@rice.edu",
   "nsf_id": "000374516",
   "pi_start_date": "2012-04-04",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "William Marsh Rice University",
  "inst_street_address": "6100 MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "Houston",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "7133484820",
  "inst_zip_code": "770051827",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "TX09",
  "org_lgl_bus_name": "WILLIAM MARSH RICE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "K51LECU1G8N3"
 },
 "perf_inst": {
  "perf_inst_name": "William Marsh Rice University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "770051827",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "TX09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7944",
   "pgm_ref_txt": "SOFTWARE ENG & FORMAL METHODS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 151489.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 294604.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 153907.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project sought to develop algorithms and tools for solving difficult optimization/learning problems involving programs. For instance, such problems include synthesizing programs (possibly with complex control flow) that optimize a quantitative objective, and discovering optimal program parameters that guarantee the satisfaction of a formal correctness requirement.&nbsp; The key technical insight behind that because programs often represent discontinuous and nondifferentiable functions, using off-the-shelf optimization techniques (for example gradient descent) in these problems is unlikely to be fruitful. Instead, the project studied methods that deeply couple numerical techniques from optimization and machine learning with symbolic techniques from programming languages and formal methods.<br />On the scientific front, the intellectual merit outcomes of our project include the following.</p>\n<p>* The project proposed the use of gradient-based optimization to learn optimal program parameters. This idea is the core insight of&nbsp;differentiable programming, an emerging subarea of deep learning. We worked on developing this idea before the deep learning revolution started. In fact, we went beyond differentiable programming by not requiring programs to be differentiable by construction, but developing algorithmic methods for \"smoothing\" nondifferentiable programs.&nbsp;</p>\n<p>* This project was among the first to study the problem of verified learning and optimization. Specifically, our work on Smoothed ProofSearch studied the problem of discovering optimal program parameters that provably guarantee that the program satisfies a correctness property.</p>\n<p>* The project found applications of its insights in a range of areas including control, robotics, artificial intelligence, and software engineering.&nbsp;</p>\n<p><br />The broader impacts of our project include:<br /><br />* Training and professional development of graduate&nbsp;researchers.&nbsp; The award supported several PhD students, undergraduate researchers, and postdoctoral researchers.&nbsp;</p>\n<p>* Computer science education. The PIs have integrated ideas from this project with the classes they teach.</p>\n<p>* Software. The award supported the development of several software systems, including the Euler system for numerical optimization of programs and the Lambda2 and Morpheus systems for program synthesis.</p>\n<p>* Workshops. The PIs organized and participated in a number of workshops that brought together researchers from formal methods and machine learning/optimization.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/24/2019<br>\n\t\t\t\t\tModified by: Swarat&nbsp;Chaudhuri</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project sought to develop algorithms and tools for solving difficult optimization/learning problems involving programs. For instance, such problems include synthesizing programs (possibly with complex control flow) that optimize a quantitative objective, and discovering optimal program parameters that guarantee the satisfaction of a formal correctness requirement.  The key technical insight behind that because programs often represent discontinuous and nondifferentiable functions, using off-the-shelf optimization techniques (for example gradient descent) in these problems is unlikely to be fruitful. Instead, the project studied methods that deeply couple numerical techniques from optimization and machine learning with symbolic techniques from programming languages and formal methods.\nOn the scientific front, the intellectual merit outcomes of our project include the following.\n\n* The project proposed the use of gradient-based optimization to learn optimal program parameters. This idea is the core insight of differentiable programming, an emerging subarea of deep learning. We worked on developing this idea before the deep learning revolution started. In fact, we went beyond differentiable programming by not requiring programs to be differentiable by construction, but developing algorithmic methods for \"smoothing\" nondifferentiable programs. \n\n* This project was among the first to study the problem of verified learning and optimization. Specifically, our work on Smoothed ProofSearch studied the problem of discovering optimal program parameters that provably guarantee that the program satisfies a correctness property.\n\n* The project found applications of its insights in a range of areas including control, robotics, artificial intelligence, and software engineering. \n\n\nThe broader impacts of our project include:\n\n* Training and professional development of graduate researchers.  The award supported several PhD students, undergraduate researchers, and postdoctoral researchers. \n\n* Computer science education. The PIs have integrated ideas from this project with the classes they teach.\n\n* Software. The award supported the development of several software systems, including the Euler system for numerical optimization of programs and the Lambda2 and Morpheus systems for program synthesis.\n\n* Workshops. The PIs organized and participated in a number of workshops that brought together researchers from formal methods and machine learning/optimization.\n\n\t\t\t\t\tLast Modified: 03/24/2019\n\n\t\t\t\t\tSubmitted by: Swarat Chaudhuri"
 }
}