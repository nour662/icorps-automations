{
 "awd_id": "1248056",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "INSPIRE: Signals to Symbols: From Bio-inspired Hardware to Cognitive Systems",
 "cfda_num": "47.075",
 "org_code": "04010000",
 "po_phone": "7032927878",
 "po_email": "slim@nsf.gov",
 "po_sign_block_name": "Soo-Siang Lim",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 600000.0,
 "awd_amount": 619200.0,
 "awd_min_amd_letter_date": "2012-09-24",
 "awd_max_amd_letter_date": "2014-07-12",
 "awd_abstract_narration": "This INSPIRE award is partially funded by the Science of Learning Centers Program in the Division of Behavioral, Cognitive and Social Sciences in the Directorate for Social, Behavioral and Economic Sciences; the Perception, Action, and Cognition Program in the Division of Behavioral, Cognitive and Social Sciences in the Directorate for Social, Behavioral and Economic Sciences;  the Energy, Power, and Adaptive Systems Program in the Division of Electrical Communication and Cyber Systems in the Directorate of Engineering; and the Applied Mathematics and Mathematical Biology Program in the Division of Mathematical Sciences in the Directorate for Mathematical and Physical Sciences.  This research project draws on knowledge from many disciplines (neuroscience, cognitive science, computational science, mathematics and engineering) to create cognitive systems capable of interpreting observed, complex human movements and actions. New design methodologies will be developed for the integration of sensory modalities (vision, audition, touch) and their support of higher cognitive function (language, reasoning).  In contrast to existing approaches which tend to be assemblies of modular components each solving its task in isolation, this team takes a novel approach called Active Cognition which has the following features: 1) Instead of modeling the different perceptual processes (vision, audition, and haptics), cognition, and motor control in isolation, the modules are integrated and capabilities co-developed in the tradition of dynamical systems theory to obtain a reasoning system where \"the whole is greater than the sum of its parts\"; 2) instead of segregating the low level processing of signals from the processing of higher level symbolic information, they will interact in a continuous dialogue, such that high level knowledge will leverage perception; and 3) instead of separating physical embodiment from algorithmic considerations, biologically inspired real-time hardware will be developed that implements complex functions by integrating signals and symbols.  The project is organized in two working groups. The first group will develop a cognitive robot that can recognize complex human activities using visual and auditory signals captured by biological-inspired hardware. The second group will study attention in humans by measuring human response to audition and vision through EEG and MEG, and subsequently implementing the findings in robots. A yearly three-week, hands-on workshop will educate students, serve as testing ground for the team's ideas, and stimulate new collaborations.   This workshop will also engage the involvement of the interdisciplinary research community that has formed around the goal of building biologically inspired cognitive systems. \r\n\r\nSuccess in integrating different components of a cognitive system (hardware, sensors, and software) has the potential to catalyze a new industry of biologically-inspired cognitive systems, including household and service robots, and systems for intelligent transportation and smart manufacturing.  In addition, this interdisciplinary project will play a significant role in building capacity for a new emphasis area in engineering and training of cognitive systems engineers who need combined expertise in computer science, electrical engineering and cognitive neuroscience.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SMA",
 "org_div_long_name": "SBE Office of Multidisciplinary Activities",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Cornelia",
   "pi_last_name": "Fermuller",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Cornelia M Fermuller",
   "pi_email_addr": "fer@cfar.umd.edu",
   "nsf_id": "000235233",
   "pi_start_date": "2012-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Shihab",
   "pi_last_name": "Shamma",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Shihab A Shamma",
   "pi_email_addr": "sas@isr.umd.edu",
   "nsf_id": "000461861",
   "pi_start_date": "2012-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Andreou",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas G Andreou",
   "pi_email_addr": "andreou@jhu.edu",
   "nsf_id": "000421506",
   "pi_start_date": "2012-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Timothy",
   "pi_last_name": "Horiuchi",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Timothy K Horiuchi",
   "pi_email_addr": "timmer@isr.umd.edu",
   "nsf_id": "000441772",
   "pi_start_date": "2012-09-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ralph",
   "pi_last_name": "Etienne-Cummings",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ralph Etienne-Cummings",
   "pi_email_addr": "retienne@jhu.edu",
   "nsf_id": "000360105",
   "pi_start_date": "2012-09-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland",
  "perf_str_addr": "3112 LEE BLDG",
  "perf_city_name": "College Park",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425141",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "733400",
   "pgm_ele_name": "MATHEMATICAL BIOLOGY"
  },
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  },
  {
   "pgm_ele_code": "770400",
   "pgm_ele_name": "Science of Learning Activities"
  },
  {
   "pgm_ele_code": "807800",
   "pgm_ele_name": "INSPIRE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8653",
   "pgm_ref_txt": "INSPIRE Track-1 Creative"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 600000.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 19200.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This INSPIRE award brought together scientists from many disciplines (neuroscience, cognitive science, computational science, mathematics and engineering) in an effort to design and create cognitive systems through the integration of low-level perception and high-level cognitive capabilities, &nbsp;the integration of multi-sensory processes (vision, audition, and haptics), and the consideration of physical embodiment in algorithmic development and implementation of algorithms in biologically inspired, neuromorphic real-time hardware. The project supported throughout its three years two working groups and an annual three-week long summer workshop that engaged the involvement of the interdisciplinary research community, which has formed around the goal of building &nbsp;biologically inspired cognitive systems.&nbsp;</p>\n<p>Integration of high-level language, reasoning, and planning processes with low-level perception was achieved with a system that learns to perform manipulation actions by observing humans. &nbsp;A robot creates knowledge models of actions by parsing visually the scene, it reasons about the actions and then performs the same actions. The novelty of the approach lies in the way actions are described. By considering their goal, videos of observed actions are broken into meaningful segments, and these segments together are organized in the form of a simple grammar. Thus, interpreting an observed action is like understanding a sentence that we read or hear. Bio-inspired vision processes parse videos of complex tasks into primitive action segments by computing the symbols that make up the action &ndash; the objects, tools, human body parts, and their movements. Reasoning modules create knowledge beyond the observed, and action execution modules convert the grammatical representation into movements of the robot&rsquo;s actuators.</p>\n<p>Integration of multi-sensory information was achieved in a project that focused on decoding non-invasive EEG measurements from humans in tasks guided by top-down attention, with the goal to use the processed signals to control devices and robots. A library of online-decoding algorithms &nbsp;to &nbsp;extract from EEG signals sensory cortical responses corresponding to auditory-visual stimuli in complex scenes was created and experiments were conducted to classify action intention from visual, auditory, and movement signals.</p>\n<p>Integration of hardware and software was achieved by developing bio-inspired, neuromorphic sensors and computing systems &nbsp; and algorithms for solving higher-level cognitive tasks. Specifically, the team developed an active acoustics sensor consisting of multiple active sonar devices, which record at different frequencies and algorithms to recognize human full-body actions. &nbsp;It created new neuromorphic &nbsp;sensors that integrate images, transient motion signals, and inertial sensor data, and used these sensors in tasks of moving object and action recognition. Furthermore, large-scale neuromorphic architectures and spike-based representations were demonstrated in high-level natural language processing tasks such as sentiment analysis and reasoning with lexical semantic spaces.&nbsp;</p>\n<p>The work on complex action understanding points to a new way of teaching cognitive robots efficiently. Instead of programming robots to execute specific actions, robots can be taught by observing the human expert. The integration of different components of a cognitive system (hardware, sensors, and software) demonstrated in this project has the potential to catalyze a new industry of biologically-inspired cognitive systems, including household and service robots, and systems for intelligent transportation and smart manufacturing. In addition, this interdisciplinary project played a significant role in building capacity for a new emphasis area in engineering, and training for a cognitive systems enginee...",
  "por_txt_cntn": "\nThis INSPIRE award brought together scientists from many disciplines (neuroscience, cognitive science, computational science, mathematics and engineering) in an effort to design and create cognitive systems through the integration of low-level perception and high-level cognitive capabilities,  the integration of multi-sensory processes (vision, audition, and haptics), and the consideration of physical embodiment in algorithmic development and implementation of algorithms in biologically inspired, neuromorphic real-time hardware. The project supported throughout its three years two working groups and an annual three-week long summer workshop that engaged the involvement of the interdisciplinary research community, which has formed around the goal of building  biologically inspired cognitive systems. \n\nIntegration of high-level language, reasoning, and planning processes with low-level perception was achieved with a system that learns to perform manipulation actions by observing humans.  A robot creates knowledge models of actions by parsing visually the scene, it reasons about the actions and then performs the same actions. The novelty of the approach lies in the way actions are described. By considering their goal, videos of observed actions are broken into meaningful segments, and these segments together are organized in the form of a simple grammar. Thus, interpreting an observed action is like understanding a sentence that we read or hear. Bio-inspired vision processes parse videos of complex tasks into primitive action segments by computing the symbols that make up the action &ndash; the objects, tools, human body parts, and their movements. Reasoning modules create knowledge beyond the observed, and action execution modules convert the grammatical representation into movements of the robot\u00c6s actuators.\n\nIntegration of multi-sensory information was achieved in a project that focused on decoding non-invasive EEG measurements from humans in tasks guided by top-down attention, with the goal to use the processed signals to control devices and robots. A library of online-decoding algorithms  to  extract from EEG signals sensory cortical responses corresponding to auditory-visual stimuli in complex scenes was created and experiments were conducted to classify action intention from visual, auditory, and movement signals.\n\nIntegration of hardware and software was achieved by developing bio-inspired, neuromorphic sensors and computing systems   and algorithms for solving higher-level cognitive tasks. Specifically, the team developed an active acoustics sensor consisting of multiple active sonar devices, which record at different frequencies and algorithms to recognize human full-body actions.  It created new neuromorphic  sensors that integrate images, transient motion signals, and inertial sensor data, and used these sensors in tasks of moving object and action recognition. Furthermore, large-scale neuromorphic architectures and spike-based representations were demonstrated in high-level natural language processing tasks such as sentiment analysis and reasoning with lexical semantic spaces. \n\nThe work on complex action understanding points to a new way of teaching cognitive robots efficiently. Instead of programming robots to execute specific actions, robots can be taught by observing the human expert. The integration of different components of a cognitive system (hardware, sensors, and software) demonstrated in this project has the potential to catalyze a new industry of biologically-inspired cognitive systems, including household and service robots, and systems for intelligent transportation and smart manufacturing. In addition, this interdisciplinary project played a significant role in building capacity for a new emphasis area in engineering, and training for a cognitive systems engineer, an amalgam of a computer scientist, an electrical engineer and a cognitive neuroscientist. \n\n\t\t\t\t\tLast Modified: 12/26/2015\n\n\t\t\t\t\tSubmitted..."
 }
}