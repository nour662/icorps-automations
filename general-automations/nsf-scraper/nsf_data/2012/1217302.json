{
 "awd_id": "1217302",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Small: Mining and Learning Visual Contexts for Video Scene Understanding",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-08-01",
 "awd_exp_date": "2018-07-31",
 "tot_intn_awd_amt": 428880.0,
 "awd_amount": 428880.0,
 "awd_min_amd_letter_date": "2012-07-24",
 "awd_max_amd_letter_date": "2017-05-11",
 "awd_abstract_narration": "This project investigates a fundamental and critical, but largely unexplored issue: automatically identifying visual contexts and discovering visual patterns.  Many contemporary approaches that attempt to divide and conquer the video scenes by analyzing the visual objects separately are largely confronted. Exploring visual context has shown its promise for video scene understanding.  Discovering visual contexts is a challenging task, due to the content uncertainty in visual data, structure uncertainty in visual contexts, and semantic uncertainty in visual patterns. The goal of this project is to lay the foundation of contextual mining and learning for video scene understanding, by pursuing innovative approaches to discovering collocation visual patterns, to empowering contextual matching of visual patterns, and to facilitating contextual modeling for visual recognition. The research team develops a unified approach to mining visual collocation patterns and learning visual contexts, and to provide methods and tools that facilitate contextual matching and modeling. \r\n\r\nThis research significantly advances video scene modeling and understanding, and produces an important enabling technology for a wide range of applications including image/video management and search, intelligent surveillance and security, human-computer interaction, social networks, etc. This research program contributes to education through curriculum development, student involvements, and workshops and tutorials outside the vision community. This project also outreaches to K-12 education, and it provides datasets and software on its website to the community.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ying",
   "pi_last_name": "Wu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ying Wu",
   "pi_email_addr": "yingwu@northwestern.edu",
   "nsf_id": "000299558",
   "pi_start_date": "2012-07-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "2145 Sheridan Road",
  "perf_city_name": "Evanston",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602083118",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 143250.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 285630.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><!-- p { margin-bottom: 0.1in; direction: ltr; color: rgb(0, 0, 0); line-height: 120%; widows: 2; orphans: 2; }p.western { font-family: \"Times New Roman\",serif; font-size: 12pt; }p.cjk { font-family: \"??\",\"SimSun\"; font-size: 12pt; }p.ctl { font-family: \"Times New Roman\",serif; font-size: 12pt; }a:visited { color: rgb(128, 0, 128); }a.western:visited {  }a.cjk:visited {  }a.ctl:visited {  }a:link { color: rgb(0, 0, 255); } -->\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\"><span style=\"font-family: Garamond,serif;\"><span style=\"font-size: x-small;\">Video scene understanding refers to recognize the objects and their activities from the video inputs. Generally, the objects in video scenes are rarely isolated but interacting with each other and the environment, i.e., their spatial contexts. Well-established psychological evidence implies that our human perception indeed makes use of a great deal of contextual information. For an automated video understanding system, before visual contexts can be used for video understanding and reasoning, a critical question is: how can the visual contexts and visual patterns can be automatically discovered? </span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\"><span style=\"font-family: Garamond,serif;\"><span style=\"font-size: x-small;\"><br /></span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\"><span style=\"font-family: Garamond,serif;\"><span style=\"font-size: x-small;\">Automatic discovery of visual contexts is a challenging task, due to the content uncertainty in visual data, structure uncertainty in visual contexts, and semantic uncertainty in visual patterns. This project investigates innovative approaches to discovering collocation visual patterns, to empowering contextual matching of</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\"><span style=\"font-family: Garamond,serif;\"><span style=\"font-size: x-small;\">visual patterns, and to facilitating contextual modeling for visual recognition. They lead to more effective ways of representing video for better search, indexing and understanding, and to largely facilitate new applications in video analysis.</span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\">&nbsp;</p>\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\"><span style=\"font-family: Garamond,serif;\"><span style=\"font-size: x-small;\">The project produces algorithms and tools for (1) mining visual contexts for visual pattern discovery, (2) matching visual contexts for visual saliency and motion analysis, and (3) modeling visual contexts for visual recognition of objects and actions. These methods advance the research of video scene modeling and understanding, and result in an important enabling technology for a wide range of applications, including image/video search, intelligent surveillance and security, human-computer interactions, social networks, etc. </span></span></p>\n<p class=\"western\" style=\"margin-bottom: 0in; line-height: 100%;\">&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/01/2018<br>\n\t\t\t\t\tModified by: Ying&nbsp;Wu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nVideo scene understanding refers to recognize the objects and their activities from the video inputs. Generally, the objects in video scenes are rarely isolated but interacting with each other and the environment, i.e., their spatial contexts. Well-established psychological evidence implies that our human perception indeed makes use of a great deal of contextual information. For an automated video understanding system, before visual contexts can be used for video understanding and reasoning, a critical question is: how can the visual contexts and visual patterns can be automatically discovered? \n\n\nAutomatic discovery of visual contexts is a challenging task, due to the content uncertainty in visual data, structure uncertainty in visual contexts, and semantic uncertainty in visual patterns. This project investigates innovative approaches to discovering collocation visual patterns, to empowering contextual matching of\nvisual patterns, and to facilitating contextual modeling for visual recognition. They lead to more effective ways of representing video for better search, indexing and understanding, and to largely facilitate new applications in video analysis.\n \nThe project produces algorithms and tools for (1) mining visual contexts for visual pattern discovery, (2) matching visual contexts for visual saliency and motion analysis, and (3) modeling visual contexts for visual recognition of objects and actions. These methods advance the research of video scene modeling and understanding, and result in an important enabling technology for a wide range of applications, including image/video search, intelligent surveillance and security, human-computer interactions, social networks, etc. \n \n\n\t\t\t\t\tLast Modified: 08/01/2018\n\n\t\t\t\t\tSubmitted by: Ying Wu"
 }
}