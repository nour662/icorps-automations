{
 "awd_id": "1257700",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Art and Vision: Scene Layout from Pictorial Cues",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-04-01",
 "awd_exp_date": "2013-12-31",
 "tot_intn_awd_amt": 151027.0,
 "awd_amount": 151027.0,
 "awd_min_amd_letter_date": "2012-09-18",
 "awd_max_amd_letter_date": "2012-09-18",
 "awd_abstract_narration": "\r\n\r\nCAREER: Art and Vision: Scene Layout from Pictorial Cues\r\n\r\nPI: \tStella (XingXing) Yu    \t\r\n\r\nInstitution: Boston College\r\nArtists are the masters of visual perception. Studying art and vision together can provide new solutions to fundamental problems in computer vision. We focus on inferring scene layout from a single image. This problem has been studied since the earliest days of Artificial Intelligence research, resulting in a host of so-called Shape-from-X methods, where X could be shading, perspective, etc.\r\nUnfortunately, each of these methods works under its own assumptions which often do not hold in real images. How these cues interact and integrate remains elusive. Painters constantly use a combination of four techniques: occlusion, perspective, shading, and form to effectively evoke a 3D percept from a 2D picture. Studying their techniques can lend insights into the computation of recovering scene layout from pixel values. The PI proposes to bring artists and vision scientists together to solve the computational problem of scene layout from pictorial cues. This project realizes it in three areas:\r\neducation, experiments and computational modeling.\r\n\r\nA new interdisciplinary course, Art and Visual Perception, has been developed at Boston College to give a comprehensive cross-examination of how art contributes to the understanding of vision, and how vision contributes to the generation and viewing of art. Students are actively engaged in both art practice and vision experiments.\r\nLearning art and vision together results in a deeper understanding than studying each discipline separately. Students' assignments also result in valuable datasets for vision research.\r\n\r\nThe computational approach to scene layout from pictorial cues in this project is to group pixels into spatially organized surfaces from a global integration of multiple pictorial cues in a spectral graph-theoretic framework. The goal is to turn artistic rendering knowledge on how these cues interact into a computational reality.\r\nThe PI will study geometry (occlusion and perspective), appearance (brightness and color), and form using eye tracking and psychophysics experiments and computational models. These efforts are organized into two phases that progress from inferring the spatial layout from scenes made of planar surfaces (rooms and streets) to scenes made of curved surfaces (landscape and generic scenes).\r\n\r\nIntellectual Merit\r\n\r\nWhat is most remarkable about vision is its ability to perceive 3D spatial layout from a single 2D image. The proposed research replicates this ability in computation from a grouping perspective.\r\nCompared to statistical learning approaches, the grouping method is not only generic and thus scales well with the number of scenes, but can also produce a precise organization of surfaces in the scene.\r\nCompared to traditional Shape-from-X approaches, the grouping method examines each pictorial cue in conjunction with others. The integration of these multiple pictorial cues allows them for the first time to become applicable to real images. The PI has developed the essential grouping machinery in spectral graph theory for depth segregation. Compared to most existing formulations on this topic, it has unparalleled conceptual simplicity, computational efficiency, and guaranteed near-global optimality. The proposed research on brightness and color perception, in connection with Shape-from- Shading and surface organization, will help clarify the role of low- level and high-level mechanisms in the long-standing scientific debate between Hering and Helmholtz on color perception.\r\n\r\nBroader Impact\r\n\r\nThis project bridges the gap between art and science not only in research but also in education by developing a new curriculum that traverses the areas of neuroscience, psychology, computer science, and visual arts, by involving students in art practice and scientific experiments, and by providing a forum for artists and scientists to exchange ideas on visual perception. These interdisciplinary efforts befit the liberal arts education tradition at Boston College. This project will not only benefit from the strong Fine Arts department on campus, but also cultivate computer science awareness and outreach to non-technical people, and promote the growth of the young Computer Science department at Boston College.\r\n\r\nURL:  http://www.cs.bc.edu/~syu/artvis/\r\n",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Stella",
   "pi_last_name": "Yu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stella Yu",
   "pi_email_addr": "stellayu@umich.edu",
   "nsf_id": "000217299",
   "pi_start_date": "2012-09-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "International Computer Science Institute",
  "inst_street_address": "2150 SHATTUCK AVE",
  "inst_street_address_2": "SUITE 250",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106662900",
  "inst_zip_code": "947041345",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "INTERNATIONAL COMPUTER SCIENCE INSTITUTE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GSRMP1QCXU74"
 },
 "perf_inst": {
  "perf_inst_name": "International Computer Science Institute",
  "perf_str_addr": "1947 CENTER ST STE 600",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947044115",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 39474.0
  },
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 111553.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of our project is to infer scene layout from a single image,which is simply an array of numbers indicating the intensity of light at individual pixel locations. &nbsp;We need to organize these numbers into surfaces oriented towards the viewer in the 3D space. &nbsp;When we understand the spatial layout, we can visualize what the scene looks like from a different vantage point (Figure 1).</p>\n<p>Painters constantly use a combination of techniques to effectively evoke a 3D percept from a 2D picture, and studying their techniques can lend insights into the computation of recovering scene layout from pixel values (Figure 2). &nbsp;Occlusion effectively depicts elevation and the range of depth, and it is the most universal and earliest depth cue developed along with line drawings. &nbsp;Perspective was popularized by Renaissance, and it includes focal convergence, foreshortening, and texture gradients. &nbsp;Shading can be subtle but powerful, and requires keen observation and mastery of chiaroscuro. &nbsp;Form can evoke a rich sense of space and volume from a flat 2D pattern by its interaction with visual memory. &nbsp;This is achieved by the fine precision of simple shapes in Kelly&rsquo;s work, and by the viewer&rsquo;s long scrutiny and problem solving of complex geometrical configurations in Twaddle&rsquo;s work.</p>\n<p>We have developed a new interdisciplinary course on Art and Vision. &nbsp;We bring neuroscience, psychology, computer science, visual art, scientific imaging and visualization together in examining how we perceive light, color, motion, shape, material, depth and distance. &nbsp;Students learn basic drawing skills along with rudimentary intuitions in computation and programming. &nbsp;Emphasis is placed on appreciating how artistic rendering contributes to the understanding of inner workings of visual sense, and how effective visual communication can be achieved through more knowledge on visual perception.</p>\n<p>In computer vision, some of these artistic techniques can find their counterparts&nbsp;known as Shape-from-X, where X could be junctions and contours, perspective, texture, shadows, or shading. Unfortunately, each Shape-from-X method makes its own assumptions that could conflict with others and often hold poorly in real scene images.</p>\n<p>An alternative to Shape-from-X is statistical learning. &nbsp;Unlike any Shape-from-X that has its own stylized features, statistical learning approaches take many real images as training examples, extract many candidate features from them, and memorize the association between the 2D features and annotated 3D attributes. &nbsp;Given a new image, its features are computed and used as a query to the memory, and the most likely 3D attributes are retrieved. &nbsp;The success thus critically depends on how similar the test image is to the training images.</p>\n<p>Our computational approach to scene layout from a single image is to pop surfaces out from a global integration of multiple sources of information. &nbsp;These cues act upon some intermediate representations (e.g. lines and planes) and their compatibility with each other is evaluated, so that scene layout emerges from the most consistent group of visual representations (Figure 3). &nbsp;We have conducted a series of human vision experiments and made progress on several aspects about this computational framework.</p>\n<p>For example, we have developed a new integration machinery in spectral&nbsp;graph theory, called Angular Embedding, for reconciling multiple&nbsp;local pairwise measurements into global ordering of elements in a&nbsp;metric space. &nbsp;The problem is similar to obtaining a consensus movie&nbsp;ranking from many users' individual rankings of movies.&nbsp;However, unlike conventional embedding which ranks elements&nbsp;sequentially on a line, our angular embedding places elements in the&nbsp;complex plane, with angles encoding th...",
  "por_txt_cntn": "\nThe goal of our project is to infer scene layout from a single image,which is simply an array of numbers indicating the intensity of light at individual pixel locations.  We need to organize these numbers into surfaces oriented towards the viewer in the 3D space.  When we understand the spatial layout, we can visualize what the scene looks like from a different vantage point (Figure 1).\n\nPainters constantly use a combination of techniques to effectively evoke a 3D percept from a 2D picture, and studying their techniques can lend insights into the computation of recovering scene layout from pixel values (Figure 2).  Occlusion effectively depicts elevation and the range of depth, and it is the most universal and earliest depth cue developed along with line drawings.  Perspective was popularized by Renaissance, and it includes focal convergence, foreshortening, and texture gradients.  Shading can be subtle but powerful, and requires keen observation and mastery of chiaroscuro.  Form can evoke a rich sense of space and volume from a flat 2D pattern by its interaction with visual memory.  This is achieved by the fine precision of simple shapes in Kelly\u00c6s work, and by the viewer\u00c6s long scrutiny and problem solving of complex geometrical configurations in Twaddle\u00c6s work.\n\nWe have developed a new interdisciplinary course on Art and Vision.  We bring neuroscience, psychology, computer science, visual art, scientific imaging and visualization together in examining how we perceive light, color, motion, shape, material, depth and distance.  Students learn basic drawing skills along with rudimentary intuitions in computation and programming.  Emphasis is placed on appreciating how artistic rendering contributes to the understanding of inner workings of visual sense, and how effective visual communication can be achieved through more knowledge on visual perception.\n\nIn computer vision, some of these artistic techniques can find their counterparts known as Shape-from-X, where X could be junctions and contours, perspective, texture, shadows, or shading. Unfortunately, each Shape-from-X method makes its own assumptions that could conflict with others and often hold poorly in real scene images.\n\nAn alternative to Shape-from-X is statistical learning.  Unlike any Shape-from-X that has its own stylized features, statistical learning approaches take many real images as training examples, extract many candidate features from them, and memorize the association between the 2D features and annotated 3D attributes.  Given a new image, its features are computed and used as a query to the memory, and the most likely 3D attributes are retrieved.  The success thus critically depends on how similar the test image is to the training images.\n\nOur computational approach to scene layout from a single image is to pop surfaces out from a global integration of multiple sources of information.  These cues act upon some intermediate representations (e.g. lines and planes) and their compatibility with each other is evaluated, so that scene layout emerges from the most consistent group of visual representations (Figure 3).  We have conducted a series of human vision experiments and made progress on several aspects about this computational framework.\n\nFor example, we have developed a new integration machinery in spectral graph theory, called Angular Embedding, for reconciling multiple local pairwise measurements into global ordering of elements in a metric space.  The problem is similar to obtaining a consensus movie ranking from many users' individual rankings of movies. However, unlike conventional embedding which ranks elements sequentially on a line, our angular embedding places elements in the complex plane, with angles encoding the positions and radii encoding the confidence in the positions.  Elements with low confidence in their positions are placed near the origin of the complex plane, which naturally indicates that all angular positions become equally possib..."
 }
}