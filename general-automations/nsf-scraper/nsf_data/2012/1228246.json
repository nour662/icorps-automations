{
 "awd_id": "1228246",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Leverage Subsampling for Regression and Dimension Reduction",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032927299",
 "po_email": "yzeng@nsf.gov",
 "po_sign_block_name": "Yong Zeng",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2012-08-29",
 "awd_max_amd_letter_date": "2012-08-29",
 "awd_abstract_narration": "As a result of rapid advances in information technology, massive datasets are being generated in all fields of science, engineering, social science, business, and government. Useful information is often extracted from these data through statistical model fitting, e.g., through regression models. These models are useful for describing relationships between predictor variables and a response variable. Given a set of n data elements and p predictors, p and/or n can be large in much modern massive data set applications. In these cases, conventional algorithms often face severe computational challenges. Subsampling of rows and/or columns of a data matrix have traditionally been employed as a heuristic to reduce the size of large data sets, thus enabling computations to run more quickly. Recently, however, an innovative sampling methodology that uses the empirical statistical leverage scores of the data matrix as a nonuniform importance sampling distribution has been proposed. This has been applied to the ordinary least squares (OLS) problem and other related problems, and this leverage-based nonuniform sampling procedure gives a very good approximation to the OLS based on full data (when p is small and n is large) more rapidly than traditional methods, both in worst-case theory and in high-quality numerical implementations. As of yet, however, the statistical properties of these algorithms are unexplored. Understanding these properties is of interest for both fundamental and very practical reasons; and the investigators' work addresses these problems. The investigators consider both statistical theory as well as the evaluation of that theory with high-quality numerical implementations on large real-world data. \r\n\r\nThis research proposal consists of two related research thrusts, both of which center around the common goal of an integrated treatment of statistical and computational issues. The first research thrust focuses on studying the statistical properties of the subsampling estimation using the statistical leverage scores in linear regression. The second research thrust generalizes the theory and methods to nonlinear regression and dimension reduction models. The proposed theory and methods serve as an inspiration for new ideas to push statistical methodology development forward. The research provides new insight into the existing algorithms, produces innovative methodologies for analyzing large-scale data, inspires new lines of quantitative investigations in interdisciplinary research and offers a unique educational experience.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bin",
   "pi_last_name": "Yu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bin Yu",
   "pi_email_addr": "binyu@stat.berkeley.edu",
   "nsf_id": "000465148",
   "pi_start_date": "2012-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "367 Evans Hall",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947203860",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "806900",
   "pgm_ele_name": "CDS&E-MSS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>With rapid advances of information technology, massive datasets are collected by all fields of science, engineering, social science, business, and government. Useful or meaningful information is extracted from these data often through statistical means or model fitting, typically through regression models.&nbsp;</span>Various models and methods have been developed for regression analysis in the literature, ranging from classic linear regression to nonparametric regression. Nevertheless, every regression model and method can be seriously compromised if the dimensionality of X, say p, is large. For example, the computational cost and the sample size that needed for smoothing the nonparametric regression escalate exponentially as p increases. Therefore, unless we have gigantic samples and unlimited computational resources, it is necessary to reduce the number of predictors or dimensions of X to ensure the success of regression analysis. Supported by this project, we have developed powerful statistical tools that make use of sampling ideas for tackling big data regression problem.</p>\n<div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><strong>Intellectual Merit&nbsp;</strong></p>\n</div>\n</div>\n</div>\n<p>Supported by this project, we obtained the following significant results. &nbsp;</p>\n<p>(1) &nbsp;We provide the first interpretation of algorithmic leveraging&nbsp;paradigm from a statistical analysis point of view. For theoretical results, we provide a simple yet effective framework to evaluate the&nbsp;statistical properties of algorithmic leveraging in the context of estimating&nbsp;parameters in a linear regression model with a fixed number of predictors.&nbsp;</p>\n<p>(2) We bridge that gap by providing the statistical analysis of the algorithmic leveraging paradigm. We do so in the context of parameter estimation in fitting linear regression models for large-scale data|where, by \"large-scale,\" we mean that the data dene a high-dimensional problem in terms of sample size n, as opposed to the dimension p of the parameter space.&nbsp;</p>\n<p>(3) We&nbsp;reviewed the general framework of sliced inverse regression and introduced the weighted leverage score for variable screening, illustrated the asymptotic behavior and rank consistency of weighted leverage score. Simulation studies and real data applications are also conducted to show the power of proposed methods.</p>\n<p>(4)&nbsp;We proposed a reference-free and distribution-free binning method, MetaGen, which makes use of the relative abundance information from multiple samples to cluster contigs into different species bins and the Bayesian information criterion (BIC) to determine the number of species in the samples.</p>\n<p>(5) We proposed the sequential leveraging method and stated the main theorem, present some simulation results to support the theorem presented and do a real data analysis using sequential leveraging method.</p>\n<p>(6) F<span>or the first time in the case of p&gt;&gt;n, valid confidence intervals for post-Lasso estimators via parametric bootstrap when p&gt;&gt;n and first statistical analysis of leverage sampling and LS estimation with two new improved procedures based on leverage sampling for the fixed p case. Both problems are important for big data since both LS and Lasso are canonical methods for discovering relationships between variables.</span></p>\n<p><span>(7)&nbsp;In line with statistical dimension reduction methods, we developed staNMF, a method that combines a scalable implementation of nonnegative matrix factorization with a new stability-driven model selection criterion.&nbsp;</span></p>\n<p><span>(8) &nbsp;We&nbsp;<span>use the Lasso to both select relevant&nbsp;covariates and perform the adjustment. We study the resulting estimator under&nbsp;the Neyman-Rubin&nbsp;model for randomization and present conditions on the&nbsp;covariates and potential outcomes which guarantee that the Lasso is more&nbsp;efficient than the unadjusted estimator and provide a conservative estimate of the&nbsp;asymptotic variance.</span></span></p>\n<div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><strong>Broader Impacts&nbsp;</strong></p>\n</div>\n</div>\n</div>\n<div class=\"page\" title=\"Page 1\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>Our broad research spectrum from basic statistical theory research to applied fields such as Biology and Chemistry.&nbsp;</span></p>\n</div>\n</div>\n</div>\n<p><span>As a popular sampling based framework for solving scalable least squares&nbsp;regression and other matrix problems&nbsp;via the empirical statistical leverage scores of the data, algorithmic leveraging has been demonstrating many&nbsp;desirable algorithmic properties. Our proposed algorithms&nbsp;<span>demonstrated its benefits on several real dataset. Especially, the proposed&nbsp;<span>SSANOVAs show the promising potential for discovering interesting functional&nbsp;relationships among large noisy data sets in the physical and social sciences.</span></span></span></p>\n<p>In Biology, t<span>he proposed leveraging algorithms have been applied to analyze high throughput RNAseq&nbsp;</span><span>data. We observe that LEV, LEVUNW, SLEV, and UNIF all have comparable sample variances. When the subsample size is very small, all four methods have comparable sample bias; but when the subsample size is larger, then LEVUNW has a slightly larger bias than the other three estimates. In the other direction of genetic data analysis, o</span>ur purposed statNMF method has been very successful in analyzing noisy gene expression data.&nbsp;</p>\n<p>In Chemistry, the tensor sufficient dimension reduction model is proposed to analyze the optical electronic nose data (also known as the colorimetric sensor array data). The data set contained large amount of tensor observations, which are multiway arrays.</p>\n</div>\n</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/15/2016<br>\n\t\t\t\t\tModified by: Bin&nbsp;Yu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith rapid advances of information technology, massive datasets are collected by all fields of science, engineering, social science, business, and government. Useful or meaningful information is extracted from these data often through statistical means or model fitting, typically through regression models. Various models and methods have been developed for regression analysis in the literature, ranging from classic linear regression to nonparametric regression. Nevertheless, every regression model and method can be seriously compromised if the dimensionality of X, say p, is large. For example, the computational cost and the sample size that needed for smoothing the nonparametric regression escalate exponentially as p increases. Therefore, unless we have gigantic samples and unlimited computational resources, it is necessary to reduce the number of predictors or dimensions of X to ensure the success of regression analysis. Supported by this project, we have developed powerful statistical tools that make use of sampling ideas for tackling big data regression problem.\n\n\n\n\nIntellectual Merit \n\n\n\n\nSupported by this project, we obtained the following significant results.  \n\n(1)  We provide the first interpretation of algorithmic leveraging paradigm from a statistical analysis point of view. For theoretical results, we provide a simple yet effective framework to evaluate the statistical properties of algorithmic leveraging in the context of estimating parameters in a linear regression model with a fixed number of predictors. \n\n(2) We bridge that gap by providing the statistical analysis of the algorithmic leveraging paradigm. We do so in the context of parameter estimation in fitting linear regression models for large-scale data|where, by \"large-scale,\" we mean that the data dene a high-dimensional problem in terms of sample size n, as opposed to the dimension p of the parameter space. \n\n(3) We reviewed the general framework of sliced inverse regression and introduced the weighted leverage score for variable screening, illustrated the asymptotic behavior and rank consistency of weighted leverage score. Simulation studies and real data applications are also conducted to show the power of proposed methods.\n\n(4) We proposed a reference-free and distribution-free binning method, MetaGen, which makes use of the relative abundance information from multiple samples to cluster contigs into different species bins and the Bayesian information criterion (BIC) to determine the number of species in the samples.\n\n(5) We proposed the sequential leveraging method and stated the main theorem, present some simulation results to support the theorem presented and do a real data analysis using sequential leveraging method.\n\n(6) For the first time in the case of p&gt;&gt;n, valid confidence intervals for post-Lasso estimators via parametric bootstrap when p&gt;&gt;n and first statistical analysis of leverage sampling and LS estimation with two new improved procedures based on leverage sampling for the fixed p case. Both problems are important for big data since both LS and Lasso are canonical methods for discovering relationships between variables.\n\n(7) In line with statistical dimension reduction methods, we developed staNMF, a method that combines a scalable implementation of nonnegative matrix factorization with a new stability-driven model selection criterion. \n\n(8)  We use the Lasso to both select relevant covariates and perform the adjustment. We study the resulting estimator under the Neyman-Rubin model for randomization and present conditions on the covariates and potential outcomes which guarantee that the Lasso is more efficient than the unadjusted estimator and provide a conservative estimate of the asymptotic variance.\n\n\n\n\n\n\n\nBroader Impacts \n\n\n\n\n\n\n\nOur broad research spectrum from basic statistical theory research to applied fields such as Biology and Chemistry. \n\n\n\n\nAs a popular sampling based framework for solving scalable least squares regression and other matrix problems via the empirical statistical leverage scores of the data, algorithmic leveraging has been demonstrating many desirable algorithmic properties. Our proposed algorithms demonstrated its benefits on several real dataset. Especially, the proposed SSANOVAs show the promising potential for discovering interesting functional relationships among large noisy data sets in the physical and social sciences.\n\nIn Biology, the proposed leveraging algorithms have been applied to analyze high throughput RNAseq data. We observe that LEV, LEVUNW, SLEV, and UNIF all have comparable sample variances. When the subsample size is very small, all four methods have comparable sample bias; but when the subsample size is larger, then LEVUNW has a slightly larger bias than the other three estimates. In the other direction of genetic data analysis, our purposed statNMF method has been very successful in analyzing noisy gene expression data. \n\nIn Chemistry, the tensor sufficient dimension reduction model is proposed to analyze the optical electronic nose data (also known as the colorimetric sensor array data). The data set contained large amount of tensor observations, which are multiway arrays.\n\n\n\n\n \n\n\t\t\t\t\tLast Modified: 12/15/2016\n\n\t\t\t\t\tSubmitted by: Bin Yu"
 }
}