{
 "awd_id": "1222390",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "ATD Collaborative Research:   Theory and Algorithms for High Dimensional Learning",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Leland Jameson",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 300000.0,
 "awd_min_amd_letter_date": "2012-08-28",
 "awd_max_amd_letter_date": "2014-08-07",
 "awd_abstract_narration": "The investigators and their collaborators study how to organize and query high dimensional data in order to extract relevant content while avoiding the so-called curse of dimensionality.   The team is developing new analytical and numerical methods based on sparsity, adaptivity, and variable reduction. The focus is placed on developing a coherent theory that results in sophisticated state-of-the-art numerical algorithms that can be applied in a variety of settings. This activity is a  critical component of many scientific problems since it  complements and supports the scientific methods of theory, experimentation, and simulation. A setting of particular interest  to this project is learning tasks such as regression and classification. The research team is developing quantifiable frameworks and algorithms for learning that systematically break down the high dimensional barriers and exploit empirical data collections. \r\n\r\nMany scientic problems, vital to the security, economy, and health of our nation, are so complex that they challenge this nation's most sophisticated computational resources. Examples occur in modeling physical and biological systems, e.g. in atmospheric modeling; in optimal design (optimal control and shape optimization);  and also in understanding social networks such as those that occur in threat detection. The complexity of these problems prohibits the use of traditional off-the-shelf computational techniques for their solution. This research team develops new computational tools that lead to state of the art algorithms for detecting and the capturing critical information held in the solution of such complex systems. An emphasis in  this project is the  processing of  data that arise in threat detection,  damage assessment, and containment.  This requires the simultaneous analysis of  data obtained from different modalities and \r\na variety of sensors. The new algorithms are applied, for example,  to identify and track the migration of airborne biological and chemical contaminants. Another application area is  the development of new approaches to high dimensional problems related to gene sequencing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Binev",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peter Binev",
   "pi_email_addr": "binev@math.sc.edu",
   "nsf_id": "000482290",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Wolfgang",
   "pi_last_name": "Dahmen",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Wolfgang A Dahmen",
   "pi_email_addr": "dahmen@math.sc.edu",
   "nsf_id": "000520191",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jerry",
   "pi_last_name": "Ebalunode",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jerry Ebalunode",
   "pi_email_addr": "jebaluno@mailbox.sc.edu",
   "nsf_id": "000587574",
   "pi_start_date": "2012-08-28",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University South Carolina Research Foundation",
  "inst_street_address": "915 BULL ST",
  "inst_street_address_2": "STE 202",
  "inst_city_name": "COLUMBIA",
  "inst_state_code": "SC",
  "inst_state_name": "South Carolina",
  "inst_phone_num": "8037777093",
  "inst_zip_code": "292084009",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "SC06",
  "org_lgl_bus_name": "SOUTH CAROLINA RESEARCH FOUNDATION",
  "org_prnt_uei_num": "",
  "org_uei_num": "ELBVJ1KYX976"
 },
 "perf_inst": {
  "perf_inst_name": "University of South Carolina at Columbia",
  "perf_str_addr": "",
  "perf_city_name": "Columbia",
  "perf_st_code": "SC",
  "perf_st_name": "South Carolina",
  "perf_zip_code": "292080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "SC06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "755200",
   "pgm_ele_name": "COFFES"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "6877",
   "pgm_ref_txt": "ALGORITHMS IN THREAT DETECTION"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 86052.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 130132.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 83816.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High dimensional learning focuses on problems of extracting essential information from multidimensional data in order to learn about the object or the process, which the data is about. The goals are to design algorithms that are both reliable, meaning that the result is correct with very high probability, and effective, meaning that the used computational resources and the processing time are reasonable. The usual understanding of the latter is that the complexity of the algorithms should scale nearly linearly with respect to the size of the data.</p>\n<p>The problem of classification is to predict a certain property based on sampling the investigated process. It arises in a plentitude of practical settings often requiring different approaches. While we try to be as general as possible in our research, we focus here on those relevant for the problems arising from threat detection situations. The prediction is based on some preliminary observations of these parameters for which the outcome is known. Formally, one can say that we record +1 for a particular set of parameters if the outcome is positive and -1 otherwise. It is often the case that observations with the same values of the parameters have different outcomes. The learning algorithm has to account for this by trying to minimize the risk of misclassification. The main challenge is that the amount of preliminary data is limited and it is subject to noise. Ideally, assuming that we know everything about the phenomenon, for any particular observation of the parameters we can assess which outcome, +1 or -1, is more likely and then set it to be our prediction. Combining all such predictions with outcome +1 will constitute a multidimensional subset of the parameter domain, called Bayes set. This set has a minimal risk of misclassification. The goal of learning algorithms is to find a good approximation in terms of minimizing with high probability the excess risk of this set using a limited amount of data. The theoretical understanding of this problem leads to two contradicting conditions: smoothness, requiring that the outcomes do not differ a lot for close observations, and a margin condition, requiring that there is a sharp change from -1 to 1 near the boundary of the Bayes set. In [Annals of Statistics 42, 2141], we study classification algorithms for data sets generated by an unknown random process. We develop both theoretical and numerical results. We build an approximation of the Bayes set by adaptive partitioning of the parametric domain using set estimators and analyze the performance through the introduction of a new modulus. This theory is then applied to the construction of set estimators built on adaptive partitioning with decoration allowing the subsets of the partition to be split by an arbitrary hyperplane, which vastly increases the accuracy. These algorithms are proven to give optimal performance on model classes built on weaker and hence less conflicting Besov smoothness (allowing piecewise smooth behavior) and margin conditions on the Bayes set.</p>\n<p>Several other results were received in the general area of data registration, in particular, in connection with applications in electron microscopy. Significant achievements featuring methods for nonrigid registration of scanning transmission electron miscroscopy (STEM) images, application of nonlocal means in Poisson noise removal from STEM images, a new alignment procedure and limited data recovery for electron tomography, as well as a new approach to discrete recovery of tomographic data called Discrete Iterative Partial Segmentation technique (DIPS). In [Ultramicroscopy 138 (2014), 46] we proposed a methodology for extracting an increased level of information by processing a series of data sets suffering, in particular, from high degree of spatial uncertainty caused by complex multiscale motion during the acquisition process. An important role is played by a nonrigid pixel-wise registration method that copes with low signal-to-noise ratios. This is accompanied by formulating objective quality measures, which replace human intervention and visual inspection in the processing chain. This methodology is applied, in particular, to scanning transmission electron microscopic imaging of siliceous zeolite material that exhibits the above-mentioned obstructions and therefore serves as orientation and a test of our procedures. Further improvements of our non-rigid registration procedure using minimization in a high-dimensional parametric space resulted in the paper [Nature Communications 5 (2014), #4155], where we demonstrate sub-picometre precision measurements of atom positions in aberration-corrected Z-contrast STEM images. This is more than five times better than previous methods!</p>\n<p>&nbsp;A major difficulty in solving high-dimensional problems is the fact that computational work tends to scale exponentially in the spatial dimension known as the curse of dimensionality. The way around it could be to exploit that the solution has in addition some sparse representation. We investigated sparsity in different contexts including sparse tensor representations of solutions of partial differential equations (PDEs) and in terms of data assimilation problems based on certified reduced models for parametric PDEs.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2016<br>\n\t\t\t\t\tModified by: Peter&nbsp;Binev</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHigh dimensional learning focuses on problems of extracting essential information from multidimensional data in order to learn about the object or the process, which the data is about. The goals are to design algorithms that are both reliable, meaning that the result is correct with very high probability, and effective, meaning that the used computational resources and the processing time are reasonable. The usual understanding of the latter is that the complexity of the algorithms should scale nearly linearly with respect to the size of the data.\n\nThe problem of classification is to predict a certain property based on sampling the investigated process. It arises in a plentitude of practical settings often requiring different approaches. While we try to be as general as possible in our research, we focus here on those relevant for the problems arising from threat detection situations. The prediction is based on some preliminary observations of these parameters for which the outcome is known. Formally, one can say that we record +1 for a particular set of parameters if the outcome is positive and -1 otherwise. It is often the case that observations with the same values of the parameters have different outcomes. The learning algorithm has to account for this by trying to minimize the risk of misclassification. The main challenge is that the amount of preliminary data is limited and it is subject to noise. Ideally, assuming that we know everything about the phenomenon, for any particular observation of the parameters we can assess which outcome, +1 or -1, is more likely and then set it to be our prediction. Combining all such predictions with outcome +1 will constitute a multidimensional subset of the parameter domain, called Bayes set. This set has a minimal risk of misclassification. The goal of learning algorithms is to find a good approximation in terms of minimizing with high probability the excess risk of this set using a limited amount of data. The theoretical understanding of this problem leads to two contradicting conditions: smoothness, requiring that the outcomes do not differ a lot for close observations, and a margin condition, requiring that there is a sharp change from -1 to 1 near the boundary of the Bayes set. In [Annals of Statistics 42, 2141], we study classification algorithms for data sets generated by an unknown random process. We develop both theoretical and numerical results. We build an approximation of the Bayes set by adaptive partitioning of the parametric domain using set estimators and analyze the performance through the introduction of a new modulus. This theory is then applied to the construction of set estimators built on adaptive partitioning with decoration allowing the subsets of the partition to be split by an arbitrary hyperplane, which vastly increases the accuracy. These algorithms are proven to give optimal performance on model classes built on weaker and hence less conflicting Besov smoothness (allowing piecewise smooth behavior) and margin conditions on the Bayes set.\n\nSeveral other results were received in the general area of data registration, in particular, in connection with applications in electron microscopy. Significant achievements featuring methods for nonrigid registration of scanning transmission electron miscroscopy (STEM) images, application of nonlocal means in Poisson noise removal from STEM images, a new alignment procedure and limited data recovery for electron tomography, as well as a new approach to discrete recovery of tomographic data called Discrete Iterative Partial Segmentation technique (DIPS). In [Ultramicroscopy 138 (2014), 46] we proposed a methodology for extracting an increased level of information by processing a series of data sets suffering, in particular, from high degree of spatial uncertainty caused by complex multiscale motion during the acquisition process. An important role is played by a nonrigid pixel-wise registration method that copes with low signal-to-noise ratios. This is accompanied by formulating objective quality measures, which replace human intervention and visual inspection in the processing chain. This methodology is applied, in particular, to scanning transmission electron microscopic imaging of siliceous zeolite material that exhibits the above-mentioned obstructions and therefore serves as orientation and a test of our procedures. Further improvements of our non-rigid registration procedure using minimization in a high-dimensional parametric space resulted in the paper [Nature Communications 5 (2014), #4155], where we demonstrate sub-picometre precision measurements of atom positions in aberration-corrected Z-contrast STEM images. This is more than five times better than previous methods!\n\n A major difficulty in solving high-dimensional problems is the fact that computational work tends to scale exponentially in the spatial dimension known as the curse of dimensionality. The way around it could be to exploit that the solution has in addition some sparse representation. We investigated sparsity in different contexts including sparse tensor representations of solutions of partial differential equations (PDEs) and in terms of data assimilation problems based on certified reduced models for parametric PDEs.\n\n\t\t\t\t\tLast Modified: 11/30/2016\n\n\t\t\t\t\tSubmitted by: Peter Binev"
 }
}