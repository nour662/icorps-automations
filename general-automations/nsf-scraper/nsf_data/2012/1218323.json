{
 "awd_id": "1218323",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small:Energy-Optimized Memory Hierarchies",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2012-06-25",
 "awd_max_amd_letter_date": "2012-06-25",
 "awd_abstract_narration": "The early 21st century finds power and energy as the central challenges to continued improvements in computer system performance and cost. From handheld mobile devices to the data centers that support them, computer architects must invent new energy-efficient techniques to facilitate future innovations in science, education, government and commerce. In current systems, the memory hierarchy-which stores and moves the data values used and produced by the computation-consumes more energy than the computation itself. For example, obtaining operands for a double-precision multiply-add can consume 1.7 to 200 times the operation's energy depending on where in the memory hierarchy the values are stored. Improving the energy-efficiency of memory hierarchies can not only enable advances in future computer systems, but also reduces the emission of greenhouse gases.\r\n\r\nThis project seeks novel memory hierarchy designs that minimize power and energy, rather than the classical focus on reducing latency and/or bandwidth. These designs build on three key hypotheses: (1) cache memories can reduce energy more than either latency or bandwidth, (2) optimizing latency becomes less important when it can be tolerated, and (3) overlapping activity does not save power, but can save energy due to static power dissipation. Initial research directions include (1) a technique to reduce address translation energy using a hybrid virtual/physical cache that eliminates the need to access a highly-associative TLB on every memory access and (2) energy-efficient cache hierarchies that use data compression techniques to replace the high energy cost of misses with lower compression and decompression overheads. This research will also extend the widely-used open-source gem5 simulation infrastructure to more accurately model the power and energy of emerging memory hierarchies.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Mark",
   "pi_last_name": "Hill",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Mark D Hill",
   "pi_email_addr": "markhill@cs.wisc.edu",
   "nsf_id": "000328470",
   "pi_start_date": "2012-06-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Wood",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Wood",
   "pi_email_addr": "david@cs.wisc.edu",
   "nsf_id": "000442514",
   "pi_start_date": "2012-06-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Wisconsin-Madison",
  "inst_street_address": "21 N PARK ST STE 6301",
  "inst_street_address_2": "",
  "inst_city_name": "MADISON",
  "inst_state_code": "WI",
  "inst_state_name": "Wisconsin",
  "inst_phone_num": "6082623822",
  "inst_zip_code": "537151218",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "WI02",
  "org_lgl_bus_name": "UNIVERSITY OF WISCONSIN SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "LCLSJAGTNZQ7"
 },
 "perf_inst": {
  "perf_inst_name": "University of Wisconsin-Madison",
  "perf_str_addr": "21 North Park Street, Suite 6401",
  "perf_city_name": "Madison",
  "perf_st_code": "WI",
  "perf_st_name": "Wisconsin",
  "perf_zip_code": "537151218",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "WI02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The early 21st century brings power and energy challenges to computer systems from the handheld to the data center. More energy-efficient computers provide greater utility or save environmental resources.</p>\n<p>Two important aspects of computer systems are memory and processing. Memory keeps working information and is usually implemented in a hierarchy: small level-one caches holding the most active data, followed by larger caches, then even larger main memory, and ultimately permanent storage (disks, FLASH). Processing has been traditionally done with central processing units (CPUs), but it can be done more energy-efficiently with graphics processing units (GPUs).</p>\n<p>This project worked to improve computer system energy-efficiency with many contributions that can be organized in the three.</p>\n<p><strong>Improved Cache Compression.</strong> Cache compression seeks to losslessly reduce the size of commonly-stored data so that data is more often found in a cache (a hit) rather than not (a miss). Misses cost time and energy. One result of this project is a cache design that ingeniously has cache data point back to address information (tags) to facilitate fast hits to data compressed by varying amounts. A follow-on result uses skewing to reduce address tag overhead and improve performance and energy.</p>\n<p><strong>Efficient Virtual Memory.</strong> Virtual memory is a technique for caching data in main memory from a vast storage system. Data in virtual memory is found using a translation lookaside buffer (TLB) that&mdash;like a cache&mdash;can hit or miss. TLB miss frequencies are increasing for especially (cloud) server workloads where the amount of main memory that can be purchased for $10K has increased a million-fold (from megabytes to terabytes) while TLB sizes have stayed relatively fixed. A result from this grant is a design with an arbitrarily-large ``direct segment&rsquo;&rsquo; that can eliminate TLB misses from a large fraction of memory, saving time and energy. This result was also generalized to support virtualized systems and to enable many direct segments.</p>\n<p><strong>Saving Energy with Graphics Processing Units.</strong> GPUs offer an energy efficient alternative to CPUs for executing some general-purpose workloads. GPUs can be hard to program and can interact poorly with memory hierarchies. A result is to provide programmer-friendly virtual memory for GPUs by adapting CPU TLB methods including proposing hardware that can process dozens of concurrent TLB misses. This is useful for handling bursts of misses that analysis found common for GPUs. Another result enables GPU programs to benefit from the synergy of two heretofore-incompatible alternatives: static hierarchical GPU synchronization and dynamic load balancing (e.g., via work stealing).</p>\n<p>Moreover, the project provided broader impacts several ways. To help NSF dollars go further, it developed and refined open-simulation infrastructures that have been used by 1000s of students and researchers (gem5, gem5-gpu, BadgerTrap). This grant has provided partial support for 6 different Ph.D. students. It has promulgated the work through an annual computer architecture affiliates meetings, and disseminated ideas and simulators in courses (Wisconsin and elsewhere) and via external presentations by faculty and students. Through student internships and graduating students joining the computer industry, ultimate outcome of work funded by this grant is not yet bounded.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 07/29/2015<br>\n\t\t\t\t\tModified by: Mark&nbsp;D&nbsp;Hill</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe early 21st century brings power and energy challenges to computer systems from the handheld to the data center. More energy-efficient computers provide greater utility or save environmental resources.\n\nTwo important aspects of computer systems are memory and processing. Memory keeps working information and is usually implemented in a hierarchy: small level-one caches holding the most active data, followed by larger caches, then even larger main memory, and ultimately permanent storage (disks, FLASH). Processing has been traditionally done with central processing units (CPUs), but it can be done more energy-efficiently with graphics processing units (GPUs).\n\nThis project worked to improve computer system energy-efficiency with many contributions that can be organized in the three.\n\nImproved Cache Compression. Cache compression seeks to losslessly reduce the size of commonly-stored data so that data is more often found in a cache (a hit) rather than not (a miss). Misses cost time and energy. One result of this project is a cache design that ingeniously has cache data point back to address information (tags) to facilitate fast hits to data compressed by varying amounts. A follow-on result uses skewing to reduce address tag overhead and improve performance and energy.\n\nEfficient Virtual Memory. Virtual memory is a technique for caching data in main memory from a vast storage system. Data in virtual memory is found using a translation lookaside buffer (TLB) that&mdash;like a cache&mdash;can hit or miss. TLB miss frequencies are increasing for especially (cloud) server workloads where the amount of main memory that can be purchased for $10K has increased a million-fold (from megabytes to terabytes) while TLB sizes have stayed relatively fixed. A result from this grant is a design with an arbitrarily-large ``direct segment\u00c6\u00c6 that can eliminate TLB misses from a large fraction of memory, saving time and energy. This result was also generalized to support virtualized systems and to enable many direct segments.\n\nSaving Energy with Graphics Processing Units. GPUs offer an energy efficient alternative to CPUs for executing some general-purpose workloads. GPUs can be hard to program and can interact poorly with memory hierarchies. A result is to provide programmer-friendly virtual memory for GPUs by adapting CPU TLB methods including proposing hardware that can process dozens of concurrent TLB misses. This is useful for handling bursts of misses that analysis found common for GPUs. Another result enables GPU programs to benefit from the synergy of two heretofore-incompatible alternatives: static hierarchical GPU synchronization and dynamic load balancing (e.g., via work stealing).\n\nMoreover, the project provided broader impacts several ways. To help NSF dollars go further, it developed and refined open-simulation infrastructures that have been used by 1000s of students and researchers (gem5, gem5-gpu, BadgerTrap). This grant has provided partial support for 6 different Ph.D. students. It has promulgated the work through an annual computer architecture affiliates meetings, and disseminated ideas and simulators in courses (Wisconsin and elsewhere) and via external presentations by faculty and students. Through student internships and graduating students joining the computer industry, ultimate outcome of work funded by this grant is not yet bounded.\n\n\t\t\t\t\tLast Modified: 07/29/2015\n\n\t\t\t\t\tSubmitted by: Mark D Hill"
 }
}