{
 "awd_id": "1212798",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Large: Collaborative Research: Reconstructive recognition: Uniting statistical scene understanding and physics-based visual reasoning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 818181.0,
 "awd_amount": 818181.0,
 "awd_min_amd_letter_date": "2012-09-12",
 "awd_max_amd_letter_date": "2012-09-12",
 "awd_abstract_narration": "This project is creating a novel paradigm for computer vision, termed \"reconstructive recognition\", that incorporates the strongest elements of previous machine learning-based recognition efforts and the strongest elements of previous reconstruction efforts based on radiometric reasoning. The goal is to provide a new foundation for machine perception, and the potential for a transformative advance in applications of computer vision. The project seeks novel physics-based methods for recognition as well as novel learning-based methods for interpreting pixel values in terms of the physics of a scene. The agenda is structured around four aims: Aim I develops generalized reconstructive processes that unify the recovery of shape, materials, motion and illumination. Aim II focuses on supervised visual learning methods that exploit such reconstructive image representations. Aim III pursues unsupervised discovery of reconstructive representations that converge to be similar to the engineered models of Aim I. Finally, Aim IV introduces well-defined challenge problems that focus the field and serve as measurable proxies for progress in computer vision applications that have high potential impact on society. \r\n\r\nThere is a significant broader impact to this project, not least being the improvement in computer vision pedagogy that ensues from a reunification of the currently divergent recognition and reconstruction views of the field. More broadly, this project pursues critical steps toward a future where machines can see, a future that will bring changes to robotics, human-computer interfaces, security, and autonomous navigation, to name a few.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Trevor",
   "pi_last_name": "Darrell",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Trevor J Darrell",
   "pi_email_addr": "trevor@eecs.berkeley.edu",
   "nsf_id": "000175078",
   "pi_start_date": "2012-09-12",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jitendra",
   "pi_last_name": "Malik",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jitendra Malik",
   "pi_email_addr": "malik@cs.berkeley.edu",
   "nsf_id": "000447614",
   "pi_start_date": "2012-09-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "2150 Shattuck Ave. Suite 300",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 818181.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"m_-5855498894022896694p1\">A long-term goal of computer vision is to create artificial vision systems that can perform the same visual tasks as humans, using comparable amounts of training data (i.e. comparable amounts of prior visual experience), and using comparable amounts of computational resources.</p>\n<p class=\"m_-5855498894022896694p2\">In order to succeed, these artificial vision systems will need to combine two modes of reasoning: recognition and reconstruction. Reconstruction is the process by which the laws of physics are used to \"rule out\" the impossible explanations for an input image, and to concisely represent the explanations that are feasible. Recognition is the statistical process by which prior visual experience is used to identify the most probable explanations for an image based on other images that have previously been explained.</p>\n<p class=\"m_-5855498894022896694p2\">This project helped to establish a new paradigm for research in computer vision, one in which the best of previous recognition approaches based on machine learning are combined with the best of previous reconstruction approaches based on the mathematical modeling of optics, geometry, and radiometry. We call this new paradigm \"reconstructive recognition.\" The outcomes of the project span two directions.</p>\n<p class=\"m_-5855498894022896694p2\">1. Unifying reconstructive processes</p>\n<p class=\"m_-5855498894022896694p2\">There are many types of visual cues that provide constraints on the feasible set of explanations for an image. Color variations, shading, glossy highlights, shadows, and texture variations are all visual cues that provide complementary constraints on the 3D shapes and materials that can explain an image. During the award period, we developed unifying computational techniques for combining these complimentary cues, and we discovered some new cues that were previously unknown. The publications related to these topics include: Xiong et al., 2015; Chakrabarti et al., 2015; Guo et al., ICCV 2017; and Wang et al. ICCV 2017.&nbsp;</p>\n<p class=\"m_-5855498894022896694p2\">2. Supervised and unsupervised processes for reconstructive recognition</p>\n<p class=\"m_-5855498894022896694p1\">We developed two categories of visual learning methods that can learn from image collections the ability to identify the most probable explanations for a novel image while exploiting the constraints and representations that result from reconstructive reasoning.</p>\n<p class=\"m_-5855498894022896694p2\">The first category consists of supervised methods, where each image in the collection is associated with one or more explanatory annotations, such as the type of object in the image, or the object's position, orientation, or 3D shape. In the case of captured photographs the explanatory annotations can be provided by humans, and in the case of synthetically-rendered images, they can be provided by the rendering engine that produces the images. The publications that relate to supervised methods are: Xu and Saenko ECCV'2016; Hu et al. CVPR'17; Venugopalan et al.CVPR'17; Ramanishka et al. CVPR'2017; and Petsiuk et al. BMVC'2018.</p>\n<p class=\"m_-5855498894022896694p2\">The second category is unsupervised methods, where the available image collections are not associated with any explanatory annotations. In this case, the visual learning system must&nbsp; discover the explanations by automatically discovering the repetative patterns and structures that naturally exist within the collections. One particular class of approaches that we developed are those for \"domain adaptation\", where a visual learning system has already mastered some tasks and must then quickly adapt that experience to a new task for which annotated examples images are not available. The publications that relate to unsupervised methods are: Sun and Saenko BMVC'14; Sun and Saenko BMVC'15; and Sun et al. AAAI'2015.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/30/2019<br>\n\t\t\t\t\tModified by: Trevor&nbsp;J&nbsp;Darrell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "A long-term goal of computer vision is to create artificial vision systems that can perform the same visual tasks as humans, using comparable amounts of training data (i.e. comparable amounts of prior visual experience), and using comparable amounts of computational resources.\nIn order to succeed, these artificial vision systems will need to combine two modes of reasoning: recognition and reconstruction. Reconstruction is the process by which the laws of physics are used to \"rule out\" the impossible explanations for an input image, and to concisely represent the explanations that are feasible. Recognition is the statistical process by which prior visual experience is used to identify the most probable explanations for an image based on other images that have previously been explained.\nThis project helped to establish a new paradigm for research in computer vision, one in which the best of previous recognition approaches based on machine learning are combined with the best of previous reconstruction approaches based on the mathematical modeling of optics, geometry, and radiometry. We call this new paradigm \"reconstructive recognition.\" The outcomes of the project span two directions.\n1. Unifying reconstructive processes\nThere are many types of visual cues that provide constraints on the feasible set of explanations for an image. Color variations, shading, glossy highlights, shadows, and texture variations are all visual cues that provide complementary constraints on the 3D shapes and materials that can explain an image. During the award period, we developed unifying computational techniques for combining these complimentary cues, and we discovered some new cues that were previously unknown. The publications related to these topics include: Xiong et al., 2015; Chakrabarti et al., 2015; Guo et al., ICCV 2017; and Wang et al. ICCV 2017. \n2. Supervised and unsupervised processes for reconstructive recognition\nWe developed two categories of visual learning methods that can learn from image collections the ability to identify the most probable explanations for a novel image while exploiting the constraints and representations that result from reconstructive reasoning.\nThe first category consists of supervised methods, where each image in the collection is associated with one or more explanatory annotations, such as the type of object in the image, or the object's position, orientation, or 3D shape. In the case of captured photographs the explanatory annotations can be provided by humans, and in the case of synthetically-rendered images, they can be provided by the rendering engine that produces the images. The publications that relate to supervised methods are: Xu and Saenko ECCV'2016; Hu et al. CVPR'17; Venugopalan et al.CVPR'17; Ramanishka et al. CVPR'2017; and Petsiuk et al. BMVC'2018.\nThe second category is unsupervised methods, where the available image collections are not associated with any explanatory annotations. In this case, the visual learning system must  discover the explanations by automatically discovering the repetative patterns and structures that naturally exist within the collections. One particular class of approaches that we developed are those for \"domain adaptation\", where a visual learning system has already mastered some tasks and must then quickly adapt that experience to a new task for which annotated examples images are not available. The publications that relate to unsupervised methods are: Sun and Saenko BMVC'14; Sun and Saenko BMVC'15; and Sun et al. AAAI'2015.\n\n \n\n\t\t\t\t\tLast Modified: 06/30/2019\n\n\t\t\t\t\tSubmitted by: Trevor J Darrell"
 }
}