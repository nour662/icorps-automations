{
 "awd_id": "1213057",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF:Large:Collaborative Research:Unified Runtime for Supporting Hybrid Programming Models on Heterogeneous Architecture",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2016-06-30",
 "tot_intn_awd_amt": 371903.0,
 "awd_amount": 371903.0,
 "awd_min_amd_letter_date": "2012-05-16",
 "awd_max_amd_letter_date": "2014-04-07",
 "awd_abstract_narration": "Most of the traditional High-End Computing (HEC) applications and\r\ncurrent petascale applications are written using the Message Passing\r\nInterface (MPI) programming model. Some of these applications are run\r\nin MPI+OpenMP mode.  However, it can be very difficult to use MPI or\r\nMPI+OpenMP and maintain performance for applications which demonstrate\r\nirregular and dynamic communication patterns.  The Partitioned Global\r\nAddress Space (PGAS) programming model presents a flexible way for\r\nthese applications to express parallelism.  Accelerators introduce\r\nadditional programming models: CUDA, OpenCL or OpenACC.  Thus, the\r\nemerging heterogeneous architectures require support for various\r\nhybrid programming models: MPI+OpenMP, MPI+PGAS, and MPI+PGAS+OpenMP\r\nwith extended APIs for multiple levels of parallelism.  Unfortunately,\r\nthere is no unified runtime which delivers the best performance and\r\nscalability for all of these hybrid programming models for a range of\r\napplications on current and next-generation HEC systems.  This leads\r\nto the following broad challenge: \"Can a unified runtime for hybrid\r\nprogramming model be designed which can provide benefits that are\r\ngreater than the sum of its parts?\"\r\n\r\nA synergistic and comprehensive research plan, involving computer\r\nscientists from The Ohio State University (OSU) and Ohio Supercomputer\r\nCenter (OSC) and computational scientists from the Texas Advanced\r\nComputing Center (TACC) and San Diego Supercomputer Center (SDSC),\r\nUniversity of California San Diego (UCSD), is proposed to address the\r\nabove broad challenge with innovative solutions.  The investigators\r\nwill specifically address the following challenges: 1) What are the\r\nrequirements and limitations of using hybrid programming models for a\r\nset of petascale applications?  2) What features and mechanisms are\r\nneeded in a unified runtime?  3) How can the unified runtime and\r\nassociated extension to programming model APIs be designed and\r\nimplemented?  4) How can candidate petascale applications be\r\nredesigned to take advantage of proposed unified runtime?  and 5) What\r\nkind of benefits (in terms of performance, scalability and\r\nproductivity) can be achieved by the proposed approach?  The research\r\nwill be driven by a set of applications from established NSF\r\ncomputational science researchers running large scale simulations on\r\nRanger and other systems at OSC, SDSC and OSU.  The proposed designs\r\nwill be integrated into the open-source MVAPICH2 library.  The\r\nestablished national-scale training and outreach programs at TACC,\r\nSDSC and OSC will be used to disseminate the results of this research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "Karl",
   "pi_last_name": "Schulz",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Karl W Schulz",
   "pi_email_addr": "karl@ices.utexas.edu",
   "nsf_id": "000370250",
   "pi_start_date": "2012-05-16",
   "pi_end_date": "2014-04-07"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Barth",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "William L Barth",
   "pi_email_addr": "bbarth@tacc.utexas.edu",
   "nsf_id": "000596862",
   "pi_start_date": "2014-04-07",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Barth",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "William L Barth",
   "pi_email_addr": "bbarth@tacc.utexas.edu",
   "nsf_id": "000596862",
   "pi_start_date": "2012-05-16",
   "pi_end_date": "2014-04-07"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jerome",
   "pi_last_name": "Vienne",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Jerome F Vienne",
   "pi_email_addr": "viennej@tacc.utexas.edu",
   "nsf_id": "000658593",
   "pi_start_date": "2014-04-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "PO Box 7726",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787137726",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 371903.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The Message Passing Interface (MPI) has been the most popular<br />programming model for developing parallel scientific applications on<br />modern High-End Computing (HEC) systems. A hybrid model combining MPI<br />with the shared-memory OpenMP model is also widely used (referred to<br />as MPI+OpenMP), where typically a single MPI process is used per<br />compute server or socket with OpenMP \"threads\" running on each CPU<br />core in the server/socket. On the other hand, Partitioned Global<br />Address Space (PGAS) programming models are an attractive alternative<br />for designing applications with irregular communication patterns. They<br />improve programmability by providing a shared memory abstraction<br />globally while exposing data placement control required for<br />performance. It is widely believed that a hybrid programming model<br />(MPI+X, where X is a PGAS model) is optimal for many scientific<br />computing problems, especially for exascale computing. Specialize<br />computing devices, such as NVIDIA GPUs and Intel Many Integrated Cores<br />(MIC) introduce additional programming models such as CUDA, OpenCL or<br />OpenACC to express parallelism within them. Thus, the emerging<br />heterogeneous architectures for HEC systems require support for a range<br />of hybrid programming models: MPI+OpenMP, MPI+PGAS, and<br />MPI+PGAS+OpenMP with extended APIs for multiple levels of parallelism.<br /><br />Unfortunately, there is no unified runtime layer to orchestrate the<br />communication and synchronization operations for these models which<br />delivers the best performance and scalability for a range of<br />applications on current and next-generation HEC systems. This leads to<br />the following broad challenge: \"Can a unified runtime for hybrid<br />programming model be designed which can provide benefits that are<br />greater than the sum of its parts?\"<br /><br />In this project we have addressed this challenge in five areas: 1)<br />Developed a unified runtime supporting MPI, PGAS (OpenSHMEM, UPC,<br />UPC++ and CAF), Hybrid MPI+PGAS, and MPI+CUDA; 2) Designed efficient<br />algorithms for one-sided and collective communications for PGAS<br />models; 3) Developed high-performance GPU-aware and MIC-aware<br />communications algorithms which leverage latest hardware features; 4)<br />Co-Designed MPI+PGAS and MPI+CUDA applications and demonstrated<br />performance advantages; and 5) Developed benchmarks for new hybrid<br />models.<br /><br />Contributions were made in all five areas and evaluated with a range<br />of science applications including - Graph500, CNTK Deep Learning<br />Framework, Mizan Graph Processing Framework, COSMO Weather prediction,<br />AWP-ODC Seismic modeling, and an Out-of-Core Sort. Some highlights of<br />these results are:<br /><br />* An optimized Graph500 design using hybrid MPI+OpenSHMEM model demonstrated<br />a 13X factor of improvement in comparison with the pure MPI version <br />on 16,000 cores.<br /><br />* An Out-of-Core Sort design using hybrid MPI+OpenSHMEM models reduced<br />the runtime by 45% on 8,192 processes compared with the existing<br />design and an improvement of 7X on 1,024 cores in comparison with<br />Hadoop.<br /><br />* A Scalable GPU-Aware implementation of the CNTK Deep Learning<br />framework that runs 10-18% faster than the prior implementation on<br />benchmark datasets.<br /><br />* A new MPI memory registration approach in the unified runtime which<br />reduces the amount of pinned memory required by the library by as much<br />as eleven-fold.<br /><br />* GPU-aware halo-data exchange and new GPU-kernel supported<br />collective communication algorithms reduce communication operations<br />for the COSMO Weather prediction application by 20% and 30%, <br />respectively.<br /><br />The results of this research (new designs, performance results,<br />benchmarks, etc.) have been made available to the community through<br />MVAPICH2, MVAPICH2-X and MVAPICH2-GDR libraries (1.9, 2.0, 2.1, and<br />2.2 release series including alpha, beta, RC and GA versions). The latest<br />versions of these libraries are currently running on many large-scale<br />XSEDE systems including TACC Stampede and SDSC Comet. Currently,the<br />MVAPICH2 libraries are being used by more than 2,675 organizations in<br />83 countries. The MVAPICH2 libraries and the associated enhancements<br />are being used by a large number of users of these systems.<br /><br />In each of these releases, information about the tuned designs for<br />various components (such as point-to-point, collectives, GPU-GPU<br />communication, etc.) has been shared with the MVAPICH2 user community<br />through mailing lists. The applications-based tuning results have been<br />made available to the community through the \"Best Practices\" link of<br />the MVAPICH project web page. In order to achieve direct face-to-face<br />discussion with MVAPICH2 users and get their feedback, in 2013 we<br />started holding an MVAPICH2 User Group Meeting (MUG) each year in<br />August in Columbus, Ohio. This meeting has been continuing successfully<br />for the last four years and has helped to disseminate the results of<br />this research to a wider community.&nbsp; In addition to the software<br />distribution and the MUG events, the results have been presented at<br />various conferences and events through talks and tutorials.&nbsp; Multiple<br />Ph.D and Masters students have performed research work and received<br />their Ph.D and M.S. degrees as a part of this project.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/12/2016<br>\n\t\t\t\t\tModified by: William&nbsp;L&nbsp;Barth</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe Message Passing Interface (MPI) has been the most popular\nprogramming model for developing parallel scientific applications on\nmodern High-End Computing (HEC) systems. A hybrid model combining MPI\nwith the shared-memory OpenMP model is also widely used (referred to\nas MPI+OpenMP), where typically a single MPI process is used per\ncompute server or socket with OpenMP \"threads\" running on each CPU\ncore in the server/socket. On the other hand, Partitioned Global\nAddress Space (PGAS) programming models are an attractive alternative\nfor designing applications with irregular communication patterns. They\nimprove programmability by providing a shared memory abstraction\nglobally while exposing data placement control required for\nperformance. It is widely believed that a hybrid programming model\n(MPI+X, where X is a PGAS model) is optimal for many scientific\ncomputing problems, especially for exascale computing. Specialize\ncomputing devices, such as NVIDIA GPUs and Intel Many Integrated Cores\n(MIC) introduce additional programming models such as CUDA, OpenCL or\nOpenACC to express parallelism within them. Thus, the emerging\nheterogeneous architectures for HEC systems require support for a range\nof hybrid programming models: MPI+OpenMP, MPI+PGAS, and\nMPI+PGAS+OpenMP with extended APIs for multiple levels of parallelism.\n\nUnfortunately, there is no unified runtime layer to orchestrate the\ncommunication and synchronization operations for these models which\ndelivers the best performance and scalability for a range of\napplications on current and next-generation HEC systems. This leads to\nthe following broad challenge: \"Can a unified runtime for hybrid\nprogramming model be designed which can provide benefits that are\ngreater than the sum of its parts?\"\n\nIn this project we have addressed this challenge in five areas: 1)\nDeveloped a unified runtime supporting MPI, PGAS (OpenSHMEM, UPC,\nUPC++ and CAF), Hybrid MPI+PGAS, and MPI+CUDA; 2) Designed efficient\nalgorithms for one-sided and collective communications for PGAS\nmodels; 3) Developed high-performance GPU-aware and MIC-aware\ncommunications algorithms which leverage latest hardware features; 4)\nCo-Designed MPI+PGAS and MPI+CUDA applications and demonstrated\nperformance advantages; and 5) Developed benchmarks for new hybrid\nmodels.\n\nContributions were made in all five areas and evaluated with a range\nof science applications including - Graph500, CNTK Deep Learning\nFramework, Mizan Graph Processing Framework, COSMO Weather prediction,\nAWP-ODC Seismic modeling, and an Out-of-Core Sort. Some highlights of\nthese results are:\n\n* An optimized Graph500 design using hybrid MPI+OpenSHMEM model demonstrated\na 13X factor of improvement in comparison with the pure MPI version \non 16,000 cores.\n\n* An Out-of-Core Sort design using hybrid MPI+OpenSHMEM models reduced\nthe runtime by 45% on 8,192 processes compared with the existing\ndesign and an improvement of 7X on 1,024 cores in comparison with\nHadoop.\n\n* A Scalable GPU-Aware implementation of the CNTK Deep Learning\nframework that runs 10-18% faster than the prior implementation on\nbenchmark datasets.\n\n* A new MPI memory registration approach in the unified runtime which\nreduces the amount of pinned memory required by the library by as much\nas eleven-fold.\n\n* GPU-aware halo-data exchange and new GPU-kernel supported\ncollective communication algorithms reduce communication operations\nfor the COSMO Weather prediction application by 20% and 30%, \nrespectively.\n\nThe results of this research (new designs, performance results,\nbenchmarks, etc.) have been made available to the community through\nMVAPICH2, MVAPICH2-X and MVAPICH2-GDR libraries (1.9, 2.0, 2.1, and\n2.2 release series including alpha, beta, RC and GA versions). The latest\nversions of these libraries are currently running on many large-scale\nXSEDE systems including TACC Stampede and SDSC Comet. Currently,the\nMVAPICH2 libraries are being used by more than 2,675 organizations in\n83 countries. The MVAPICH2 libraries and the associated enhancements\nare being used by a large number of users of these systems.\n\nIn each of these releases, information about the tuned designs for\nvarious components (such as point-to-point, collectives, GPU-GPU\ncommunication, etc.) has been shared with the MVAPICH2 user community\nthrough mailing lists. The applications-based tuning results have been\nmade available to the community through the \"Best Practices\" link of\nthe MVAPICH project web page. In order to achieve direct face-to-face\ndiscussion with MVAPICH2 users and get their feedback, in 2013 we\nstarted holding an MVAPICH2 User Group Meeting (MUG) each year in\nAugust in Columbus, Ohio. This meeting has been continuing successfully\nfor the last four years and has helped to disseminate the results of\nthis research to a wider community.  In addition to the software\ndistribution and the MUG events, the results have been presented at\nvarious conferences and events through talks and tutorials.  Multiple\nPh.D and Masters students have performed research work and received\ntheir Ph.D and M.S. degrees as a part of this project.\n\n\t\t\t\t\tLast Modified: 10/12/2016\n\n\t\t\t\t\tSubmitted by: William L Barth"
 }
}