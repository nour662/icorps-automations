{
 "awd_id": "1149432",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CAREER: Programming Environments and Runtime for Data Enabled Science",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Sushil K Prasad",
 "awd_eff_date": "2012-03-01",
 "awd_exp_date": "2018-02-28",
 "tot_intn_awd_amt": 499994.0,
 "awd_amount": 499994.0,
 "awd_min_amd_letter_date": "2012-02-23",
 "awd_max_amd_letter_date": "2012-02-23",
 "awd_abstract_narration": "This research is at the nexus of the data deluge in science and business and two major computing thrusts - clouds and exascale scientific systems which are unified with an interoperable runtime system. The project has the potential to transform the approach to applications that varies from data mining of genomic and proteomic data for science to data analytics for business. Computer science areas at the heart of the research - namely Iterative Map Collective runtime, fault tolerance, data-computing co-location and high level languages - will be advanced. Furthermore, the new applications enabled and new software paradigms will feed back into the architecture of cloud and exascale systems possibly suggesting particular storage and communication choices and new directions for the national infrastructure. The investigator will incorporate this novel research into courses and graduate and undergraduate research experiences at both Indiana University and with national and international collaborators. The work blends scientific research (computer science and applications) with mainstream commercial practice (clouds). Thus, curricula built around this research will motivate and inspire the entry of students into the workforce and so it has potential for supporting needed economic development.\r\n\r\nThe research is based on initial research on Iterative MapReduce with successful prototypes Twister (on HPC) and Twister4Azure (on clouds). The project will architect and prototype a Discovery Environments for Data-Enabled Science and Engineering with the following components developed: (1) a next generation Iterative MapReduce using a Map-Collective model as the runtime for data analysis (mining) interoperably between clouds and clusters; (2) polymorphic collective operations needed to support parallel linear algebra and other data analysis operations such as those in MapReduce; (3) a software message routing using publish-subscribe to scale to tens of thousands of nodes or above; (4) a storage model that builds on current object stores, data parallel file systems (as in Hadoop), and wide area models like Lustre but respects compute-data co-location; (5) a fault tolerance model implemented as a Collective operation with configurable settings that supports checkpointing between iterations for robustness and individual node failure without compromising performance. Later research objectives include security and a higher-level programming model that compiles to an iterative MapReduce runtime.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Judy",
   "pi_last_name": "Fox",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Judy Fox",
   "pi_email_addr": "ckw9mp@virginia.edu",
   "nsf_id": "000555330",
   "pi_start_date": "2012-02-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Indiana University",
  "inst_street_address": "107 S INDIANA AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BLOOMINGTON",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "3172783473",
  "inst_zip_code": "474057000",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IN09",
  "org_lgl_bus_name": "TRUSTEES OF INDIANA UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "YH86RTW2YVJ4"
 },
 "perf_inst": {
  "perf_inst_name": "Indiana University",
  "perf_str_addr": "919 E 10th Street",
  "perf_city_name": "Bloomington",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "474083912",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IN09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 499994.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project of Data-Enabled Discovery Environments for Science and Engineering (DEDESE) performs fundamental computer science research in Innovative Cloud-HPC Programming Models and Data Intensive Applications for bioinformatics, computer vision, network science and scientific simulations.</p>\n<p>Programming models and tools are one point of divergence between the scientific computing and big data ecosystems. Maturing of Cloud software around the Apache Big Data Stack (ABDS) has gained striking community support while there is continued progress in HPC spanning up to exascale. Analysis of Big Data use cases identifies the need for HPC technologies in ABDS. Deep learning, using GPU clusters, is a clear example. But many machine learning algorithms also need iterative computation, high-performance communication and other HPC optimizations such as node level kernels and emerging hardware architectures. This rapid change in technology has further implications for research and education.</p>\n<p>There is increasing interest in using HPC machines for Big Data problems. By integrating topics in a data-centric approach, one can address research issues both in the data systems and in their integration with programming models, runtimes (e.g. Twister Iterative MapReduce), and scalable storage (e.g. IndexedHBase). The project shows promising results from reusing HPC-ABDS to enhance three well-known Apache systems (Hadoop, Storm, and HBase) and construct DEDESE. The architecture is based on using Map-Collective and Map-Streaming computation models for an integrated solution to handle large data size, complexity, and speed. This is illustrated by Harp (a Hadoop plug-in) that offers both data abstractions useful for high-performance iterative computation and MPI-quality communication. Harp can run K-means, Force-directed Graph Drawing, and Multidimensional Scaling algorithms with realistic application datasets over 4096 cores on the IU Big Red II Supercomputer and Intel&rsquo;s Xeon architectures while achieving linear speedup.</p>\n<p>A major challenge of scaling for machine learning is owing to the fact that computation is irregular and the model size can be huge. At the meantime, parallel workers need to synchronize the model continually. This project targets hard problems with &ldquo;Big Model&rdquo; machine learning and investigates different parallel patterns (kernels) of machine learning applications. It further classifies parallel iterative algorithms into four types of computation models (a) locking, (b) rotation, (c) allreduce, (d) asynchronous, based on the synchronization patterns and the effectiveness of the model parameter update. A novel HPC-Cloud convergence framework named Harp-DAAL is introduced to demonstrate that the combination of Big Data (Hadoop) and HPC techniques can simultaneously achieve productivity and performance. Harp-DAAL shows how simulations and Big Data can use&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/31/2018<br>\n\t\t\t\t\tModified by: Judy&nbsp;Qiu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project of Data-Enabled Discovery Environments for Science and Engineering (DEDESE) performs fundamental computer science research in Innovative Cloud-HPC Programming Models and Data Intensive Applications for bioinformatics, computer vision, network science and scientific simulations.\n\nProgramming models and tools are one point of divergence between the scientific computing and big data ecosystems. Maturing of Cloud software around the Apache Big Data Stack (ABDS) has gained striking community support while there is continued progress in HPC spanning up to exascale. Analysis of Big Data use cases identifies the need for HPC technologies in ABDS. Deep learning, using GPU clusters, is a clear example. But many machine learning algorithms also need iterative computation, high-performance communication and other HPC optimizations such as node level kernels and emerging hardware architectures. This rapid change in technology has further implications for research and education.\n\nThere is increasing interest in using HPC machines for Big Data problems. By integrating topics in a data-centric approach, one can address research issues both in the data systems and in their integration with programming models, runtimes (e.g. Twister Iterative MapReduce), and scalable storage (e.g. IndexedHBase). The project shows promising results from reusing HPC-ABDS to enhance three well-known Apache systems (Hadoop, Storm, and HBase) and construct DEDESE. The architecture is based on using Map-Collective and Map-Streaming computation models for an integrated solution to handle large data size, complexity, and speed. This is illustrated by Harp (a Hadoop plug-in) that offers both data abstractions useful for high-performance iterative computation and MPI-quality communication. Harp can run K-means, Force-directed Graph Drawing, and Multidimensional Scaling algorithms with realistic application datasets over 4096 cores on the IU Big Red II Supercomputer and Intel?s Xeon architectures while achieving linear speedup.\n\nA major challenge of scaling for machine learning is owing to the fact that computation is irregular and the model size can be huge. At the meantime, parallel workers need to synchronize the model continually. This project targets hard problems with \"Big Model\" machine learning and investigates different parallel patterns (kernels) of machine learning applications. It further classifies parallel iterative algorithms into four types of computation models (a) locking, (b) rotation, (c) allreduce, (d) asynchronous, based on the synchronization patterns and the effectiveness of the model parameter update. A novel HPC-Cloud convergence framework named Harp-DAAL is introduced to demonstrate that the combination of Big Data (Hadoop) and HPC techniques can simultaneously achieve productivity and performance. Harp-DAAL shows how simulations and Big Data can use \n\n \n\n\t\t\t\t\tLast Modified: 05/31/2018\n\n\t\t\t\t\tSubmitted by: Judy Qiu"
 }
}