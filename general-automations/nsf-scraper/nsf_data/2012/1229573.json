{
 "awd_id": "1229573",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MRI: Acquisition of a Shared Parallel High Performance Storage System to Enable Computational Science and Engineering",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rita Rodriguez",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 525000.0,
 "awd_amount": 525000.0,
 "awd_min_amd_letter_date": "2012-09-17",
 "awd_max_amd_letter_date": "2012-09-17",
 "awd_abstract_narration": "Proposal #:\t12-29573\r\nPI(s):\t\tHillegas, Curtis W. \r\n\t\tCarter, Emily A.; Pretorius, Frans; Spitkovsky, Anatoly; Wood, Eric F.\r\nInstitution:\tPrinceton University\r\nTitle: MRI/Acq.: Shared Parallel High Performance Storage System to Enable Computational Science and Engineering\r\nProject Proposed:\r\nThis project, acquiring a High Performance Computing (HPC) storage system, aims to provide storage and I/O bandwidth required to enable advancement of research to new, previously unattainable areas. Specifically, in astrophysics the storage will allow an increase in dimensionality to perform full 3D modeling of shock formation; in civil and environmental engineering, it will enable the analysis of higher resolution water condition data in both time and space; in mechanical and aerospace engineering, the instrument will facilitate rigorous physical modeling of rate constants for biofuel combustion and fundamental understanding of molecular adsorptions; and in physics it will enable the modeling of gravitational waves and compact object mergers considering a broader range of physics. More generally, the system will enable projects across many disciplines in computational science and engineering.\r\nSince data storage and access have become a bottleneck hampering researchers in the TIGRESS systems, the proposed system should contribute to remove the bottleneck. Understanding that data growth will continue, a modular storage system design has been chosen that will allow the system to grow in capacity and performance as the data deluge continuous to mount. Planned is the purchase of a 1.5 PB storage system based on hardware from NetApp, integrated with servers from SGI by Comnetco. The system will run IBMs General Parallel File System (GPFS) and provide 12 GB/s parallel performance across the institution?s TIGRESS HPC systems.\r\nBroader Impacts: \r\nThe instrument will facilitate collaboration by making it easy for researchers to share data within the institution; furthermore, it will foster collaboration nationally and internationally through the included web server facility by allowing researchers to broadly share their data. As a research tool available to postdocs, graduate students, and advanced undergraduate students, including many researchers underrepresented minority groups, the instrument will serve as a training platform, teaching data layout, management and performance optimization.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Curtis",
   "pi_last_name": "Hillegas",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Curtis W Hillegas",
   "pi_email_addr": "curt@princeton.edu",
   "nsf_id": "000058896",
   "pi_start_date": "2012-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Eric",
   "pi_last_name": "Wood",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Eric F Wood",
   "pi_email_addr": "efw227@gmail.com",
   "nsf_id": "000101567",
   "pi_start_date": "2012-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Emily",
   "pi_last_name": "Carter",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Emily A Carter",
   "pi_email_addr": "eac@princeton.edu",
   "nsf_id": "000438494",
   "pi_start_date": "2012-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Frans",
   "pi_last_name": "Pretorius",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frans Pretorius",
   "pi_email_addr": "fpretori@princeton.edu",
   "nsf_id": "000436941",
   "pi_start_date": "2012-09-17",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Anatoly",
   "pi_last_name": "Spitkovsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Anatoly Spitkovsky",
   "pi_email_addr": "anatoly@princeton.edu",
   "nsf_id": "000071074",
   "pi_start_date": "2012-09-17",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "4 New South Building",
  "perf_city_name": "Princeton",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1189",
   "pgm_ref_txt": "MAJOR RESEARCH INSTRUMENTATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 525000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This grant was used to acquire a shared parallel high performance storage system.&nbsp; The system, which was purchased from IBM and Go-Compute provides 2.6 petabytes of usable storage with an aggregate performance of 14 gigabytes per second.&nbsp; All of Princeton University&rsquo;s major central High Performance Computing (HPC) systems have access to the storage system, and it is available for usage by all of the researchers who have accounts on those HPC systems (which any member of the university can obtain through a simple proposal process). &nbsp;At the end of the grant period, the storage system contains 2.0 petabytes of research data stoared in 466 million files owned by over 1000 individual researchers from a broad range of academic departments at the university.&nbsp; Research topics enabled by the system are as varied as Astrophysics, Mechanical and Aerospace Engineering, Physics, Civil and Environmental Engineering, Chemistry, Genomics, Neuroscience, Politics, Geosciences, Atmospheric and Oceanographic Sciences, and many others.</p>\n<p>Specifically, for Co-I Carter&rsquo;s research, the new Tigress storage system has enabled and accelerated research carried out to evaluate electron correlation in molecules and condensed matter. For Co-I Wood&rsquo;s research, the storage system has enabled creation of a consistent surface temperature dataset from merging multi-satellite observations with land surface model estimates; seasonal and sub-seasonal hydrologic forecasting; and a number of other projects in the area of terrestrial hydrology. For Co-I Pretorius&rsquo; research, the storage system has enabled investigations related to eccentric black hole/neutron star and binary neutron star systems.&nbsp; For Co-I Spitkovsky&rsquo;s research, the storage system has enabled advancements in ab-initio particle-in-cell simulations of particle acceleration in astrophysical shock waves.</p>\n<p>Over the lifetime of this grant there has been significant broadening of the number of departments and disciplines using HPC in their research as well as a significant deepening of such usage in the departments and disciplines that have traditionally relied on HPC. &nbsp;The storage system acquired with this grant has been critical to enabling research in all of these areas. This instrument has allowed the undergraduate students, graduate students, postdoctoral fellows, research staff, and faculty to focus on their science and research.&nbsp; The scale, performance, reliability, and robustness of this instrument have made it easier and faster for researchers to create, manage, and analyze their datam and it has enable projects that would not otherwise have been possible resulting in advancement in many areas of science and engineering.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/30/2015<br>\n\t\t\t\t\tModified by: Curtis&nbsp;W&nbsp;Hillegas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis grant was used to acquire a shared parallel high performance storage system.  The system, which was purchased from IBM and Go-Compute provides 2.6 petabytes of usable storage with an aggregate performance of 14 gigabytes per second.  All of Princeton University\u00c6s major central High Performance Computing (HPC) systems have access to the storage system, and it is available for usage by all of the researchers who have accounts on those HPC systems (which any member of the university can obtain through a simple proposal process).  At the end of the grant period, the storage system contains 2.0 petabytes of research data stoared in 466 million files owned by over 1000 individual researchers from a broad range of academic departments at the university.  Research topics enabled by the system are as varied as Astrophysics, Mechanical and Aerospace Engineering, Physics, Civil and Environmental Engineering, Chemistry, Genomics, Neuroscience, Politics, Geosciences, Atmospheric and Oceanographic Sciences, and many others.\n\nSpecifically, for Co-I Carter\u00c6s research, the new Tigress storage system has enabled and accelerated research carried out to evaluate electron correlation in molecules and condensed matter. For Co-I Wood\u00c6s research, the storage system has enabled creation of a consistent surface temperature dataset from merging multi-satellite observations with land surface model estimates; seasonal and sub-seasonal hydrologic forecasting; and a number of other projects in the area of terrestrial hydrology. For Co-I Pretorius\u00c6 research, the storage system has enabled investigations related to eccentric black hole/neutron star and binary neutron star systems.  For Co-I Spitkovsky\u00c6s research, the storage system has enabled advancements in ab-initio particle-in-cell simulations of particle acceleration in astrophysical shock waves.\n\nOver the lifetime of this grant there has been significant broadening of the number of departments and disciplines using HPC in their research as well as a significant deepening of such usage in the departments and disciplines that have traditionally relied on HPC.  The storage system acquired with this grant has been critical to enabling research in all of these areas. This instrument has allowed the undergraduate students, graduate students, postdoctoral fellows, research staff, and faculty to focus on their science and research.  The scale, performance, reliability, and robustness of this instrument have made it easier and faster for researchers to create, manage, and analyze their datam and it has enable projects that would not otherwise have been possible resulting in advancement in many areas of science and engineering.\n\n \n\n\t\t\t\t\tLast Modified: 12/30/2015\n\n\t\t\t\t\tSubmitted by: Curtis W Hillegas"
 }
}