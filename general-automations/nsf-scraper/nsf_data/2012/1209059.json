{
 "awd_id": "1209059",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Variable Selection, Variable Screening and Dimension Reduction",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2012-08-15",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 140000.0,
 "awd_amount": 140000.0,
 "awd_min_amd_letter_date": "2012-08-13",
 "awd_max_amd_letter_date": "2013-05-22",
 "awd_abstract_narration": "The objectives of this proposal are (1) to enrich the variable selection alternatives by merging ideas from the variable selection and dimension reduction  areas, and (2) develop variable screening procedures using alternative methods of association learning. The specific goals include: a) develop post-dimension reduction model checking procedures, b) develop variable selection procedures using the model checking procedures and  multiple testing ideas, c) develop dimension reduction procedures for very high dimensional data using matrix regularization, d) develop variable screening based on partial correlation learning, e) develop variable screening using nonparametric association learning.\r\n\r\nIn many areas of contemporary research, including gene expression and proteomics studies, biomedical imaging, functional magnetic resonance imaging, tomography, tumor classifications, signal processing, image analysis,  finance, text retrieval and climate studies, the data collected include a large number of variables with only a few of them being relevant for prediction. Procedures for identifying the relevant predictors have mostly been developed under certain model assumptions and may fail to discern the relevance of some important predictors. The proposed research aims at developing variable screening and variable selection procedures which do not rely on potentially restrictive modeling assumptions.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Akritas",
   "pi_mid_init": "G",
   "pi_sufx_name": "",
   "pi_full_name": "Michael G Akritas",
   "pi_email_addr": "mga@stat.psu.edu",
   "nsf_id": "000333149",
   "pi_start_date": "2012-08-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 45558.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 94442.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Prediction of a response variable Y on the basis of available information on a number, p, of predictor variables, X = (X1,...,Xp), is typically based on the regression function, m(X), which expresses the expected value of Y for any given value of X. Estimation of the regression function is the aim of any regression methodology. A growing feature in many areas of contemporary research is that the number p of available predictors is quite high. The high number of predictors implies that prediction models, such as the ubiquitous linear model, include a high number of parameters. This results in reduced model interpretability and in diminished accuracy in model estimation. To deal effectively with the presence of a high number of predictors, statistical research has relied on the principle of sparsity which states that prediction of the response variable can typically be based on only a small number of predictors. Statistical tools for identifying the set of relevant predictors include hypothesis testing and variable selection. In this context, hypothesis testing refers to the problem of testing for the significance of a particular group of predictors, while variable selection is typically addressed through minimization of a constrained or penalized objective function. At a conceptual level, however, the problem of variable selection is intimately connected to hypothesis testing: dropping a variable from the model is equivalent to not declaring that variable significant. This connection was made precise by showing that, in the context of the linear regression model, multiple testing using the false discovery approach can be translated into minimizing a model selection criterion.</p>\n<p>Significance testing and variable selection procedures based on the assumption that the regression function is linear may fail to discern the relevance of covariates whose effect on m(x) is nonlinear. Even if good results are obtained by linear model fitting, they can sometimes be bettered by using an approach that allows nonlinear effects. This statement has been thoroughly demonstrated by simulation studies where the classical approach of linear model fitting fails completely to detect the significance of predictors having a highly nonlinear effect. Research conducted under the present grant has developed test-based variable selection procedures designed to detect all types of effects under a completely general framework.</p>\n<p>For ultra-high dimensional data, i.e., ultra-high number of predictors, the selection of the relevant variables is best achieved through a two-step process. The first stage consists of variable screening, a process designed to eliminate irrelevant variables reducing thus the number of candidate predictors. Once the number of predictors has been reduced, a variable selection method can be applied more efficiently. The original variable screening method was applied to the linear model and thus suffers from the same deficiencies, discussed in the previous paragraphs, as variable selection methods developed for the linear model. Subsequent research considered more general models, but not quite fully nonparametric models. A different class of generalizations of the original variable screening method consists of replacing Pearson&rsquo;s linear correlation coefficient by a measures of association that characterize independence. Such screening methods eliminate predictor variables that appear to be independent from the response variable, and have the advantage of being applicable in a fully nonparametric, i.e., completely general, setting. However, not every variable which is statistically dependent with the response is necessarily a relevant predictor variable. For example, a variable may influence the variance function of the response, but not the regression function. For the purposes of building a prediction model, a screening method that eliminates variables that do not contribute to the regression function is more desirable. Research conducted under the present grant has developed such a screening method under quite general conditions. It also developed a new, copula-based, class of measures that characterize independence.</p>\n<p>Variable selection procedures are dimension reduction procedures in the sense that they reduce the number of predictors to be considered in model building. However, the term \"dimension reduction\" refers, primarily, to the class of procedures which attempt to identify a number of linear combinations of the available predictor variables on which the regression function depends. This area of research uses the dimension reduction model which specifies that the regression function m(x) depends on K, say, linear combination of X1,...,Xp through an unknown function. Imposing K = 1 yields the single index model and, in this context, coefficients making up the linear combination is the first projective direction. For K&gt;1 the model is called multi index model. For K=p the multi index model is equivalent to the fully nonparametric model; having K much less than p amounts to a type of sparsity. Research conducted under the present grant has studied the properties of the first projective direction under a fully nonparametric model. It also developed a class of new dimension reduction models that bridge the gap between projection pursuit and multi-index models.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/16/2017<br>\n\t\t\t\t\tModified by: Michael&nbsp;G&nbsp;Akritas</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPrediction of a response variable Y on the basis of available information on a number, p, of predictor variables, X = (X1,...,Xp), is typically based on the regression function, m(X), which expresses the expected value of Y for any given value of X. Estimation of the regression function is the aim of any regression methodology. A growing feature in many areas of contemporary research is that the number p of available predictors is quite high. The high number of predictors implies that prediction models, such as the ubiquitous linear model, include a high number of parameters. This results in reduced model interpretability and in diminished accuracy in model estimation. To deal effectively with the presence of a high number of predictors, statistical research has relied on the principle of sparsity which states that prediction of the response variable can typically be based on only a small number of predictors. Statistical tools for identifying the set of relevant predictors include hypothesis testing and variable selection. In this context, hypothesis testing refers to the problem of testing for the significance of a particular group of predictors, while variable selection is typically addressed through minimization of a constrained or penalized objective function. At a conceptual level, however, the problem of variable selection is intimately connected to hypothesis testing: dropping a variable from the model is equivalent to not declaring that variable significant. This connection was made precise by showing that, in the context of the linear regression model, multiple testing using the false discovery approach can be translated into minimizing a model selection criterion.\n\nSignificance testing and variable selection procedures based on the assumption that the regression function is linear may fail to discern the relevance of covariates whose effect on m(x) is nonlinear. Even if good results are obtained by linear model fitting, they can sometimes be bettered by using an approach that allows nonlinear effects. This statement has been thoroughly demonstrated by simulation studies where the classical approach of linear model fitting fails completely to detect the significance of predictors having a highly nonlinear effect. Research conducted under the present grant has developed test-based variable selection procedures designed to detect all types of effects under a completely general framework.\n\nFor ultra-high dimensional data, i.e., ultra-high number of predictors, the selection of the relevant variables is best achieved through a two-step process. The first stage consists of variable screening, a process designed to eliminate irrelevant variables reducing thus the number of candidate predictors. Once the number of predictors has been reduced, a variable selection method can be applied more efficiently. The original variable screening method was applied to the linear model and thus suffers from the same deficiencies, discussed in the previous paragraphs, as variable selection methods developed for the linear model. Subsequent research considered more general models, but not quite fully nonparametric models. A different class of generalizations of the original variable screening method consists of replacing Pearson?s linear correlation coefficient by a measures of association that characterize independence. Such screening methods eliminate predictor variables that appear to be independent from the response variable, and have the advantage of being applicable in a fully nonparametric, i.e., completely general, setting. However, not every variable which is statistically dependent with the response is necessarily a relevant predictor variable. For example, a variable may influence the variance function of the response, but not the regression function. For the purposes of building a prediction model, a screening method that eliminates variables that do not contribute to the regression function is more desirable. Research conducted under the present grant has developed such a screening method under quite general conditions. It also developed a new, copula-based, class of measures that characterize independence.\n\nVariable selection procedures are dimension reduction procedures in the sense that they reduce the number of predictors to be considered in model building. However, the term \"dimension reduction\" refers, primarily, to the class of procedures which attempt to identify a number of linear combinations of the available predictor variables on which the regression function depends. This area of research uses the dimension reduction model which specifies that the regression function m(x) depends on K, say, linear combination of X1,...,Xp through an unknown function. Imposing K = 1 yields the single index model and, in this context, coefficients making up the linear combination is the first projective direction. For K&gt;1 the model is called multi index model. For K=p the multi index model is equivalent to the fully nonparametric model; having K much less than p amounts to a type of sparsity. Research conducted under the present grant has studied the properties of the first projective direction under a fully nonparametric model. It also developed a class of new dimension reduction models that bridge the gap between projection pursuit and multi-index models.\n\n\t\t\t\t\tLast Modified: 08/16/2017\n\n\t\t\t\t\tSubmitted by: Michael G Akritas"
 }
}