{
 "awd_id": "1237134",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHB: Type II (INT): Synthesizing Self-Model and Mirror Feedback Imageries with Applications to Behavior Modeling for Children with Autism",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032922568",
 "po_email": "wnilsen@nsf.gov",
 "po_sign_block_name": "Wendy Nilsen",
 "awd_eff_date": "2012-10-01",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 798912.0,
 "awd_amount": 798912.0,
 "awd_min_amd_letter_date": "2012-09-06",
 "awd_max_amd_letter_date": "2014-01-22",
 "awd_abstract_narration": "This project is an interdisciplinary, integrated research and education program to develop novel technologies in manipulating mirror images, aimed at studying and enabling behavioral modeling of children with autism spectrum disorder (ASD). Central to the research is a \"virtual-mirror\" device that combines a network of calibrated depth and visual sensors to render a viewpoint-dependent dynamic view of an arbitrary-shaped virtual mirror on a room-size see-through display. Through multimodal and spatially-diverse sensors, the proposed system provides high-fidelity, non-intrusive capturing of eye gaze, facial expression, body pose, body movement, and other human behavioral patterns. New multimedia processing algorithms will be developed for transferring 2D and 3D physical appearances, as well as behaviors from a source individual to a target individual with limited target training data to be rendered on regular displays and the virtual mirror.\r\n \r\nChildren with ASD typically lack interest in social interactions, but appear to be highly interested in their own image in mirrors and others imitating their actions. The software and hardware systems developed in this project can provide unprecedented capability in creating novel behaviors of self in both traditional visual medium and immersive devices. The system is expected to provide greater flexibility to therapists, teachers, and caretakers in creating material for video modeling and self-modeling therapy. Deployment over the web lowers the technology barrier, increases access to evidence-based treatments, and promotes consistent practice of behavioral modeling in home and beyond clinics.  By combining visual feedback and real-time rendering of new behaviors, the virtual mirror is expected to deliver more effective behavioral modeling for children with ASD. Success will indicate that the self/other system in ASD can be modified, and suggest that a fundamental deficit in ASD is subject to environmental manipulation. The proposed research program is a collaborative effort of PIs from electrical engineering, psychology, medicine, and education. The educational objective is to develop a research program in which problem identification, data sharing, and problem solving occur collaboratively from the very beginning.  Outreach programs including device demonstrations at pediatric clinics, television documentaries of research results, and visits to high schools in rural districts are planned to promote the use of technologies in solving important health and societal problems, as well as to broaden the participation of under-represented groups in STEM activities.\r\n\r\nInformation about this project, including but not limited to the description, personnel, acknowledgements, latest results, and publications, will be posted at http://www.vis.uky.edu/mialab/NSF_autism.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sen-ching",
   "pi_last_name": "Cheung",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Sen-ching S Cheung",
   "pi_email_addr": "cheung@engr.uky.edu",
   "nsf_id": "000427666",
   "pi_start_date": "2012-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Ramesh",
   "pi_last_name": "Bhatt",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Ramesh S Bhatt",
   "pi_email_addr": "rbhatt@email.uky.edu",
   "nsf_id": "000390374",
   "pi_start_date": "2012-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Neelkamal",
   "pi_last_name": "Soares",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Neelkamal S Soares",
   "pi_email_addr": "neelkamal.soares@uky.edu",
   "nsf_id": "000600498",
   "pi_start_date": "2012-09-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Lisa",
   "pi_last_name": "Ruble",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lisa Ruble",
   "pi_email_addr": "lisa.ruble@uky.edu",
   "nsf_id": "000600501",
   "pi_start_date": "2012-09-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Kentucky Research Foundation",
  "inst_street_address": "500 S LIMESTONE",
  "inst_street_address_2": "109 KINKEAD HALL",
  "inst_city_name": "LEXINGTON",
  "inst_state_code": "KY",
  "inst_state_name": "Kentucky",
  "inst_phone_num": "8592579420",
  "inst_zip_code": "405260001",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "KY06",
  "org_lgl_bus_name": "UNIVERSITY OF KENTUCKY RESEARCH FOUNDATION, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "H1HYA8Z1NTM5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Kentucky Research Foundation",
  "perf_str_addr": "500 S Limestone 109 Kinkead Hall",
  "perf_city_name": "Lexington",
  "perf_st_code": "KY",
  "perf_st_name": "Kentucky",
  "perf_zip_code": "405260001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "KY06",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "801800",
   "pgm_ele_name": "Smart and Connected Health"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "8018",
   "pgm_ref_txt": "Smart and Connected Health"
  },
  {
   "pgm_ref_code": "8062",
   "pgm_ref_txt": "SCH Type II: INT"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 798912.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Autism Spectrum Disorder (ASD) is the most prevalent developmental disorder among children in the US. The Center of Disease Control estimated that ASD currently affects 1 in 68 children. Hallmarks of the condition are social and communicative impairments. The former includes poor eye contact, a lack of shared enjoyment and social reciprocity, while the latter leads to significant difficulty in producing speech and using gestures to communicate with others.These impairments could lead to life-long obstacles in developing social relations and maintaining communicative engagements with others. If left untreated, most children with ASD will not be able to live independently as adults. Although early behavioral and educational interventions are effective in addressing many of these deficits, such interventions traditionally require significant effort from parents, therapists, and teachers.</p>\n<p><br />In this NSF project, our interdisciplinary team at University of Kentucky have developed novel multimedia-based instruction (MBI) technologies for children with ASD. MBI presents concepts in a systematic, simple format, and it effectively gains and keeps the child&rsquo;s attention while providing a less emotionally laden way to learn. Many children with ASD find explicit routines in MBI comforting, circumventing difficult social demands through predictable interaction patterns. From the standpoint of delivering the interventions, MBI offers portability across different learning environments, accessibility by diverse population, controlled presentation of instructional stimuli, and customization based on individual needs. Our project aimed at developing tools that could provide greater flexibility for therapists, teachers, and caretakers to adopt MBI in their programs. The deployment of such tools should lower the technology barrier, increase access to evidence-based treatments, and promote consistent practice of behavioral modeling in home and other places beyond clinics.</p>\n<p><br />The two main prototypes developed in this project are MEBook and Virtual Mirror. MEBook is a social narrative tool combined with serious games for social greeting training (Fig. 1). While most of us do not think much about saying &ldquo;good morning&rdquo; or &ldquo;goodbye,&rdquo; greetings involve all key elements of social interaction&mdash;proper eye gaze, hand movement, and vocalization, all of which are difficult for most children with ASD to master. MEBook combines elements from three different evidence-based interventions: social narratives,video self-modeling (VSM), and reinforcement. A novel component of MEBook is the use of an RGB-depth sensor to implement a gesture-based video game that serves a dual purpose of reinforcing the learning and creating raw material for VSM. Human-subject study on MEBook with ASD children age 7 to 12 showed that subjects demonstrated marked improvements in both vocalization and eye contact after the intervention and were able to maintain even after the end of the MEBook intervention.</p>\n<p><br />The Virtual Mirror system extends beyond MEBook and other VSM systems which rely solely on genuine video records of the target behaviors of the learners. The fidelity of those behaviors is often poor because the learner has not yet mastered the skills. Virtual Mirror is a room-based augmented mirror display system in which the learner can see, in real-time, a computer-generated himself or herself engaging in the target behavior at a slightly higher skill level. It is a complex system with many innovations that break new grounds in computer vision and graphics. These innovations include (1) denoising and segmentation algorithms for both color and depth cameras (Fig. 2), (2) color and depth camera network calibration and rendering systems for 3D virtual environment (Fig. 3), (3) planar and curve mirror simulators (Fig. 4), and (4) a behavior transfer system that can transfer facial expression, eye gaze, and body pose from one image to another (Fig. 5). Human subject studies are underway to measure the effectiveness of the Virtual Mirror system in helping children with ASD in various learning tasks.</p>\n<p><br />The research products stemmed from this project have been widely disseminated. A total of 19 journal papers and 21 conference papers have been published in the past five years. This project has provided partial financial support for 12 graduate students and five undergraduate students from three different departments: Electrical &amp; Computer Engineering, Computer Science, and Educational Psychology. Five doctoral students and six master students have successfully defended their degrees based on the work directly related to the project goals. A broad range of outreach activities have been conducted during the funding period, including a year-long programming class for special-need high school students, project demonstrations in local K-12 schools, and technical talks in autism advocacy groups, local makerspace, as well as government meetings and entrepreneurship groups.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/18/2018<br>\n\t\t\t\t\tModified by: Sen-Ching&nbsp;S&nbsp;Cheung</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524014572743_MEBook--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524014572743_MEBook--rgov-800width.jpg\" title=\"MEBook Screenshots\"><img src=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524014572743_MEBook--rgov-66x44.jpg\" alt=\"MEBook Screenshots\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Fig. 1: Two elements of MEBook: social narrative with self-modeling (left), and gesture-game with positive reinforcement (right)</div>\n<div class=\"imageCredit\">Nkiruka Uzuegbunam</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sen-Ching&nbsp;S&nbsp;Cheung</div>\n<div class=\"imageTitle\">MEBook Screenshots</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524015427023_DenoisingandSegmentation--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524015427023_DenoisingandSegmentation--rgov-800width.jpg\" title=\"Depth Segmentation and Denoising\"><img src=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524015427023_DenoisingandSegmentation--rgov-66x44.jpg\" alt=\"Depth Segmentation and Denoising\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Fig. 2: From top left to bottom right: original color image, original noisy depth image, segmentation and denoised depth image produced by our algorithm.</div>\n<div class=\"imageCredit\">Ju Shen</div>\n<div class=\"imageSubmitted\">Sen-Ching&nbsp;S&nbsp;Cheung</div>\n<div class=\"imageTitle\">Depth Segmentation and Denoising</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524026061680_calibration--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524026061680_calibration--rgov-800width.jpg\" title=\"Virtual 3D Environment and Telepresence\"><img src=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524026061680_calibration--rgov-66x44.jpg\" alt=\"Virtual 3D Environment and Telepresence\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Fig. 3: Our calibration system captures and renders virtual 3D environment (center) in real-time from five live-feed cameras (bottom), as well as one remote camera (top right) from where an individual is captured and inserted into the virtual scene.</div>\n<div class=\"imageCredit\">Po-chang Su</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sen-Ching&nbsp;S&nbsp;Cheung</div>\n<div class=\"imageTitle\">Virtual 3D Environment and Telepresence</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524027728902_mirror--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524027728902_mirror--rgov-800width.jpg\" title=\"Mirror simulations\"><img src=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524027728902_mirror--rgov-66x44.jpg\" alt=\"Mirror simulations\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Fig. 4: From left to right: physical scene, plane mirror simulator with a different background, plane mirror simulator with another background and shirt color, and ?hourglass? curved mirror simulator on a different scene.</div>\n<div class=\"imageCredit\">Po-chang Su and Ju Shen</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Sen-Ching&nbsp;S&nbsp;Cheung</div>\n<div class=\"imageTitle\">Mirror simulations</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524029019460_behaviortransfer--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524029019460_behaviortransfer--rgov-800width.jpg\" title=\"Behavior Transfer\"><img src=\"/por/images/Reports/POR/2018/1237134/1237134_10211616_1524029019460_behaviortransfer--rgov-66x44.jpg\" alt=\"Behavior Transfer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Fig .5: The 3 images on the left show how Tony Blair?s facial expression and eye gaze are transferred to George Clooney. The right figures show how the body pose is transferred from the original generic figure, first to the target pose, and then to the final 3D shape with clothing and body textures.</div>\n<div class=\"imageCredit\">Wanxin Xu</div>\n<div class=\"imageSubmitted\">Sen-Ching&nbsp;S&nbsp;Cheung</div>\n<div class=\"imageTitle\">Behavior Transfer</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAutism Spectrum Disorder (ASD) is the most prevalent developmental disorder among children in the US. The Center of Disease Control estimated that ASD currently affects 1 in 68 children. Hallmarks of the condition are social and communicative impairments. The former includes poor eye contact, a lack of shared enjoyment and social reciprocity, while the latter leads to significant difficulty in producing speech and using gestures to communicate with others.These impairments could lead to life-long obstacles in developing social relations and maintaining communicative engagements with others. If left untreated, most children with ASD will not be able to live independently as adults. Although early behavioral and educational interventions are effective in addressing many of these deficits, such interventions traditionally require significant effort from parents, therapists, and teachers.\n\n\nIn this NSF project, our interdisciplinary team at University of Kentucky have developed novel multimedia-based instruction (MBI) technologies for children with ASD. MBI presents concepts in a systematic, simple format, and it effectively gains and keeps the child?s attention while providing a less emotionally laden way to learn. Many children with ASD find explicit routines in MBI comforting, circumventing difficult social demands through predictable interaction patterns. From the standpoint of delivering the interventions, MBI offers portability across different learning environments, accessibility by diverse population, controlled presentation of instructional stimuli, and customization based on individual needs. Our project aimed at developing tools that could provide greater flexibility for therapists, teachers, and caretakers to adopt MBI in their programs. The deployment of such tools should lower the technology barrier, increase access to evidence-based treatments, and promote consistent practice of behavioral modeling in home and other places beyond clinics.\n\n\nThe two main prototypes developed in this project are MEBook and Virtual Mirror. MEBook is a social narrative tool combined with serious games for social greeting training (Fig. 1). While most of us do not think much about saying \"good morning\" or \"goodbye,\" greetings involve all key elements of social interaction&mdash;proper eye gaze, hand movement, and vocalization, all of which are difficult for most children with ASD to master. MEBook combines elements from three different evidence-based interventions: social narratives,video self-modeling (VSM), and reinforcement. A novel component of MEBook is the use of an RGB-depth sensor to implement a gesture-based video game that serves a dual purpose of reinforcing the learning and creating raw material for VSM. Human-subject study on MEBook with ASD children age 7 to 12 showed that subjects demonstrated marked improvements in both vocalization and eye contact after the intervention and were able to maintain even after the end of the MEBook intervention.\n\n\nThe Virtual Mirror system extends beyond MEBook and other VSM systems which rely solely on genuine video records of the target behaviors of the learners. The fidelity of those behaviors is often poor because the learner has not yet mastered the skills. Virtual Mirror is a room-based augmented mirror display system in which the learner can see, in real-time, a computer-generated himself or herself engaging in the target behavior at a slightly higher skill level. It is a complex system with many innovations that break new grounds in computer vision and graphics. These innovations include (1) denoising and segmentation algorithms for both color and depth cameras (Fig. 2), (2) color and depth camera network calibration and rendering systems for 3D virtual environment (Fig. 3), (3) planar and curve mirror simulators (Fig. 4), and (4) a behavior transfer system that can transfer facial expression, eye gaze, and body pose from one image to another (Fig. 5). Human subject studies are underway to measure the effectiveness of the Virtual Mirror system in helping children with ASD in various learning tasks.\n\n\nThe research products stemmed from this project have been widely disseminated. A total of 19 journal papers and 21 conference papers have been published in the past five years. This project has provided partial financial support for 12 graduate students and five undergraduate students from three different departments: Electrical &amp; Computer Engineering, Computer Science, and Educational Psychology. Five doctoral students and six master students have successfully defended their degrees based on the work directly related to the project goals. A broad range of outreach activities have been conducted during the funding period, including a year-long programming class for special-need high school students, project demonstrations in local K-12 schools, and technical talks in autism advocacy groups, local makerspace, as well as government meetings and entrepreneurship groups.\n\n\t\t\t\t\tLast Modified: 04/18/2018\n\n\t\t\t\t\tSubmitted by: Sen-Ching S Cheung"
 }
}