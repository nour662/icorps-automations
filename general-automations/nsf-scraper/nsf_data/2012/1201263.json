{
 "awd_id": "1201263",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Computability and Randomness in Dynamical Systems and Fractal Geometry",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924885",
 "po_email": "tbartosz@nsf.gov",
 "po_sign_block_name": "Tomek Bartoszynski",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 91693.0,
 "awd_amount": 91693.0,
 "awd_min_amd_letter_date": "2012-03-15",
 "awd_max_amd_letter_date": "2012-03-15",
 "awd_abstract_narration": "Reimann proposes to investigate interactions between computability theory, dynamical systems, and geometric measure theory. Reimann intends to use concepts from dynamical systems and fractal geometry to study computability theoretic structures, in particular, the set MIN of reals of minimal Turing degree. Being a central object in the study of degrees of unsolvability, minimal degrees have recently exhibited interesting properties with respect to geometric measure theory. An open problem is the determination of the Hausdorff dimension of MIN. This problem is related to questions concerning extraction of randomness, diagonally non-computable functions, and Sacks forcing. It also motivates questions about algorithmic independence of reals, and how effective randomness (with respect to arbitrary measures) behaves under splits and joins. In a second area of investigation, Reimann proposes to study algorithmic reducibilities from the point of view of Borel equivalence relations. In a remarkable confluence of methods from descriptive set theory, ergodic theory, topological dynamics, and other areas, researchers have successfully classified many equivalence relations. Yet most equivalence relations arising from computability theoretic reducibilities have so far resisted complete classification. Reimann intends to investigate LR-equivalence, an equivalence relation of fundamental importance in effective randomness, and also the role uniformity plays in the classification of Borel equivalence relations.\r\n\r\nComputability and randomness are two of the fundamental ideas that drove the scientific revolutions of the 20th century and changed the way we think about the world. Computability theory concerns itself with trying to understand which problems are solvable by computers. It was developed as a rigorous mathematical discipline in the 1930s through the work of G\u00f6del, Church, Turing and others. Around the same time, Kolmogorov provided the notion of randomness with a solid mathematical foundation in the form of measure theoretic probability. The theory of effective randomness, which brings together probability theory and computability theory, has made it possible to qualitatively and quantitatively study the ways in which the two notions, computability and randomness, delimit and condition each other. It gives a mathematically precise meaning to questions like \"Are random processes necessarily uncomputable?\" or \"If we have access to randomness, can we facilitate computation?\" The major objective of the proposed project is to further the study of the interaction between randomness and computability. In one part of the project, this interaction is to be studied with the help of concepts from two other areas of mathematics - geometric measure theory and ergodic theory. In another part, Reimann plans to explore the role computability and randomness play in certain areas of dynamical systems. The latter objective can be seen as a first step of a long-term project - to help pave the way for a better, more exact understanding of the forms in which randomness does occur in this world, through the theory of effective randomness.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jan",
   "pi_last_name": "Reimann",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jan Reimann",
   "pi_email_addr": "jan.reimann@psu.edu",
   "nsf_id": "000296492",
   "pi_start_date": "2012-03-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "",
  "perf_city_name": "University Park",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168027000",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "PA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126800",
   "pgm_ele_name": "FOUNDATIONS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 91693.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We often hear that certain processes are referred to as \"random\" or \"unpredictable\". This may be a sequence of coin tosses, radioactive decay, or the occurrence of large earthquakes.</p>\n<p><br />While the randomness of coin tosses is often a welcome feature, as it helps us make \"unbiased\" decisions (e.g. who gets the ball first at a football game). But in the case of earthquakes, the apparent randomness is a most unwelcome feature as arguably many lives could be saved if the occurrence of a large seismic event was predictable, say with a five-day warning.</p>\n<p>But what would it mean that earthquakes occur truly \"randomly\" (as we believe radioactive decay does)? And how could one rigorously \"prove\" such a fact? In the usual practice of probabilistic modeling, while we cannot explain, let alone predict, a certain sequence of events, one can nevertheless observe certain statistical regularities. While we cannot tell which side of the coin will land up on the next toss, we see that over time the relative frequency of each side is about 1/2. We then try to model this process by means of a suitable probability distribution. This can be a very simple one, as in the case of a coin toss, or a rather complicated one, as in the case of earthquakes. We then see how the data \"fits\" the distribution, using statistical tests, such as the Kolmogorov-Smirnov test.</p>\n<p>The problem is that data can fit a distribution really well without being \"truly\" random - there are sequences, such as the famous Champernown sequence, that exhibit the necessary statistical regularities while being completely predictable. Is it possible to mathematically quantify the amount of randomness present, so that we can, for instance, say that if a process satisfies a strong enough randomness property, we will not be able to compute/predict it? &nbsp;</p>\n<p>Reimann's research focused on a theory that makes such a quantification possible, in a rigorous mathematical way. The framework of algorithmic information theory and randomness was developed over the past four decades to give a precise mathematical meaning to statements like \"this is a random sequence\". While initially devised as a solution to a foundational problem in mathematical logic, it has recently been applied to other mathematical structures. For example, in joint work with T. Slaman, Reimann showed that in order to prove that most data <em>is</em> random with respect to some distribution, one has to make set theoretic assumptions far beyond the immediate setting of random sequences (real numbers), revealing an interesting metamathematical twist.&nbsp;</p>\n<p><br />With C. Freer, Reimann studied randomness for graph sequences, where the randomness manifests itself in the way new nodes join existing ones in a network. They showed that for a certain class of graphs, the so-called Henson graphs, to generate them via a random process, the underlying distribution (a so-called graphon) must be rather complicated, in the sense that it is extremely fractal.</p>\n<p>A large part of Reimann's project was devoted to developing a sound mathematical foundation for quantifying the randomness of earthquake occurrences. One of the most popular probabilistic models for seismological processes is the ETAS (Epidemic Type AfterShock) model, in which earthquakes are taken to trigger other earthquakes with a probability proportional to their magnitude. Mathematically, the ETAS model is a multidimensional point process. Reimann's work with L. Axon established a rigorous formulation of algorithmic randomness for general point processes, via the theory of random closed sets. Moreover, Reimann showed a strong connection between Kolmgorov complexity (a notion that captures how well data is compressible at all) and the mutlifractal formalism. Multifractal measures have been introduced by Mandelbrot and others in the 1980s and have become one of the major tools to measu...",
  "por_txt_cntn": "\nWe often hear that certain processes are referred to as \"random\" or \"unpredictable\". This may be a sequence of coin tosses, radioactive decay, or the occurrence of large earthquakes.\n\n\nWhile the randomness of coin tosses is often a welcome feature, as it helps us make \"unbiased\" decisions (e.g. who gets the ball first at a football game). But in the case of earthquakes, the apparent randomness is a most unwelcome feature as arguably many lives could be saved if the occurrence of a large seismic event was predictable, say with a five-day warning.\n\nBut what would it mean that earthquakes occur truly \"randomly\" (as we believe radioactive decay does)? And how could one rigorously \"prove\" such a fact? In the usual practice of probabilistic modeling, while we cannot explain, let alone predict, a certain sequence of events, one can nevertheless observe certain statistical regularities. While we cannot tell which side of the coin will land up on the next toss, we see that over time the relative frequency of each side is about 1/2. We then try to model this process by means of a suitable probability distribution. This can be a very simple one, as in the case of a coin toss, or a rather complicated one, as in the case of earthquakes. We then see how the data \"fits\" the distribution, using statistical tests, such as the Kolmogorov-Smirnov test.\n\nThe problem is that data can fit a distribution really well without being \"truly\" random - there are sequences, such as the famous Champernown sequence, that exhibit the necessary statistical regularities while being completely predictable. Is it possible to mathematically quantify the amount of randomness present, so that we can, for instance, say that if a process satisfies a strong enough randomness property, we will not be able to compute/predict it?  \n\nReimann's research focused on a theory that makes such a quantification possible, in a rigorous mathematical way. The framework of algorithmic information theory and randomness was developed over the past four decades to give a precise mathematical meaning to statements like \"this is a random sequence\". While initially devised as a solution to a foundational problem in mathematical logic, it has recently been applied to other mathematical structures. For example, in joint work with T. Slaman, Reimann showed that in order to prove that most data is random with respect to some distribution, one has to make set theoretic assumptions far beyond the immediate setting of random sequences (real numbers), revealing an interesting metamathematical twist. \n\n\nWith C. Freer, Reimann studied randomness for graph sequences, where the randomness manifests itself in the way new nodes join existing ones in a network. They showed that for a certain class of graphs, the so-called Henson graphs, to generate them via a random process, the underlying distribution (a so-called graphon) must be rather complicated, in the sense that it is extremely fractal.\n\nA large part of Reimann's project was devoted to developing a sound mathematical foundation for quantifying the randomness of earthquake occurrences. One of the most popular probabilistic models for seismological processes is the ETAS (Epidemic Type AfterShock) model, in which earthquakes are taken to trigger other earthquakes with a probability proportional to their magnitude. Mathematically, the ETAS model is a multidimensional point process. Reimann's work with L. Axon established a rigorous formulation of algorithmic randomness for general point processes, via the theory of random closed sets. Moreover, Reimann showed a strong connection between Kolmgorov complexity (a notion that captures how well data is compressible at all) and the mutlifractal formalism. Multifractal measures have been introduced by Mandelbrot and others in the 1980s and have become one of the major tools to measure complexity of dynamical systems and stochastic processes. \n\n\nReimann, with the help of his students, also conducted some pra..."
 }
}