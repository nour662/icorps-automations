{
 "awd_id": "1256172",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: A Nugget-Based Information Retrieval Evaluation Paradigm",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2012-09-15",
 "awd_exp_date": "2015-02-28",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2012-09-21",
 "awd_max_amd_letter_date": "2012-09-25",
 "awd_abstract_narration": "Evaluating information retrieval systems, such as search engines, is critical to their effective development. Current performance evaluation methodologies are generally variants of the Cranfield paradigm, which relies on effectively complete, and thus prohibitively expensive, relevance judgment sets: tens to hundreds of thousands of documents must be judged by human assessors for relevance with respect to dozens to hundreds of user queries, at great cost both in time and expense. This exploratory project investigates a new alternative to information retrieval evaluation paradigm -- based on \"nuggets\".  \"Nuggets\" are atomic units of relevant information, and one instantiation of these nuggets is simply the sentence or short passage that causes a judge to deem a document relevant at the time of document assessment. The hypothesis is that while it is likely impossible to find all relevant documents for a query with respect to web-scale and/or dynamic collections, it is much more tractable to find all or nearly all nuggets (i.e., relevant information), with which one can then perform effective and reusable evaluation, at scale and with ease. At evaluation time, relevance assessments are dynamically created for documents based on the quantity and quality of relevant information found in the documents retrieved. This new evaluation paradigm is inherently scalable and permits the use of all standard measures of retrieval performance, including those involving graded relevance judgments, novelty, diversity, and so on; it further permits new kinds of evaluations not heretofore possible.\r\n\r\nThe project plan includes the development and release of nugget-based evaluation data sets for use by academia and industry. In fostering this effort, the project team has close ties with the US National Institute of Standards and Technology (NIST) and the Japanese National Institute of Informatics (through NTCIR), two of the premier organizations that develop and release information retrieval data sets. All research results and data sets developed as part of this project are available at the project website (http://www.ccs.neu.edu/home/jaa/IIS-1256172/). The project also provides educational and training experience for students and the development of curricular materials based on the project results.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Javed",
   "pi_last_name": "Aslam",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Javed A Aslam",
   "pi_email_addr": "jaa@ccs.neu.edu",
   "nsf_id": "000312612",
   "pi_start_date": "2012-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northeastern University",
  "inst_street_address": "360 HUNTINGTON AVE",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173735600",
  "inst_zip_code": "021155005",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "NORTHEASTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HLTMVS2JZBS6"
 },
 "perf_inst": {
  "perf_inst_name": "Northeastern University",
  "perf_str_addr": "360 Huntington Avenue",
  "perf_city_name": "Boston",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021155005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Evaluating information retrieval systems, such as search engines, is critical to their effective development. Existing performance evaluation methodologies are generally variants of the Cranfield paradigm, which relies on effectively complete, and thus prohibitively expensive, relevance judgment sets: tens to hundreds of thousands of documents must be judged by human assessors for relevance with respect to dozens to hundreds of user queries, at great cost both in time and expense.</p>\n<p><br />Our work instead proposed a new information retrieval evaluation paradigm based on nuggets. The thesis is that while it is likely impossible to find all relevant documents for a query with respect to web-scale and/or dynamic collections, it is much more tractable to find all or nearly all relevant information, with which one can then perform effective and reusable evaluation, at scale and with ease. These atomic units of relevant information are referred to as \"nuggets\", and one instantiation of these nuggets is simply the sentence or short passage that causes a judge to deem a document relevant at the time of document assessment. At evaluation time, relevance assessments are dynamically created for documents based on the quantity and quality of relevant information found in the documents retrieved. This new evaluation paradigm is inherently scalable and permits the use of all standard measures of retrieval performance. &nbsp;It further permits new kinds of evaluations not heretofore possible, including the evaluation of temporal and mobile summarization systems.</p>\n<p><br />Our work included (1) the development of semi-automated algorithmic techniques for nugget extraction and test collection creation, (2) the development of algorithmic techniques for inferring document relevance from these extracted nuggets, (3) the development of working evaluation systems based on these techniques, and (4) the application of these techniques to the evaluation of temporal and mobile summarization systems, including task specification, new evaluation metrics, and new nugget-based evaluation systems. &nbsp;As part of our project, nugget-based evaluation data sets were development and release for use by academia and industry through partnerships with the US National Institute of Standards and Technology (NIST) and the Japanese National Institute of Informatics (through NTCIR), two of the premier organizations that develop and release information retrieval data sets.&nbsp;</p>\n<p><br />Finally, the project provided thesis topics for two PhD students who have gone on to pursue careers in governmental and industrial research.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/23/2017<br>\n\t\t\t\t\tModified by: Javed&nbsp;A&nbsp;Aslam</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nEvaluating information retrieval systems, such as search engines, is critical to their effective development. Existing performance evaluation methodologies are generally variants of the Cranfield paradigm, which relies on effectively complete, and thus prohibitively expensive, relevance judgment sets: tens to hundreds of thousands of documents must be judged by human assessors for relevance with respect to dozens to hundreds of user queries, at great cost both in time and expense.\n\n\nOur work instead proposed a new information retrieval evaluation paradigm based on nuggets. The thesis is that while it is likely impossible to find all relevant documents for a query with respect to web-scale and/or dynamic collections, it is much more tractable to find all or nearly all relevant information, with which one can then perform effective and reusable evaluation, at scale and with ease. These atomic units of relevant information are referred to as \"nuggets\", and one instantiation of these nuggets is simply the sentence or short passage that causes a judge to deem a document relevant at the time of document assessment. At evaluation time, relevance assessments are dynamically created for documents based on the quantity and quality of relevant information found in the documents retrieved. This new evaluation paradigm is inherently scalable and permits the use of all standard measures of retrieval performance.  It further permits new kinds of evaluations not heretofore possible, including the evaluation of temporal and mobile summarization systems.\n\n\nOur work included (1) the development of semi-automated algorithmic techniques for nugget extraction and test collection creation, (2) the development of algorithmic techniques for inferring document relevance from these extracted nuggets, (3) the development of working evaluation systems based on these techniques, and (4) the application of these techniques to the evaluation of temporal and mobile summarization systems, including task specification, new evaluation metrics, and new nugget-based evaluation systems.  As part of our project, nugget-based evaluation data sets were development and release for use by academia and industry through partnerships with the US National Institute of Standards and Technology (NIST) and the Japanese National Institute of Informatics (through NTCIR), two of the premier organizations that develop and release information retrieval data sets. \n\n\nFinally, the project provided thesis topics for two PhD students who have gone on to pursue careers in governmental and industrial research.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 04/23/2017\n\n\t\t\t\t\tSubmitted by: Javed A Aslam"
 }
}