{
 "awd_id": "1217648",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: AF: Small: Collaborative Research:RESAR:  Robust, Efficient, Scalable, Autonomous Reliable Storage for the Cloud",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032927498",
 "po_email": "achtchel@nsf.gov",
 "po_sign_block_name": "Almadena Chtchelkanova",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 109986.0,
 "awd_amount": 109986.0,
 "awd_min_amd_letter_date": "2012-06-12",
 "awd_max_amd_letter_date": "2012-06-12",
 "awd_abstract_narration": "With the growth of cloud computing and the changing manner in which individuals and businesses interact with data, it is increasingly important to manage data efficiently and reliably. The RESAR project tackles the problem of building ever-larger data stores, and offers a novel approach to reducing the energy impact of such increases in scale while allowing easier management and adaptation of the system as it ages. In other words, RESAR offers a means to gracefully adapt a storage system to offer increased reliability or performance as demanded by the systems' age or administrator's requirements. This project develops, studies, and optimizes reliable, energy-efficient storage needed in modern data centers and large-scale data storage environments, and allows such storage systems to gracefully increase its performance and reliability while efficiently scaling to millions of storage devices.\r\n\r\nFor storage systems to be feasible and manageable at increasing scales, need to be self-healing and  self-optimizing, able to adapt to aging and new components whilst dynamically recovering from inevitable component failures. Cloud computing promises savings in staffing as the volume of work in a data center would be distributed over fewer, but better trained staff. While the increasing scale of such data centers offers greater opportunities for energy-saving measures to become more effective, such scales rapidly increase fears of individual components failing. This demands that such large scale storage systems be arranged in such a way as to offer an ability to survive the failure of multiple components, and to do so with minimal management overheads.\r\n\r\nTo survive the increasingly likely component failures (brought about by the increasing numbers of components in ever-growing data warehouses), storage systems typically employ some form of data replication or redundancy scheme. This strategy not only protects data against loss, but also allows faster access. Unfortunately, doubling or tripling the number of storage devices (or entire data centers) comes at a considerable cost. Alternatively, a site could use erasure correcting codes that provide protection against device failures while only increasing hardware demands by a smaller increment. But such erasure correcting schemes offer limited scalability and can complicate  the implementation and self-management of a system considerably. The RESAR approach is to employ novel erasure codes that allow faster layout restructuring, while offering increased scalability, and improved reliability over competing schemes. RESAR allows for restructuring on the fly, and as such, has the added benefit of being complementary to data relocation tasks necessary for routine maintenance and optimization.\r\n\r\nCloud computing and data centers are taking hold as technologies with great promise for cheaper, more flexible, and more energy-efficient information processing. RESAR enables cheaper, more reliable, automated and more easily scaled storage systems. RESAR offers a novel graph representation of a failure tolerance scheme that allows the construction of flexible, dynamically reconfigurable, parity-based redundancy schemes that are well-suited for cloud storage infrastructure. By offering the benefits of more highly-convolved erasure coding schemes, whilst remaining simple and efficient, RESAR offers a new path to self-organizing large-scale storage systems. The resulting systems are more maintainable, easily reconfigured for increasing levels of reliability on-demand, and more cost effective. This efficiency further extends to reduced maintenance and energy demands.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ahmed",
   "pi_last_name": "Amer",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Ahmed M Amer",
   "pi_email_addr": "aamer@scu.edu",
   "nsf_id": "000486455",
   "pi_start_date": "2012-06-12",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Santa Clara University",
  "inst_street_address": "500 EL CAMINO REAL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA CLARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "4085544764",
  "inst_zip_code": "950504776",
  "inst_country_name": "United States",
  "cong_dist_code": "17",
  "st_cong_dist_code": "CA17",
  "org_lgl_bus_name": "PRESIDENT AND BOARD OF TRUSTEES OF SANTA CLARA COLLEGE",
  "org_prnt_uei_num": "",
  "org_uei_num": "YE8LRJWSY3K9"
 },
 "perf_inst": {
  "perf_inst_name": "Santa Clara University",
  "perf_str_addr": "500 El Camino Real",
  "perf_city_name": "Santa Clara",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "950530250",
  "perf_ctry_code": "US",
  "perf_cong_dist": null,
  "perf_st_cong_dist": "CA",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 109986.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With ever-greater volumes of digital data being stored on an increasing number of storage devices. With increasing amounts of data being stored, and with lives and economies becoming more dependent on such data, it is increasingly critical to ensure that such data is stored reliably. But this must also be achieved efficiently, since simply making many copies of data would only increase the number of devices and the scale and expense of the infrastructure required to house it. With the ever-growing ocean of data being produced, this becomes ever-more costly.</p>\n<p><br />One technique to increase the reliability of stored data, at limited additional cost (at least in terms of the extra volume of data to be stored) is to utilize an encoding scheme, or an arrangement of the data across multiple related devices, that allows for the recovery of lost data once a device has failed. This sort of scheme traditionally works well for arrangements that include tens of devices, commonly arranged into what are called RAID (Redundant Arrays of Inexpensive/Independent Disk) arrays. To scale a RAID arrangement to hundreds of disks may be reasonable, but as we start to have tens and hundreds of thousands of devices, such schemes are typically applied as arrangements on individual arrays. A reliability scheme that is designed to be applied across such large numbers of storage devices would be more effective at leveraging the advantages of cloud-scale computing. &nbsp;&nbsp;This project has evaluated and developed an alternative to traditional RAID schemes: RESAR (Robust, Efficient, Scalable, Autonomous Reliable) storage. This scheme is both suitable for use at large scales, and works well with upcoming storage technologies.<br />Storage clusters are getting so large that the number of disk failures is quickly overwhelming efficient data layout techniques. &nbsp;As a result many service providers are falling back to primitive and inefficient techniques such as duplication and triplication. To address the need for scale and efficiency we developed RESAR Storage. &nbsp;RESAR employs a reliability scheme that abstracts the data management and layout problem as a graph-coloring algorithm, and is applicable to produce reliable data layouts across hundreds of thousands, up to millions, of devices. RESAR schemes provide a reliability considerably greater than RAID 6 or full data duplication, while adding a small storage overhead (on the order of 20% for the simplest RESAR arrangements).</p>\n<p><br />The RESAR project has demonstrated a technique for dealing with large scale distributed resource management and how this technique can be used to create a resilient and efficient storage cluster scaling from thousands to millions of disks. &nbsp;Its model for expressing data layout as a graph-coloring problem presents great promise for large cluster efficiencies with fine grain control over parameters for reliability and overhead. &nbsp;Analysis and experiments have shown how a RESAR cluster would compare to the reliability of techniques like replication andRAID. RESAR was analytically shown to be 15 times more resilient than RAID 6 with the same storage overhead and nearly as resilient as triplication with far less overhead (triplication would require 200% additional storage capacity in contrast to the extra %20 needed by RESAR). RESAR by its nature provides a more resilient structure for reliable large scale data. &nbsp;In addition this system offers great potential for fine grained control of system parameters such as parity overhead and reliability guarantees. In addition to its greater reliability and reduced storage overheads (which translate to achieving the desired reliability at considerably lower costs), the RESAR project also shows promise for how its graph-based schemes fit naturally with data layout schemes that are increasingly essential for the adoption of new storage technologies ...",
  "por_txt_cntn": "\nWith ever-greater volumes of digital data being stored on an increasing number of storage devices. With increasing amounts of data being stored, and with lives and economies becoming more dependent on such data, it is increasingly critical to ensure that such data is stored reliably. But this must also be achieved efficiently, since simply making many copies of data would only increase the number of devices and the scale and expense of the infrastructure required to house it. With the ever-growing ocean of data being produced, this becomes ever-more costly.\n\n\nOne technique to increase the reliability of stored data, at limited additional cost (at least in terms of the extra volume of data to be stored) is to utilize an encoding scheme, or an arrangement of the data across multiple related devices, that allows for the recovery of lost data once a device has failed. This sort of scheme traditionally works well for arrangements that include tens of devices, commonly arranged into what are called RAID (Redundant Arrays of Inexpensive/Independent Disk) arrays. To scale a RAID arrangement to hundreds of disks may be reasonable, but as we start to have tens and hundreds of thousands of devices, such schemes are typically applied as arrangements on individual arrays. A reliability scheme that is designed to be applied across such large numbers of storage devices would be more effective at leveraging the advantages of cloud-scale computing.   This project has evaluated and developed an alternative to traditional RAID schemes: RESAR (Robust, Efficient, Scalable, Autonomous Reliable) storage. This scheme is both suitable for use at large scales, and works well with upcoming storage technologies.\nStorage clusters are getting so large that the number of disk failures is quickly overwhelming efficient data layout techniques.  As a result many service providers are falling back to primitive and inefficient techniques such as duplication and triplication. To address the need for scale and efficiency we developed RESAR Storage.  RESAR employs a reliability scheme that abstracts the data management and layout problem as a graph-coloring algorithm, and is applicable to produce reliable data layouts across hundreds of thousands, up to millions, of devices. RESAR schemes provide a reliability considerably greater than RAID 6 or full data duplication, while adding a small storage overhead (on the order of 20% for the simplest RESAR arrangements).\n\n\nThe RESAR project has demonstrated a technique for dealing with large scale distributed resource management and how this technique can be used to create a resilient and efficient storage cluster scaling from thousands to millions of disks.  Its model for expressing data layout as a graph-coloring problem presents great promise for large cluster efficiencies with fine grain control over parameters for reliability and overhead.  Analysis and experiments have shown how a RESAR cluster would compare to the reliability of techniques like replication andRAID. RESAR was analytically shown to be 15 times more resilient than RAID 6 with the same storage overhead and nearly as resilient as triplication with far less overhead (triplication would require 200% additional storage capacity in contrast to the extra %20 needed by RESAR). RESAR by its nature provides a more resilient structure for reliable large scale data.  In addition this system offers great potential for fine grained control of system parameters such as parity overhead and reliability guarantees. In addition to its greater reliability and reduced storage overheads (which translate to achieving the desired reliability at considerably lower costs), the RESAR project also shows promise for how its graph-based schemes fit naturally with data layout schemes that are increasingly essential for the adoption of new storage technologies like Shingled Magnetic Recording (SMR).\n\n\t\t\t\t\tLast Modified: 07/02/2015\n\n\t\t\t\t\tSubmitted by: Ahmed M Amer"
 }
}