{
 "awd_id": "1200162",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Design Principles for Parallel Simulation Optimization",
 "cfda_num": "47.041",
 "org_code": "07030000",
 "po_phone": "7032922443",
 "po_email": "gaklutke@nsf.gov",
 "po_sign_block_name": "Georgia-Ann Klutke",
 "awd_eff_date": "2012-07-01",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 149714.0,
 "awd_amount": 149714.0,
 "awd_min_amd_letter_date": "2012-03-09",
 "awd_max_amd_letter_date": "2012-03-09",
 "awd_abstract_narration": "This award provides funding to advance the design of algorithms for solving simulation-optimization (SO) problems on parallel computing platforms. SO problems are optimization problems where the objective function and constraints can only be observed through a stochastic simulation. Virtually all current algorithms for solving SO problems assume a single processor as the computing platform. However, the trend in computing devices is towards multi-processor computers, not just at the laptop/desktop level, but all the way up to cloud computing environments. This research will explore how to design algorithms for solving SO problems that exploit such environments, to attempt to return high-quality solutions at a reasonable cost and within a reasonable amount of time.\r\n\r\nIf successful, the results of this research will lead to a new line of research - parallel SO - with ensuing improvements in the design and implementation of SO algorithms on parallel computing platforms, thus making this currently computing-intensive technology much more accessible and effective. SO already holds an important place in application fields, as evidenced by the variety of SO problems in an existing testbed <www.simopt.org>. The proposed research will provide further opportunities for major impact through reliable solution of important SO problems. This research is part of a continuing thrust to enhance the implementability of SO by creating methods, theory, and computational tools to facilitate the automatic solution of larger and more realistic SO problems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "CMMI",
 "org_div_long_name": "Division of Civil, Mechanical, and Manufacturing Innovation",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Raghu",
   "pi_last_name": "Pasupathy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Raghu Pasupathy",
   "pi_email_addr": "pasupath@purdue.edu",
   "nsf_id": "000495715",
   "pi_start_date": "2012-03-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Virginia Polytechnic Institute and State University",
  "inst_street_address": "300 TURNER ST NW",
  "inst_street_address_2": "STE 4200",
  "inst_city_name": "BLACKSBURG",
  "inst_state_code": "VA",
  "inst_state_name": "Virginia",
  "inst_phone_num": "5402315281",
  "inst_zip_code": "240603359",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "VA09",
  "org_lgl_bus_name": "VIRGINIA POLYTECHNIC INSTITUTE & STATE UNIVERSITY",
  "org_prnt_uei_num": "X6KEFGLHSJX7",
  "org_uei_num": "QDE5UHE5XD16"
 },
 "perf_inst": {
  "perf_inst_name": "Virginia Polytechnic Institute and State University",
  "perf_str_addr": "",
  "perf_city_name": "Blacksburg",
  "perf_st_code": "VA",
  "perf_st_name": "Virginia",
  "perf_zip_code": "240603580",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "VA09",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "551400",
   "pgm_ele_name": "OPERATIONS RESEARCH"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "072E",
   "pgm_ref_txt": "NETWORKS & QUEUING SYSTEMS"
  },
  {
   "pgm_ref_code": "073E",
   "pgm_ref_txt": "OPTIMIZATION & DECISION MAKING"
  },
  {
   "pgm_ref_code": "077E",
   "pgm_ref_txt": "SIMULATION MODELS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 149714.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our overarching goal is investigating design priciples for parallel simulation optimization (SO) environments. By this we mean, identifying (statistical) sampling strategies in SO algorithms that are executed within a parallel computing environment. Of interest to us are SO algorithms in the small finite (Ranking and Selection), large finite, local continuous, and global continuous contexts.&nbsp; It is important to recognize that our goal is not traditional parallel simulation, which is mostly about how operations&nbsp;<em>within</em>&nbsp;a simulation should be conducted so as to, say, increase efficiency. Instead, we treat simulation as a primitive, that is, something that can be called as an oracle to obtain stochastic function estimates at any point in the search space but whose structure cannot be altered.&nbsp;</p>\n<p>The answers to our investigations will fall broadly in two (related) categories: (i) strategies for distributing simulation replications&nbsp;<em>across the potentially large number of computing units</em>&nbsp;that are present, for particular algorithm and problem structures; and (ii) strategies for distributing simulation replications&nbsp;<em>across the search space</em>,&nbsp;for particular algorithm and problem structures. The latter category, of course, exists even in a single processor environment; more importantly, understanding (i) relies crucially on understanding (ii) in a single processor environment. Interestingly, (ii) is poorly understood even in a single processor environment except for the Ranking and Selection context. We envision that our investigations will lead to a broad theory which in turn will lead to specific methods, algorithms, and software for solving various flavors of SO problems executed within a parallel computing environment.</p>\n<p>A second goal of the project is to continue developing the library of SO problem  by expanding it to include a library of SO solvers. Such a library would host freely downlaodable SO solvers for various SO problem flavors. The website is meant only as a ``single stop\" for available modern solvers; it will provide no guarantees on the functioning or the efficacy of the solvers.</p>\n<pre>The following are signifcant results from the investigations that were undertaken.</pre>\n<pre>1. For large scale finite SO context, we have been able to fully characterize the nature of Monte Carlo <br />sampling within optimally evolving SO algorithms. Recall that \"optimally evolving\" means ensuring that the <br />probability of selecting an incorrect solution decays to zero at the fastest possible rate with respect to the <br />Monte Carlo budget. Importantly, our characterization leads to a general heuristic of sampling that we call <br />SCORE. SCORE essentially asks to sample a system (or a policy) in inverse proportion to its optimality gap <br />after appropriate standardization, and guarantees optimality of budget allocation as the Monte Carlo budget <br />becomes large. SCORE can incoporate expected value constraints as well, yielding something close to a <br />general philosophy of sampling for large scale finite SO.</pre>\n<pre>2. For integer nonconvex SO problems that involve stochastic constraints, we have devised an algorithm <br />called cg-RSPLINE which combines a search procedure that mimics Line Search along with a carefully<br /> constructed procedure involving constraint relaxations. Broadly speaking, cg-RSPLINE solves a sequence <br />of constraint-relaxed sampled problems using a procedure that mimics Line Search for deterministic optimization. <br />The constraint relaxations become progressively less stringent. The resulting sequence of solutions converge <br />with probability one to a global solution assuming that the intial solution for the sampled problem is drawn from an <br />appropriate distribution. cg-RSPLINE has been extensively tested and is arguably the most efficient algorithm of its <br />kind. </pre>\n<pre>3. The third (revised) objective of the project was the development of stochastic analogues of the Line Search and <br />Trust Region methods aimed at identifying critical points of a smooth function using only a zero-th order or a <br />first-order function oracle. Line Search and Trust Region methods are especially relevant because, over the last <br />few decades, they have become indispensable as techniques for numerical deterministic optimization. We have <br />designed ASGM and ASTRO --- \"stochastic versions\" of the Line Search and Trust Region methods, respectively, <br />that are provably convergent on stochastic optimization problems where the objective function is a mathematical <br />expectation that can only be estimated using observations from a large database or by executing a Monte Carlo <br />simulation.</pre>\n<pre>We have characterized the (sampling) complexity of both ASGM and ASTRO. Such characterization is <br />significant since both ASGM and ASTRO are adaptive sampling algorithms that adjust their sampling effort <br />according to their perceived proximity to a critical point. Analyzing adaptive algorithms has proven challenging <br />in the past. Versions of ASGM and ASTRO have been implemented in large scale optimization problems arising in <br />the context of vaccine allocation during the spread of an epidemic within a large city. (The particular case that was <br />implemented was for the city of Seattle.) A number of other ``toy\" implementations have been constructed.</pre>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/13/2017<br>\n\t\t\t\t\tModified by: Raghu&nbsp;Pasupathy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur overarching goal is investigating design priciples for parallel simulation optimization (SO) environments. By this we mean, identifying (statistical) sampling strategies in SO algorithms that are executed within a parallel computing environment. Of interest to us are SO algorithms in the small finite (Ranking and Selection), large finite, local continuous, and global continuous contexts.  It is important to recognize that our goal is not traditional parallel simulation, which is mostly about how operations within a simulation should be conducted so as to, say, increase efficiency. Instead, we treat simulation as a primitive, that is, something that can be called as an oracle to obtain stochastic function estimates at any point in the search space but whose structure cannot be altered. \n\nThe answers to our investigations will fall broadly in two (related) categories: (i) strategies for distributing simulation replications across the potentially large number of computing units that are present, for particular algorithm and problem structures; and (ii) strategies for distributing simulation replications across the search space, for particular algorithm and problem structures. The latter category, of course, exists even in a single processor environment; more importantly, understanding (i) relies crucially on understanding (ii) in a single processor environment. Interestingly, (ii) is poorly understood even in a single processor environment except for the Ranking and Selection context. We envision that our investigations will lead to a broad theory which in turn will lead to specific methods, algorithms, and software for solving various flavors of SO problems executed within a parallel computing environment.\n\nA second goal of the project is to continue developing the library of SO problem  by expanding it to include a library of SO solvers. Such a library would host freely downlaodable SO solvers for various SO problem flavors. The website is meant only as a ``single stop\" for available modern solvers; it will provide no guarantees on the functioning or the efficacy of the solvers.\nThe following are signifcant results from the investigations that were undertaken.\n1. For large scale finite SO context, we have been able to fully characterize the nature of Monte Carlo \nsampling within optimally evolving SO algorithms. Recall that \"optimally evolving\" means ensuring that the \nprobability of selecting an incorrect solution decays to zero at the fastest possible rate with respect to the \nMonte Carlo budget. Importantly, our characterization leads to a general heuristic of sampling that we call \nSCORE. SCORE essentially asks to sample a system (or a policy) in inverse proportion to its optimality gap \nafter appropriate standardization, and guarantees optimality of budget allocation as the Monte Carlo budget \nbecomes large. SCORE can incoporate expected value constraints as well, yielding something close to a \ngeneral philosophy of sampling for large scale finite SO.\n2. For integer nonconvex SO problems that involve stochastic constraints, we have devised an algorithm \ncalled cg-RSPLINE which combines a search procedure that mimics Line Search along with a carefully\n constructed procedure involving constraint relaxations. Broadly speaking, cg-RSPLINE solves a sequence \nof constraint-relaxed sampled problems using a procedure that mimics Line Search for deterministic optimization. \nThe constraint relaxations become progressively less stringent. The resulting sequence of solutions converge \nwith probability one to a global solution assuming that the intial solution for the sampled problem is drawn from an \nappropriate distribution. cg-RSPLINE has been extensively tested and is arguably the most efficient algorithm of its \nkind. \n3. The third (revised) objective of the project was the development of stochastic analogues of the Line Search and \nTrust Region methods aimed at identifying critical points of a smooth function using only a zero-th order or a \nfirst-order function oracle. Line Search and Trust Region methods are especially relevant because, over the last \nfew decades, they have become indispensable as techniques for numerical deterministic optimization. We have \ndesigned ASGM and ASTRO --- \"stochastic versions\" of the Line Search and Trust Region methods, respectively, \nthat are provably convergent on stochastic optimization problems where the objective function is a mathematical \nexpectation that can only be estimated using observations from a large database or by executing a Monte Carlo \nsimulation.\nWe have characterized the (sampling) complexity of both ASGM and ASTRO. Such characterization is \nsignificant since both ASGM and ASTRO are adaptive sampling algorithms that adjust their sampling effort \naccording to their perceived proximity to a critical point. Analyzing adaptive algorithms has proven challenging \nin the past. Versions of ASGM and ASTRO have been implemented in large scale optimization problems arising in \nthe context of vaccine allocation during the spread of an epidemic within a large city. (The particular case that was \nimplemented was for the city of Seattle.) A number of other ``toy\" implementations have been constructed.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/13/2017\n\n\t\t\t\t\tSubmitted by: Raghu Pasupathy"
 }
}