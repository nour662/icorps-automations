{
 "awd_id": "1228996",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EXP: Understanding the Transformative Potential of Spoken Assessments of Science Understanding for Young Learners",
 "cfda_num": "47.076",
 "org_code": "11010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Finbarr Sloane",
 "awd_eff_date": "2012-09-15",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 249567.0,
 "awd_amount": 249567.0,
 "awd_min_amd_letter_date": "2012-09-08",
 "awd_max_amd_letter_date": "2012-09-08",
 "awd_abstract_narration": "This exploratory proposal addresses a significant goal in the New Science Framework and one that was previously highlighted in the NRC report: \"Taking Science to School: Learning and Teaching Science in Grades K-8\".  The goal of the project is the teaching and learning of the norms of scientific argument, explanation, and the evaluation of evidence for productive participation in the discourses of science, starting in the early grades.  The investigators assert that even though some students do not read well, there may be significant scientific understanding which will go unrecognized unless alternative methods of evaluation are developed such as oral assessments.  Oral assessments by pedagogical agents which can elicit explanations from students and engage them in serious dialog are an alternate to oral assessments by humans. \r\n  \r\nThe project has two goals: 1) assess the benefits of incorporating spoken prompts into written assessments, and 2) investigate spoken dialogs between a student and a virtual or human teacher testing if they can produce detailed and accurate assessments of science understanding as well as the ability to verbalize complete and accurate science explanations. The site for this project is the Boulder Valley School District's low and mid-performing schools and its Summer Science Camp program for language-minority students.  FOSS, the Full Option Science System, a curriculum used by over 100,000 teachers and 2 million students, and the FOSS Summative ASK assessments will be used for the curriculum and to measure learning.  This effort builds on an earlier successful development by the same team of My Science Tutor (MyST), an intelligent tutoring system to improve science learning by third, fourth, and fifth grade students through spoken dialogs with Marni, a virtual science tutor in multimedia environments.  Elementary science will be the content area for this project as well. Primary organizations involved are Boulder Language Technologies, the Boulder Valley School District, and the University of Colorado.  It is expected that these exploratory studies will show that alternative forms of assessment are feasible and should be used more widely to raise the achievement level of both language-minority students and elementary students in general.  The evaluation will include an external expert review and regular critical review of the project?s methods and progress, analysis procedures, and interpretation of data into findings.\r\n\r\nThe proposed studies will expand the options of assessment conditions available for students in early grades, particularly those who are weak readers and writers and English language learners.  The project will expand the pool of students who can demonstrate their knowledge and understanding of science, addressing a critical need to expand the pool of students engaged in science learning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "EDU",
 "org_dir_long_name": "Directorate for STEM Education",
 "div_abbr": "DGE",
 "org_div_long_name": "Division Of Graduate Education",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wayne",
   "pi_last_name": "Ward",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Wayne H Ward",
   "pi_email_addr": "wayne.ward@colorado.edu",
   "nsf_id": "000264998",
   "pi_start_date": "2012-09-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Brandon",
   "pi_last_name": "Helding",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Brandon Helding",
   "pi_email_addr": "bhelding2@unl.edu",
   "nsf_id": "000585886",
   "pi_start_date": "2012-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Boulder Language Technologies",
  "inst_street_address": "2690 Center Green Ct S Ste 200",
  "inst_street_address_2": "",
  "inst_city_name": "Boulder",
  "inst_state_code": "CO",
  "inst_state_name": "Colorado",
  "inst_phone_num": "3035799605",
  "inst_zip_code": "803015406",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "CO02",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": "QGZ1JJJUDEZ8"
 },
 "perf_inst": {
  "perf_inst_name": "Boulder Language Technologies",
  "perf_str_addr": "2960 Center Green Court, STE 200",
  "perf_city_name": "Boulder",
  "perf_st_code": "CO",
  "perf_st_name": "Colorado",
  "perf_zip_code": "803015406",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "CO02",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "726100",
   "pgm_ele_name": "Project & Program Evaluation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9177",
   "pgm_ref_txt": "ELEMENTARY/SECONDARY EDUCATION"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0412",
   "app_name": "NSF Education & Human Resource",
   "app_symb_id": "040106",
   "fund_code": "04001213DB",
   "fund_name": "NSF Education & Human Resource",
   "fund_symb_id": "040106"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 249567.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">The major goal of the project was to learn whether poor readers produce more accurate answers to multiple choice science questions (MCQs) when they could listen to both the questions and answer choices read aloud.&nbsp; The main hypothesis tested during the study was that elementary students with low reading proficiency, would have difficulty reading questions and answer choices, and therefore would benefit from being able to listen to them.&nbsp; To test this hypothesis, we measured students&rsquo; reading proficiency using a standardized test, and compared students with lower and higher reading proficiency scores based on their answers to multiple choice questions presented in written form only, verses questions that students could both read and hear read to them.</p>\n<p class=\"Default\">&nbsp;</p>\n<p>Within this larger goal, we had two subgoals; they were to:</p>\n<ol>\n<li>develop a set of MCQs to assess students&rsquo; understanding of science curriculum taught in the Boulder Valley School District (BVSD) in Boulder County, Colorado for 1<sup>st</sup>, 2<sup>nd</sup>, 4<sup>th</sup>, and 5<sup>th</sup> grade students.</li>\n<li>modify an interactive multimedia assessment and learning tool, <em>MindStars Books</em>, to a) automatically assess students&rsquo; oral reading fluency (ORF), and b) present students with multiple choice questions, in both written only and written that could be read aloud.</li>\n</ol>\n<p><strong>Outcomes</strong></p>\n<p>The study developed two types of computer-administered MCQs presented to 1<sup>st</sup>, 2<sup>nd</sup>, 4<sup>th</sup>, and 5<sup>th</sup> grade students.&nbsp;&nbsp; One set of MCQs consisted of written only questions and answer choices.&nbsp; Students were instructed to read the question and answer choices, and select the best answer.&nbsp;&nbsp; A second set of MCQs consisted of written + spoken questions that were read aloud to the student, and written answer choices that the student could click on to hear read aloud.&nbsp; All students received both types of questions. Analyses that compared number of correct answers to the two types of MCQs indicated that students with the lowest reading scores provided more correct answers to written + spoken MCQs than students with the highest reading scores.&nbsp;</p>\n<p class=\"Pa4\">The <strong>intellectual merit</strong> of the proposed work was realized through the following specific results.</p>\n<ol>\n<li>We developed MCQs for 1<sup>st</sup> and 2<sup>nd</sup> grade life science content aligned with the BVSD 1<sup>st</sup> and 2<sup>nd</sup> grade curriculum, based on the Colorado State Standards published by the Colorado Department of Education.</li>\n<li>We developed an easy to use authoring tool for developing and presenting a) automatic assessment of children&rsquo;s ORF, a valid and reliable measure of children&rsquo;s reading proficiency for our purposes, and b) MCQs, with applicable graphics in both written only and written + spoken modes.&nbsp; Children found the MCQs easy to use, and most students said the test was &ldquo;fun.&rdquo;</li>\n<li>Our main analysis indicated there was no evidence for a statistically significant main effect associated with the type of MCQ (written only or written with spoken support), reading level, and the students responses to the MCQs (<em>F</em>(1,25) = 0.003, <em>p</em>=0.96).&nbsp; This was attributed to the lack of a sufficient number of poor readers in our sample; almost all of the children tested were above the 50<sup>th</sup> percentile in reading proficiency. When the 1/4<sup>th</sup> of students with the lowest reading proficiency were compared to 1/4<sup>th</sup> of students with the highest reading proficiency students with the lowest reading proficiency produced more correct responses to MCQs that they could listened to in 3 of the 4 grade levels.&nbsp; The results were reanalyzed with a more complex, multivariate...",
  "por_txt_cntn": "The major goal of the project was to learn whether poor readers produce more accurate answers to multiple choice science questions (MCQs) when they could listen to both the questions and answer choices read aloud.  The main hypothesis tested during the study was that elementary students with low reading proficiency, would have difficulty reading questions and answer choices, and therefore would benefit from being able to listen to them.  To test this hypothesis, we measured students\u00c6 reading proficiency using a standardized test, and compared students with lower and higher reading proficiency scores based on their answers to multiple choice questions presented in written form only, verses questions that students could both read and hear read to them.\n \n\nWithin this larger goal, we had two subgoals; they were to:\n\ndevelop a set of MCQs to assess students\u00c6 understanding of science curriculum taught in the Boulder Valley School District (BVSD) in Boulder County, Colorado for 1st, 2nd, 4th, and 5th grade students.\nmodify an interactive multimedia assessment and learning tool, MindStars Books, to a) automatically assess students\u00c6 oral reading fluency (ORF), and b) present students with multiple choice questions, in both written only and written that could be read aloud.\n\n\nOutcomes\n\nThe study developed two types of computer-administered MCQs presented to 1st, 2nd, 4th, and 5th grade students.   One set of MCQs consisted of written only questions and answer choices.  Students were instructed to read the question and answer choices, and select the best answer.   A second set of MCQs consisted of written + spoken questions that were read aloud to the student, and written answer choices that the student could click on to hear read aloud.  All students received both types of questions. Analyses that compared number of correct answers to the two types of MCQs indicated that students with the lowest reading scores provided more correct answers to written + spoken MCQs than students with the highest reading scores. \nThe intellectual merit of the proposed work was realized through the following specific results.\n\nWe developed MCQs for 1st and 2nd grade life science content aligned with the BVSD 1st and 2nd grade curriculum, based on the Colorado State Standards published by the Colorado Department of Education.\nWe developed an easy to use authoring tool for developing and presenting a) automatic assessment of children\u00c6s ORF, a valid and reliable measure of children\u00c6s reading proficiency for our purposes, and b) MCQs, with applicable graphics in both written only and written + spoken modes.  Children found the MCQs easy to use, and most students said the test was \"fun.\"\nOur main analysis indicated there was no evidence for a statistically significant main effect associated with the type of MCQ (written only or written with spoken support), reading level, and the students responses to the MCQs (F(1,25) = 0.003, p=0.96).  This was attributed to the lack of a sufficient number of poor readers in our sample; almost all of the children tested were above the 50th percentile in reading proficiency. When the 1/4th of students with the lowest reading proficiency were compared to 1/4th of students with the highest reading proficiency students with the lowest reading proficiency produced more correct responses to MCQs that they could listened to in 3 of the 4 grade levels.  The results were reanalyzed with a more complex, multivariate conceptualization of the research question.  In this case, we found a significant result associated with both question types and reading ability (Wilk\u00c6s &Lambda; (2,85) = 0.79, p&lt;0.01). \n\nThe broader impact of our work is the transformative potential of providing spoken language support to all students who have difficulty reading grade level texts.  This includes over 60% of students who score as not proficient readers on the 2016 National Assessments of Educational Progress.  It is expected that a replication of the st..."
 }
}