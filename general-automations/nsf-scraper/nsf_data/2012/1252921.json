{
 "awd_id": "1252921",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Data Association and Exploitation for Large Scale 3D Modeling from Visual Imagery",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924768",
 "po_email": "jyang@nsf.gov",
 "po_sign_block_name": "Jie Yang",
 "awd_eff_date": "2012-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 296480.0,
 "awd_amount": 296480.0,
 "awd_min_amd_letter_date": "2012-08-31",
 "awd_max_amd_letter_date": "2012-08-31",
 "awd_abstract_narration": "This project addresses three main challenges in large scale 3D modeling from photocollections: 1) efficient and complete data linkage, 2) Inference and modeling of scene and user dynamics, and 3) development of data adaptive algorithms. These  challenges are tackled within the framework of data association and exploitation. The benefits of such an approach are two fold. First, through enhanced data association, the approach increases the scope of 3D models due to more complete data linkage. Second, through the development of algorithms that are not only robust against (and mitigate) input data variability, but also explicitly designed to exploit this diversity and data richness, the approach increases fidelity of 3D models. The specific data association tasks of this project include:  location recognition, view planning for 3D reconstruction, online learning for feature matching, and modeling under scene symmetries. The specific data exploitation tasks of this project include: model update and archiving, native resolution modeling, high resolution dynamic texture estimation, and exploring and leveraging user behavior. \r\n\r\nThis work enables the broad deployment of applications where fully automated scene modeling expands from determining structure properties to encompass the modeling of observable behavioral patterns both in the scene and in the user controlled image capture process. The developed technologies have a wide range of applications, from virtual tourism, to cultural heritage preservation, to disaster response.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jan-Michael",
   "pi_last_name": "Frahm",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jan-Michael Frahm",
   "pi_email_addr": "jmf@cs.unc.edu",
   "nsf_id": "000427356",
   "pi_start_date": "2012-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Enrique",
   "pi_last_name": "Dunn",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Enrique Dunn",
   "pi_email_addr": "edunn@stevens.edu",
   "nsf_id": "000623882",
   "pi_start_date": "2012-08-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of North Carolina at Chapel Hill",
  "inst_street_address": "104 AIRPORT DR STE 2200",
  "inst_street_address_2": "",
  "inst_city_name": "CHAPEL HILL",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9199663411",
  "inst_zip_code": "275995023",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL",
  "org_prnt_uei_num": "D3LHU66KBLD5",
  "org_uei_num": "D3LHU66KBLD5"
 },
 "perf_inst": {
  "perf_inst_name": "University of North Carolina at Chapel Hill",
  "perf_str_addr": "201 S Columbia St",
  "perf_city_name": "Chapel Hill",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "275993175",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": null,
 "pgm_ref": [
  {
   "pgm_ref_code": "170E",
   "pgm_ref_txt": "Interagency Agreements"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 296480.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In this research effort we advanced the state of the art in modeling from Internet photo collections, i.e. creating a 3D model of the scenes and its semantic scene elements using just the 2D photos of a scene found on the Internet. Moreover, we also leveraged these photos for localization of newly obtained photos. Our method utilizes geometric consistency as a mid-level visual similarity cue to create a visual index of the obtained geo-located images. The attained data associations are leveraged as a means to infer semantic relationship among dataset elements. The characterization of the image content in terms of the geometric and semantic elements observed in scene then provide a general framework for both identifying and managing data association in large scale photo collections. We researched complementary data abstractions into a single framework by focusing on two main research topics:</p>\n<p>1. Determining the geographic location where an image was taken by comparing it against a large database of geo-located urban imagery. To achieve this our research tackled the challenge of balancing both search completeness and computational tractability by proposing novel data association algorithms and indexing schemes.</p>\n<p>2. We also incorporated geometric structure estimates attained from large photo-collections or ground reconnaissance video/photos as a means to identify and recognize semantically meaningful elements within the reconstructed 3D-environment.</p>\n<p>Our research leveraged the use geometric consistency as a visual data association primitive in order to introduce the concept of structural and semantic indexing within the developmed internet scale photo collection analysis systems. Moreover, by combining the complementary data abstraction levels of geometrical structure and semantic context we developed a more efficient and robust data organization framework with applicability well beyond the studied test application of urban geo-localization.</p>\n<p>We investigated methods to improve the performance of large-scale Structure-from-Motion systems by reducing the major computational effort typically spend on pairwise image matching and geometric verification. We researched a new method (called PAIGE) to classify potiential scene overlap of image pairs by using its global motion pattern semantic. This improves the effectiveness of discovering connected components in large-scale, unordered Internet image collections. Our method learns to efficiently identify image pairs with scene overlap based on our novel image appearance and topology descriptor without the need to perform exhaustive putative matching and geometric verification. PAIGE achieves state-of-the-art performance and integrates well into most of the existing Structure-from-Motion pipelines concepts.</p>\n<p>Our team researched a novel large-scale, structure-from-motion framework that enables scalable modeling from world scale dataset of tens of millions of unordered Internet images. In particular our method is able to perform modeling on a single PC instead of cloud computing platforms. Our system employs the global topology of the appearance features of each scene's photos to build a scene index. This index enables a highly efficient grouping of Internet photos that leads to highly efficient to a linear complexity connected component discovery, which is a critical element for scalability. The researched system was tested and developed on the first world scale data set of 100 million image crowd-sourced photo collection containing images of users from the entire world.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2015<br>\n\t\t\t\t\tModified by: Jan-Michael&nbsp;Frahm</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"...",
  "por_txt_cntn": "\nIn this research effort we advanced the state of the art in modeling from Internet photo collections, i.e. creating a 3D model of the scenes and its semantic scene elements using just the 2D photos of a scene found on the Internet. Moreover, we also leveraged these photos for localization of newly obtained photos. Our method utilizes geometric consistency as a mid-level visual similarity cue to create a visual index of the obtained geo-located images. The attained data associations are leveraged as a means to infer semantic relationship among dataset elements. The characterization of the image content in terms of the geometric and semantic elements observed in scene then provide a general framework for both identifying and managing data association in large scale photo collections. We researched complementary data abstractions into a single framework by focusing on two main research topics:\n\n1. Determining the geographic location where an image was taken by comparing it against a large database of geo-located urban imagery. To achieve this our research tackled the challenge of balancing both search completeness and computational tractability by proposing novel data association algorithms and indexing schemes.\n\n2. We also incorporated geometric structure estimates attained from large photo-collections or ground reconnaissance video/photos as a means to identify and recognize semantically meaningful elements within the reconstructed 3D-environment.\n\nOur research leveraged the use geometric consistency as a visual data association primitive in order to introduce the concept of structural and semantic indexing within the developmed internet scale photo collection analysis systems. Moreover, by combining the complementary data abstraction levels of geometrical structure and semantic context we developed a more efficient and robust data organization framework with applicability well beyond the studied test application of urban geo-localization.\n\nWe investigated methods to improve the performance of large-scale Structure-from-Motion systems by reducing the major computational effort typically spend on pairwise image matching and geometric verification. We researched a new method (called PAIGE) to classify potiential scene overlap of image pairs by using its global motion pattern semantic. This improves the effectiveness of discovering connected components in large-scale, unordered Internet image collections. Our method learns to efficiently identify image pairs with scene overlap based on our novel image appearance and topology descriptor without the need to perform exhaustive putative matching and geometric verification. PAIGE achieves state-of-the-art performance and integrates well into most of the existing Structure-from-Motion pipelines concepts.\n\nOur team researched a novel large-scale, structure-from-motion framework that enables scalable modeling from world scale dataset of tens of millions of unordered Internet images. In particular our method is able to perform modeling on a single PC instead of cloud computing platforms. Our system employs the global topology of the appearance features of each scene's photos to build a scene index. This index enables a highly efficient grouping of Internet photos that leads to highly efficient to a linear complexity connected component discovery, which is a critical element for scalability. The researched system was tested and developed on the first world scale data set of 100 million image crowd-sourced photo collection containing images of users from the entire world.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/29/2015\n\n\t\t\t\t\tSubmitted by: Jan-Michael Frahm"
 }
}