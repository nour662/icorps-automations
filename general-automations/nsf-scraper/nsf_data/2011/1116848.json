{
 "awd_id": "1116848",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Architecture-based Run-time Fault Diagnosis",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 450000.0,
 "awd_amount": 450000.0,
 "awd_min_amd_letter_date": "2011-08-03",
 "awd_max_amd_letter_date": "2011-08-03",
 "awd_abstract_narration": "Systems today must cope with failures induced by many factors outside the control of the organization producing the software: faults in infrastructure and components developed by third-parties, unpredictable loads, and variable resources. Modern systems must therefore take increasing responsibility for problem detection and repair at runtime. Effective fault detection and repair could be greatly enhanced by run-time fault diagnosis and localization -- the ability to identify the source of problem so that appropriate actions can be taken either by a human operator or automated mechanisms to repair the system. \r\n\r\nIn this research we are developing new foundations for run-time fault diagnosis and localization. To do this we are extending and synthesizing recent advances in two areas. The first is the use of architecture models for monitoring a system at run-time. The second is the use of spectrum-based reasoning for fault localization (SFL). SFL is a lightweight technique that takes as its input a form of trace abstraction and produces a list of likely fault candidates, ordered by probability of being the true fault explanation. It has been used with impressive results during design time but thus far has not been exploited at runtime in the context of architecture-based monitoring and diagnosis. \r\n\r\nThis research will improve the trustworthiness and robustness of modern software systems by providing new techniques for diagnosing faults while a system is running, thereby providing an improved basis for fault detection and resolution.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Garlan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "David Garlan",
   "pi_email_addr": "garlan@cs.cmu.edu",
   "nsf_id": "000437302",
   "pi_start_date": "2011-08-03",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 450000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Robustness, or the ability to continue operation in the presence of faults, is an increasingly important concern for trustworthy software systems. Software is involved in the day-to-day operation of most aspects of society, for example in managing business processes, critical infrastructure, financial transactions and transport and factory automation. While methods for designing software have been helpful in improving confidence in that software, systems today must cope with failures induced by a variety of factors largely outside the control of the organization producing the software, e.g., faults in infrastructure on which the system runs and components provided by third-parties, unpredictable loads, and varying resources. Systems must therefore take increasing responsibility for their own problem detection and repair while they operate (i.e., at run time).</p>\n<p>Fault detection and repair could be greatly enhanced by run-time fault diagnosis and localization: i.e., the ability to identify the source of a problem so that appropriate actions can be taken either by a human operator or automated mechanisms to repair the system. Run-time diagnosis for today&rsquo;s complex systems, however, is particularly challenging. First, the presence of concurrency (where many computations are happening in the system at the same time) makes it difficult to identify which of many possible computations might have caused a problem. Second, reliance on middleware for distribution, and more generally the use of components and infrastructure produced by many organizations, means that in many cases code for all parts of the system is not available. Third, in many systems problems may be intermittent, caused by transient faults or variability in loads. Fourth many &ldquo;faults&rdquo; that we care about are reflected by violation of a system&rsquo;s qualities such as how long it takes to respond, or the cost to operate, rather than by a direct failure such as a server or system crash. Such &ldquo;softer&rdquo; faults are more difficult to detect. Consequently, while fault diagnosis has been studied extensively for both hardware and software systems as a development-time activity, the ability to do this at run time for complex systems has remained an elusive goal.</p>\n<p>In this project, we addressed the challenges of providing run-time fault diagnosis and localization along the following thrusts:</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We adapted a fault localization algorithm that has previously been used during testing for use at run time. This involved developing techniques for (a) specifying and monitoring behaviors in a running system using an event processing approach that abstracts system level events to architecture level behaviors, (b) determining an optimal window of observations that would allow us to have high confidence in the results of the fault localization algorithm, which uses statistical analysis and therefore requires a number of observations, (c) detecting behavior failures, that include soft faults in addition to outright failure, to handle the failure to meet certain quality standards or service level agreements.</p>\n<p>-&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; We developed formal models that allow us to reason about the sufficiency of information for diagnosis that allow the algorithm to work in real systems. Specifically, we developed (a) an unobservability theory that allows us to set thresholds to provide information about parts of the system that we cannot observe at run time (for example, parts of the system that are provided by third parties or are critical to the operation of the system) and (b) a theory of probe placement that allows us to reason about the optimal set of probes or monitors that need to be inserted into a system to still allow us to diagnose and repair problems</p>\n<p>We applied our theories to a number of case studies, including softwar...",
  "por_txt_cntn": "\nRobustness, or the ability to continue operation in the presence of faults, is an increasingly important concern for trustworthy software systems. Software is involved in the day-to-day operation of most aspects of society, for example in managing business processes, critical infrastructure, financial transactions and transport and factory automation. While methods for designing software have been helpful in improving confidence in that software, systems today must cope with failures induced by a variety of factors largely outside the control of the organization producing the software, e.g., faults in infrastructure on which the system runs and components provided by third-parties, unpredictable loads, and varying resources. Systems must therefore take increasing responsibility for their own problem detection and repair while they operate (i.e., at run time).\n\nFault detection and repair could be greatly enhanced by run-time fault diagnosis and localization: i.e., the ability to identify the source of a problem so that appropriate actions can be taken either by a human operator or automated mechanisms to repair the system. Run-time diagnosis for today\u00c6s complex systems, however, is particularly challenging. First, the presence of concurrency (where many computations are happening in the system at the same time) makes it difficult to identify which of many possible computations might have caused a problem. Second, reliance on middleware for distribution, and more generally the use of components and infrastructure produced by many organizations, means that in many cases code for all parts of the system is not available. Third, in many systems problems may be intermittent, caused by transient faults or variability in loads. Fourth many \"faults\" that we care about are reflected by violation of a system\u00c6s qualities such as how long it takes to respond, or the cost to operate, rather than by a direct failure such as a server or system crash. Such \"softer\" faults are more difficult to detect. Consequently, while fault diagnosis has been studied extensively for both hardware and software systems as a development-time activity, the ability to do this at run time for complex systems has remained an elusive goal.\n\nIn this project, we addressed the challenges of providing run-time fault diagnosis and localization along the following thrusts:\n\n-       We adapted a fault localization algorithm that has previously been used during testing for use at run time. This involved developing techniques for (a) specifying and monitoring behaviors in a running system using an event processing approach that abstracts system level events to architecture level behaviors, (b) determining an optimal window of observations that would allow us to have high confidence in the results of the fault localization algorithm, which uses statistical analysis and therefore requires a number of observations, (c) detecting behavior failures, that include soft faults in addition to outright failure, to handle the failure to meet certain quality standards or service level agreements.\n\n-       We developed formal models that allow us to reason about the sufficiency of information for diagnosis that allow the algorithm to work in real systems. Specifically, we developed (a) an unobservability theory that allows us to set thresholds to provide information about parts of the system that we cannot observe at run time (for example, parts of the system that are provided by third parties or are critical to the operation of the system) and (b) a theory of probe placement that allows us to reason about the optimal set of probes or monitors that need to be inserted into a system to still allow us to diagnose and repair problems\n\nWe applied our theories to a number of case studies, including software that controls a computer chip fabrication factory, and web-based technology that is deployed on the cloud. We were able to localize errors happening in real time in a system with many co..."
 }
}