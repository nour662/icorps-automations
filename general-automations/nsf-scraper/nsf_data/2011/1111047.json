{
 "awd_id": "1111047",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Using Gaze Cues to Build Partner Models for Collaborative Behavior",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 749999.0,
 "awd_amount": 749999.0,
 "awd_min_amd_letter_date": "2011-08-09",
 "awd_max_amd_letter_date": "2012-07-26",
 "awd_abstract_narration": "The ability to interact remotely over the Internet is redefining the nature of collaboration.  Many collaborative activities require coordinating attention and action with another person moment-by-moment; without the benefit of being physically present with another person these sorts of collaborations are difficult to conduct efficiently.  This project explores using human eye gaze to create partner models for mediating time-critical collaborative activities.  A partner model is a dynamically learned description of what a partner is trying to do - for example, what someone may be looking for, or what they consider to be relevant within a task. \r\n\r\nIntellectual Merit: Many tasks and events are implicit or poorly defined, requiring that partner models be learned from evidence unfolding as part of a person's ongoing behavior.  Eye trackers will be used to determine the task-relevant objects that a person chooses to look at (and not look at); through analysis of these gaze patterns and the properties of the objects, human and computer partners will learn a model of what this person is attempting to do.  Various tasks will be explored, such as searching for a new and/or ambiguous moving target specified only by incomplete semantic descriptions, or monitoring a complex dynamic environment for unusual events, defined by atypical target movements and relationships between people and objects.  The findings will advance the fields of human-computer interaction, psycholinguistics, artificial intelligence, object and event detection by humans and computers, and multimodal human communication.\r\n\r\nBroader Impacts:  The results of the project will facilitate the development of new tools that can help people with their tasks, by, for example, finding and highlighting objects in a scene that match the viewer's goals and helping the viewer track moving targets.  The results will also lead to new tools for remote collaboration, with the goal being to make coordination at a distance as efficient as face-to-face interaction.  The tools and techniques from the project are expected to benefit a variety of applications, including the development of assistive technologies for people with communication impairments and the creation of better security screening procedures.  The project will provide training and research experiences for Stony Brook University's racially, ethnically, and economically diverse students, including women and others underrepresented in science and engineering.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gregory",
   "pi_last_name": "Zelinsky",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Gregory J Zelinsky",
   "pi_email_addr": "gregory.zelinsky@stonybrook.edu",
   "nsf_id": "000209271",
   "pi_start_date": "2011-08-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Susan",
   "pi_last_name": "Brennan",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Susan E Brennan",
   "pi_email_addr": "susan.brennan@stonybrook.edu",
   "nsf_id": "000106933",
   "pi_start_date": "2011-08-09",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Dimitrios",
   "pi_last_name": "Samaras",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Dimitrios Samaras",
   "pi_email_addr": "samaras@cs.sunysb.edu",
   "nsf_id": "000096125",
   "pi_start_date": "2011-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Stony Brook",
  "inst_street_address": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "inst_street_address_2": "",
  "inst_city_name": "STONY BROOK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6316329949",
  "inst_zip_code": "117940001",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NY01",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "M746VC6XMNH9",
  "org_uei_num": "M746VC6XMNH9"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Stony Brook",
  "perf_str_addr": "W5510 FRANKS MELVILLE MEMORIAL LIBRARY",
  "perf_city_name": "STONY BROOK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "117940001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NY01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "795300",
   "pgm_ele_name": "SOCIAL-COMPUTATIONAL SYSTEMS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7953",
   "pgm_ref_txt": "SOCIAL-COMPUTATIONAL SYSTEMS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 250079.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 499920.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>While once largely limited to face-to-face meetings, collaboration is increasingly being mediated by systems designed to allow remotely located partners to efficiently exchange ideas. With this rise of electronically-mediated collaboration comes the potential to change the collaborative experience. Systems need not be limited to basic video-conferencing or document exchange functions, they can become active agents in the collaborative process, able to learn the idiosyncrasies of different partners, and the task that they are trying to perform, and to actively suggest ways to improve collaboration efficiency.&nbsp; This project investigated specifically the use of gaze behavior for partner modeling--the dynamic accrual of information about what a partner is looking for or trying to do--to create an intelligent and partner-specific system for coordinating group behavior and mediating remote collaboration. Among the many intellectual merits stemming from this project were: (1) further refinements of a shared-gaze system that allows remotely-located collaborating partners to each see where the other is looking, with this information visualized as a gaze cursor superimposed over a commonly-viewed task screen, (2) a study investigating how eye gaze can be used to reveal one person's inferences about another person's intentions or reactions, (3) the development of a computational method for decoding the patterns of eye fixations that a person makes on arrays of objects to classify the category of target that they are searching for, (4) a related computational method for decoding patterns of fixations during scene viewing to classify a role or perspective that was assigned to a given viewer, (5) the development of a graphics&nbsp; tool for simulating a crowd of animated people moving through a virtual shopping mall for the purpose of studying how an intelligent collaborative system might learn deviations from predicable movements that might indicate suspicious behavior, (6) a study investigating people's ability to track multiple moving objects, particularly with respect to forming predictions about motion paths, and (7) a functioning human-computer interactive system that uses information about where a person is looking to automatically recognize and label the objects appearing in a scene or video.&nbsp;The broader impacts of this work will be a greater understanding of how shared gaze can be used to mediate joint attention in collaboration and coordinated behavior, and how these gaze cues can be learned and visualized to optimize task performance.&nbsp;This work will also inform the integration of gaze cues in assistive technologies and in human-computer interaction, more broadly.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/01/2015<br>\n\t\t\t\t\tModified by: Gregory&nbsp;J&nbsp;Zelinsky</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWhile once largely limited to face-to-face meetings, collaboration is increasingly being mediated by systems designed to allow remotely located partners to efficiently exchange ideas. With this rise of electronically-mediated collaboration comes the potential to change the collaborative experience. Systems need not be limited to basic video-conferencing or document exchange functions, they can become active agents in the collaborative process, able to learn the idiosyncrasies of different partners, and the task that they are trying to perform, and to actively suggest ways to improve collaboration efficiency.  This project investigated specifically the use of gaze behavior for partner modeling--the dynamic accrual of information about what a partner is looking for or trying to do--to create an intelligent and partner-specific system for coordinating group behavior and mediating remote collaboration. Among the many intellectual merits stemming from this project were: (1) further refinements of a shared-gaze system that allows remotely-located collaborating partners to each see where the other is looking, with this information visualized as a gaze cursor superimposed over a commonly-viewed task screen, (2) a study investigating how eye gaze can be used to reveal one person's inferences about another person's intentions or reactions, (3) the development of a computational method for decoding the patterns of eye fixations that a person makes on arrays of objects to classify the category of target that they are searching for, (4) a related computational method for decoding patterns of fixations during scene viewing to classify a role or perspective that was assigned to a given viewer, (5) the development of a graphics  tool for simulating a crowd of animated people moving through a virtual shopping mall for the purpose of studying how an intelligent collaborative system might learn deviations from predicable movements that might indicate suspicious behavior, (6) a study investigating people's ability to track multiple moving objects, particularly with respect to forming predictions about motion paths, and (7) a functioning human-computer interactive system that uses information about where a person is looking to automatically recognize and label the objects appearing in a scene or video. The broader impacts of this work will be a greater understanding of how shared gaze can be used to mediate joint attention in collaboration and coordinated behavior, and how these gaze cues can be learned and visualized to optimize task performance. This work will also inform the integration of gaze cues in assistive technologies and in human-computer interaction, more broadly. \n\n\t\t\t\t\tLast Modified: 11/01/2015\n\n\t\t\t\t\tSubmitted by: Gregory J Zelinsky"
 }
}