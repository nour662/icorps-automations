{
 "awd_id": "1118260",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Innovative Methods for Computational Workflow Optimization",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Daniel Katz",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 267342.0,
 "awd_amount": 267342.0,
 "awd_min_amd_letter_date": "2011-08-19",
 "awd_max_amd_letter_date": "2011-08-19",
 "awd_abstract_narration": "We will explore in this EAGER proposal new methodologies for matching algorithms and code to architecture, allowing the efficient execution of large complex workflows using an ensemble of available computing architectures ranging from distributed memory multi-core supercomputers to graphic processing units and high performance data intensive computing devices. In this exploratory work we will focus on the matching of computer architectures to computational needs and a tuning of the methodology of Uncertainty Quantification to the architectures at hand. We will first instrument our suite of tools to record computational, memory, data and energy usage. This data will then be analyzed and modeled to create guidelines for prediction of effective and efficient usage. The facilities available to us at the University at Buffalo, Center for Computation Research (CCR) and on the xD framework of resource providers will allow us to conduct this investigation.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Abani",
   "pi_last_name": "Patra",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Abani K Patra",
   "pi_email_addr": "abani.patra@tufts.edu",
   "nsf_id": "000395619",
   "pi_start_date": "2011-08-19",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "SUNY at Buffalo",
  "inst_street_address": "520 LEE ENTRANCE STE 211",
  "inst_street_address_2": "",
  "inst_city_name": "AMHERST",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "7166452634",
  "inst_zip_code": "142282577",
  "inst_country_name": "United States",
  "cong_dist_code": "26",
  "st_cong_dist_code": "NY26",
  "org_lgl_bus_name": "THE RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "GMZUKXFDJMA9",
  "org_uei_num": "LMCJKRFW5R81"
 },
 "perf_inst": {
  "perf_inst_name": "SUNY at Buffalo",
  "perf_str_addr": "520 LEE ENTRANCE STE 211",
  "perf_city_name": "AMHERST",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "142282577",
  "perf_ctry_code": "US",
  "perf_cong_dist": "26",
  "perf_st_cong_dist": "NY26",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  },
  {
   "pgm_ele_code": "800400",
   "pgm_ele_name": "Software Institutes"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7798",
   "pgm_ref_txt": "SOFTWARE & HARDWARE FOUNDATION"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "7942",
   "pgm_ref_txt": "HIGH-PERFORMANCE COMPUTING"
  },
  {
   "pgm_ref_code": "8004",
   "pgm_ref_txt": "Software Institutes"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 267342.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Intellectual Merit</p>\n<p>The project focused on developing new methods for optimizing computational and data analytics workflows for analysis of the outcomes of large-scale simulations. Such complex inference from simulation ensembles used in Uncertainty Quantification leads to twin computational challenges of managing large amount of data and performing cpu intensive computing. While algorithmic innovations using surrogates, localization and parallelization can make the problem feasible one still has very large data and compute tasks. The problem of dealing with large data gets compounded when data warehousing and data mining are intertwined with computationally expensive tasks. We developed here an approach to solving this problem by using a mix of hardware suitable for each task in a carefully orchestrated workow.</p>\n<p>&nbsp;</p>\n<p>In particular, we focused on the problem of estimating risk from volcanic mass flow hazards using large ensembles of simulations (O(2000) simulations) each of which generates greater than 2GB of data. The sample computing environment is essentially an integration of Netezza database and high performance cluster. It is based on the simple idea of segregating the data intensive and compute intensive tasks and assigning the right architecture for them. The computing model and the new computational scheme were adopted to generate probabilistic hazard maps in time that was order of magnitude faster than earlier practices &ndash; from a week to a day. We also worked with cheaper more readily available hardware platforms wherein the savings were still significant.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>Broader Impacts</p>\n<p>While the volcanic hazard analysis tools directly developed in this effort s will help emergency managers the generated methods for uncertainty quantification can benefit many other types of hazards from floods to hurricanes and even failure analysis calculations. During the project we supported 3 graduates students including one female student in the course of their doctoral studies. Work was disseminated not only at major national meetings but also through customized training to students and post-doctoral fellows.&nbsp; During the project we also hosted students from France, Columbia, Japan and China.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/29/2014<br>\n\t\t\t\t\tModified by: Abani&nbsp;K&nbsp;Patra</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2014/1118260/1118260_10124538_1419889446811_Workflow2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2014/1118260/1118260_10124538_1419889446811_Workflow2--rgov-800width.jpg\" title=\"Workflow for hazard analysis\"><img src=\"/por/images/Reports/POR/2014/1118260/1118260_10124538_1419889446811_Workflow2--rgov-66x44.jpg\" alt=\"Workflow for hazard analysis\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Sample workflow designed to exploit high performance computing and analytics hardware</div>\n<div class=\"imageCredit\">Abani Patra</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Abani&nbsp;K&nbsp;Patra</div>\n<div class=\"imageTitle\">Workflow for hazard analysis</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2014/1118260/1118260_10124538_1419889646325_Max_pile_height--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2014/1118260/1118260_10124538_1419889646325_Max_pile_height--rgov-800width.jpg\" title=\"Single...",
  "por_txt_cntn": "\nIntellectual Merit\n\nThe project focused on developing new methods for optimizing computational and data analytics workflows for analysis of the outcomes of large-scale simulations. Such complex inference from simulation ensembles used in Uncertainty Quantification leads to twin computational challenges of managing large amount of data and performing cpu intensive computing. While algorithmic innovations using surrogates, localization and parallelization can make the problem feasible one still has very large data and compute tasks. The problem of dealing with large data gets compounded when data warehousing and data mining are intertwined with computationally expensive tasks. We developed here an approach to solving this problem by using a mix of hardware suitable for each task in a carefully orchestrated workow.\n\n \n\nIn particular, we focused on the problem of estimating risk from volcanic mass flow hazards using large ensembles of simulations (O(2000) simulations) each of which generates greater than 2GB of data. The sample computing environment is essentially an integration of Netezza database and high performance cluster. It is based on the simple idea of segregating the data intensive and compute intensive tasks and assigning the right architecture for them. The computing model and the new computational scheme were adopted to generate probabilistic hazard maps in time that was order of magnitude faster than earlier practices &ndash; from a week to a day. We also worked with cheaper more readily available hardware platforms wherein the savings were still significant.\n\n \n\n \n\nBroader Impacts\n\nWhile the volcanic hazard analysis tools directly developed in this effort s will help emergency managers the generated methods for uncertainty quantification can benefit many other types of hazards from floods to hurricanes and even failure analysis calculations. During the project we supported 3 graduates students including one female student in the course of their doctoral studies. Work was disseminated not only at major national meetings but also through customized training to students and post-doctoral fellows.  During the project we also hosted students from France, Columbia, Japan and China.\n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/29/2014\n\n\t\t\t\t\tSubmitted by: Abani K Patra"
 }
}