{
 "awd_id": "1117128",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Small: Collaborative Research:Algorithms and Information-Theoretic Limits for Data-Limited Inference",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": "7032922981",
 "po_email": "pregalia@nsf.gov",
 "po_sign_block_name": "Phillip Regalia",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 240000.0,
 "awd_amount": 240000.0,
 "awd_min_amd_letter_date": "2011-08-09",
 "awd_max_amd_letter_date": "2011-08-09",
 "awd_abstract_narration": "It is an irony of our time that despite living in the 'information age' we are often data-limited. After decades of research, scientists still debate the causes and effects of climate change, and recent work has shown that a significant fraction of the most influential medical studies over the past 13 years have been subsequently found to be inaccurate, largely due to insufficient data. One reason for this apparent paradox is that modeling complex, real-world information sources requires rich probabilistic models that cannot be accurately learned even from very large data sets. On a deeper level, research inherently resides at the edge of the possible, and seeks to address questions that available data can only partially answer. It is therefore reasonable to expect that we will always be data-limited.\r\n\r\nThis research involves developing new algorithms and performance bounds for data-limited inference. Prior work of the PIs has shown that, by taking an information-theoretic approach, one can develop new algorithms that are tailored specifically to the data-limited regime and perform better than was previously known, and in some cases are provably optimal. This project advances the goal of developing a general theory for data-limited inference by considering a suite of problems spanning multiple application areas, such as classification; determining whether two data sets were generated by the same distribution or by different distributions; distribution estimation from event timings; entropy estimation; and communication over complex and unknown channels. Whereas these problems have all been studied before in isolation, prior work of the PIs has shown it is fruitful to view them as instances of the same underlying problem: data-limited inference.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Aaron",
   "pi_last_name": "Wagner",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Aaron B Wagner",
   "pi_email_addr": "wagner@ece.cornell.edu",
   "nsf_id": "000332581",
   "pi_start_date": "2011-08-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Cornell University",
  "inst_street_address": "341 PINE TREE RD",
  "inst_street_address_2": "",
  "inst_city_name": "ITHACA",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "6072555014",
  "inst_zip_code": "148502820",
  "inst_country_name": "United States",
  "cong_dist_code": "19",
  "st_cong_dist_code": "NY19",
  "org_lgl_bus_name": "CORNELL UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "G56PUALJ3KT5"
 },
 "perf_inst": {
  "perf_inst_name": "Cornell University",
  "perf_str_addr": "341 PINE TREE RD",
  "perf_city_name": "ITHACA",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "148502820",
  "perf_ctry_code": "US",
  "perf_cong_dist": "19",
  "perf_st_cong_dist": "NY19",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 240000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><div class=\"page\" title=\"Page 12\">\n<div class=\"layoutArea\">\n<div class=\"column\">\n<p><span>It is well accepted that we live in an age marked by prevalence of data.Yet, paradoxically, many of the problems that society confronts arise not from having too much data but from having too little.&nbsp;</span>Research in medicine, climate change, natural language processing, neuroscience, and many other areas is continually hindered by a lack of adequate data. A number of important controversies, from the effect on humankind's effect on the environment to the source of the rise of autism or gluten intolerance, could be easily settled by better data.</p>\n<p>This project considered how one can make better inference from limited data, in a sense \"squeezing\" as much information out of existing datasets as possible. Our prior work showed that traditional ways of making inference from data sets was unduly conservative. Using modern techniques, it is possible in some cases to make inferences with a high level of confidence that in some prior situations would have been viewed as impossible.</p>\n<p>The research focused in particular on natural language data sets. Automatically inferring authorship or subject matter by the relative frequency of different words in a document is notoriously difficult because even in large documents a large number of words appear rarely. Each word, taken invidually, does not exhibit a sufficiently large sample size to make inferences. This reseach showed, however, that taken together, a large number of rare words can be used to make authorship or subject matter decisions with a level of certainty that can be proven to be high mathematically. Using the same mathematical tools, we also proved mathematically that the existing ways in which certain wireless systems operate are optimal, so that it is fruitless to try to improve them.&nbsp;</p>\n<p>The project also led to a development of several course modules on modeling. Modeling has historically been an undertaught art in electrical engineering. As part of this module, we developed a text compression contest as part of a graduate information theory course. The students in the course are given a large sample of text, say from a novel, and their assignment is to build the shortest possible computer program that outputs the given text. This exercise is essentially one of modeling English text; the best models result in the most compression. The project also supported both a woman and an underrepresented minority student during their Ph.D. studies.</p>\n</div>\n</div>\n</div><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/16/2014<br>\n\t\t\t\t\tModified by: Aaron&nbsp;B&nbsp;Wagner</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\n\n\n\nIt is well accepted that we live in an age marked by prevalence of data.Yet, paradoxically, many of the problems that society confronts arise not from having too much data but from having too little. Research in medicine, climate change, natural language processing, neuroscience, and many other areas is continually hindered by a lack of adequate data. A number of important controversies, from the effect on humankind's effect on the environment to the source of the rise of autism or gluten intolerance, could be easily settled by better data.\n\nThis project considered how one can make better inference from limited data, in a sense \"squeezing\" as much information out of existing datasets as possible. Our prior work showed that traditional ways of making inference from data sets was unduly conservative. Using modern techniques, it is possible in some cases to make inferences with a high level of confidence that in some prior situations would have been viewed as impossible.\n\nThe research focused in particular on natural language data sets. Automatically inferring authorship or subject matter by the relative frequency of different words in a document is notoriously difficult because even in large documents a large number of words appear rarely. Each word, taken invidually, does not exhibit a sufficiently large sample size to make inferences. This reseach showed, however, that taken together, a large number of rare words can be used to make authorship or subject matter decisions with a level of certainty that can be proven to be high mathematically. Using the same mathematical tools, we also proved mathematically that the existing ways in which certain wireless systems operate are optimal, so that it is fruitless to try to improve them. \n\nThe project also led to a development of several course modules on modeling. Modeling has historically been an undertaught art in electrical engineering. As part of this module, we developed a text compression contest as part of a graduate information theory course. The students in the course are given a large sample of text, say from a novel, and their assignment is to build the shortest possible computer program that outputs the given text. This exercise is essentially one of modeling English text; the best models result in the most compression. The project also supported both a woman and an underrepresented minority student during their Ph.D. studies.\n\n\n\n\n\t\t\t\t\tLast Modified: 11/16/2014\n\n\t\t\t\t\tSubmitted by: Aaron B Wagner"
 }
}