{
 "awd_id": "1059218",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  CI-ADDO-EN:  Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 368205.0,
 "awd_amount": 368205.0,
 "awd_min_amd_letter_date": "2011-07-27",
 "awd_max_amd_letter_date": "2016-06-14",
 "awd_abstract_narration": "The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.\r\n \r\nThe enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. \r\n\r\nThe advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Carol",
   "pi_last_name": "Neidle",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Carol J Neidle",
   "pi_email_addr": "carol@bu.edu",
   "nsf_id": "000197237",
   "pi_start_date": "2011-07-27",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Stan",
   "pi_last_name": "Sclaroff",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Stan Sclaroff",
   "pi_email_addr": "sclaroff@bu.edu",
   "nsf_id": "000200256",
   "pi_start_date": "2011-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Trustees of Boston University",
  "inst_street_address": "1 SILBER WAY",
  "inst_street_address_2": "",
  "inst_city_name": "BOSTON",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6173534365",
  "inst_zip_code": "022151703",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "TRUSTEES OF BOSTON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "THL6A6JLE1S7"
 },
 "perf_inst": {
  "perf_inst_name": "Trustees of Boston University",
  "perf_str_addr": "1 SILBER WAY",
  "perf_city_name": "BOSTON",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "022151703",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 368205.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goal of this project was to create a linguistically annotated, publicly available, and easily searchable corpus of high-quality video (including multiple synchronized views) from American Sign Language (ASL).&nbsp;</p>\n<p><strong>Intellectual merits</strong></p>\n<p>This constitutes an important piece of infrastructure, enabling new kinds of research in both linguistics and computer vision-based recognition of ASL. In addition, a key goal has been to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. In this project, we offer hardware and software innovations that constitute a major qualitative upgrade in the organization, searchability, and public availability of our existing and expanding corpus.</p>\n<p>&nbsp;&nbsp;&nbsp;&nbsp; Specific accomplishments:</p>\n<ul>\n<li>We purchased file servers at Boston and Rutgers Universities and established mirroring between the two sites.&nbsp; This enables distribution of our video data, software tools, and linguistic annotations.</li>\n</ul>\n<ul>\n<li>We released a new version of our software for linguistic annotation of video language data, SignStream&reg; 3, in August 2017. This was released with the MIT license; our trademark was also renewed.&nbsp;</li>\n</ul>\n<ul>\n<li>SignStream&reg; was used at Gallaudet and Boston Universities for annotation of a substantial set of ASL data (some of which had been collected in conjunction with other NSF-funded projects: \"HCC: Collaborative Research: Medium: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing\" and \"III: Medium: Collaborative Research: Linguistically Based ASL Sign Recognition as a Structured Multivariate Learning Problem\").</li>\n</ul>\n<ul>\n<li>We designed, developed, and released our Data Access Interface (DAI) as well as an updated version thereof to deal with SignStream&reg; 3 files (DAI 2).&nbsp;&nbsp; This allows for easy browsing, searching, and downloading of our linguistically annotated ASL video corpora.</li>\n</ul>\n<ul>\n<li>The original DAI provides access to (1) our National Center for Sign Language and Gesture Resources (NCSLGR) continuous signing corpus, and (2) our American Sign Language Lexicon Video Dataset (ASLLVD) of citation-form signs (made possible by prior NSF funding, grant # 0705749).&nbsp; DAI 2 provides access to a subset of our new ASLLRP SignStream&reg; 3&nbsp;corpus, still under development; the remainder will be added as soon as verifications are complete.</li>\n</ul>\n<ul>\n<li>DAI 2 includes a new ASLLRP Sign Bank, built initially off of the ASLLVD.&nbsp; The Sign Bank will be expanded as new data sets are uploaded to DAI 2.</li>\n</ul>\n<ul>\n<li>We have used these resources to advance linguistic and computer science research, thereby leading to a better understanding of how language works in the visual-gestural modality and to new approaches to sign language recognition from video by computer.&nbsp; The results of this research have been shared through publications and conference presentations.<strong>&nbsp;</strong></li>\n</ul>\n<p><strong>Broader impacts</strong></p>\n<p>These software tools and corpora are invaluable resources for linguistic research; we expect they will enable new kinds of discoveries and the testing of hypotheses that would otherwise have been difficult to investigate. For computer vision, the linguistically annotated video corpora offer an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, these datasets will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing.&nbsp;</p>\n<p>Already our data sets are being used by researchers, educators, and students around the world for linguistic and computer science research and doctoral training.&nbsp; Our work has also influenced the development of corpora for other signed languages (e.g., Arabic Sign Language, Italian Sign Language, and Yiddish Sign Language).</p>\n<p>These materials will also be invaluable for those teaching/studying ASL and Deaf culture, ASL literature, and interpreting. Making our corpora available online will also allow the broader community of ASL users to access our data directly, and to analyze new data with the software tools we are sharing. Students of ASL will be able to retrieve video examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the new web interfaces to our data collections will be a useful educational resource for users, teachers, and learners of ASL. Moreover, the system will have educational benefits for deaf students, in helping them to learn English vocabulary and to connect ASL signs to English words.</p>\n<p>The research made possible by these resources holds great promise for leading to technologies that will benefit the deaf community. These include tools for language learning, mobile sign language dictionaries and retrieval, and tools for searching for signs by example. Ultimately, this resource also is likely to contribute to systems for automated machine translation and human-computer interaction.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2017<br>\n\t\t\t\t\tModified by: Carol&nbsp;J&nbsp;Neidle</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949150289_Fig1--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949150289_Fig1--rgov-800width.jpg\" title=\"SignStream&reg; 3 - Screen shot\"><img src=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949150289_Fig1--rgov-66x44.jpg\" alt=\"SignStream&reg; 3 - Screen shot\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 1. SignStream\u00ae 3 - Screen shot, single signer</div>\n<div class=\"imageCredit\">BU American Sign Language Linguistic Research Project (ASLLRP)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">SignStream\u00ae 3 - Screen shot</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949249791_Fig2--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949249791_Fig2--rgov-800width.jpg\" title=\"SignStream&reg; 3 Screen shot - dialog\"><img src=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949249791_Fig2--rgov-66x44.jpg\" alt=\"SignStream&reg; 3 Screen shot - dialog\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 2. SignStream\u00ae 3 - Screen shot, two signers in dialog</div>\n<div class=\"imageCredit\">BU American Sign Language Linguistic Research Project (ASLLRP)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">SignStream\u00ae 3 Screen shot - dialog</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949364161_Fig3--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949364161_Fig3--rgov-800width.jpg\" title=\"DAI 2 sample search results\"><img src=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949364161_Fig3--rgov-66x44.jpg\" alt=\"DAI 2 sample search results\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 3. DAI 2 ? sample search results for sign glossed as ?ALWAYS?; user can play sign or utterance videos, front, side, or face close-up views</div>\n<div class=\"imageCredit\">BU American Sign Language Linguistic Research Project (ASLLRP)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">DAI 2 sample search results</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949467552_Fig4--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949467552_Fig4--rgov-800width.jpg\" title=\"Sample DAI 2 Sign Bank search\"><img src=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949467552_Fig4--rgov-66x44.jpg\" alt=\"Sample DAI 2 Sign Bank search\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 4. Sample DAI 2 Sign Bank Search: for signs with gloss containing text string ?second?; user can display all occurrencesand play sign videos or composite video of all productions together</div>\n<div class=\"imageCredit\">BU American Sign Language Linguistic Research Project (ASLLRP)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Sample DAI 2 Sign Bank search</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949564039_Fig5--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949564039_Fig5--rgov-800width.jpg\" title=\"Sign Bank available from within SignStream&reg;\"><img src=\"/por/images/Reports/POR/2017/1059218/1059218_10113896_1508949564039_Fig5--rgov-66x44.jpg\" alt=\"Sign Bank available from within SignStream&reg;\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Figure 5. Sign Bank available from within SignStream\u00ae, to improve annotation efficiency and accuracy</div>\n<div class=\"imageCredit\">BU American Sign Language Linguistic Research Project (ASLLRP)</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Carol&nbsp;J&nbsp;Neidle</div>\n<div class=\"imageTitle\">Sign Bank available from within SignStream\u00ae</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThe goal of this project was to create a linguistically annotated, publicly available, and easily searchable corpus of high-quality video (including multiple synchronized views) from American Sign Language (ASL). \n\nIntellectual merits\n\nThis constitutes an important piece of infrastructure, enabling new kinds of research in both linguistics and computer vision-based recognition of ASL. In addition, a key goal has been to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. In this project, we offer hardware and software innovations that constitute a major qualitative upgrade in the organization, searchability, and public availability of our existing and expanding corpus.\n\n     Specific accomplishments:\n\nWe purchased file servers at Boston and Rutgers Universities and established mirroring between the two sites.  This enables distribution of our video data, software tools, and linguistic annotations.\n\n\nWe released a new version of our software for linguistic annotation of video language data, SignStream&reg; 3, in August 2017. This was released with the MIT license; our trademark was also renewed. \n\n\nSignStream&reg; was used at Gallaudet and Boston Universities for annotation of a substantial set of ASL data (some of which had been collected in conjunction with other NSF-funded projects: \"HCC: Collaborative Research: Medium: Generating Accurate, Understandable Sign Language Animations Based on Analysis of Human Signing\" and \"III: Medium: Collaborative Research: Linguistically Based ASL Sign Recognition as a Structured Multivariate Learning Problem\").\n\n\nWe designed, developed, and released our Data Access Interface (DAI) as well as an updated version thereof to deal with SignStream&reg; 3 files (DAI 2).   This allows for easy browsing, searching, and downloading of our linguistically annotated ASL video corpora.\n\n\nThe original DAI provides access to (1) our National Center for Sign Language and Gesture Resources (NCSLGR) continuous signing corpus, and (2) our American Sign Language Lexicon Video Dataset (ASLLVD) of citation-form signs (made possible by prior NSF funding, grant # 0705749).  DAI 2 provides access to a subset of our new ASLLRP SignStream&reg; 3 corpus, still under development; the remainder will be added as soon as verifications are complete.\n\n\nDAI 2 includes a new ASLLRP Sign Bank, built initially off of the ASLLVD.  The Sign Bank will be expanded as new data sets are uploaded to DAI 2.\n\n\nWe have used these resources to advance linguistic and computer science research, thereby leading to a better understanding of how language works in the visual-gestural modality and to new approaches to sign language recognition from video by computer.  The results of this research have been shared through publications and conference presentations. \n\n\nBroader impacts\n\nThese software tools and corpora are invaluable resources for linguistic research; we expect they will enable new kinds of discoveries and the testing of hypotheses that would otherwise have been difficult to investigate. For computer vision, the linguistically annotated video corpora offer an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, these datasets will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. \n\nAlready our data sets are being used by researchers, educators, and students around the world for linguistic and computer science research and doctoral training.  Our work has also influenced the development of corpora for other signed languages (e.g., Arabic Sign Language, Italian Sign Language, and Yiddish Sign Language).\n\nThese materials will also be invaluable for those teaching/studying ASL and Deaf culture, ASL literature, and interpreting. Making our corpora available online will also allow the broader community of ASL users to access our data directly, and to analyze new data with the software tools we are sharing. Students of ASL will be able to retrieve video examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the new web interfaces to our data collections will be a useful educational resource for users, teachers, and learners of ASL. Moreover, the system will have educational benefits for deaf students, in helping them to learn English vocabulary and to connect ASL signs to English words.\n\nThe research made possible by these resources holds great promise for leading to technologies that will benefit the deaf community. These include tools for language learning, mobile sign language dictionaries and retrieval, and tools for searching for signs by example. Ultimately, this resource also is likely to contribute to systems for automated machine translation and human-computer interaction.\n\n \n\n\t\t\t\t\tLast Modified: 10/29/2017\n\n\t\t\t\t\tSubmitted by: Carol J Neidle"
 }
}