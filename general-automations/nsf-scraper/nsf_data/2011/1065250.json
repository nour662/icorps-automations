{
 "awd_id": "1065250",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "III: Medium: Development and Evaluation of Search Technology for Discovery of Evidence in Civil Litigation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032927347",
 "po_email": "sspengle@nsf.gov",
 "po_sign_block_name": "Sylvia Spengler",
 "awd_eff_date": "2011-06-01",
 "awd_exp_date": "2017-05-31",
 "tot_intn_awd_amt": 1199996.0,
 "awd_amount": 1199996.0,
 "awd_min_amd_letter_date": "2011-03-23",
 "awd_max_amd_letter_date": "2014-07-16",
 "awd_abstract_narration": "The civil litigation system of the United States serves as the ultimate arbiter for commercial and personal disputes.  Under this system, plaintiffs and defendants are entitled to request relevant evidence from each other.  Although digital records seem easier to find than their older paper counterparts, rapid growth in the volume, diversity, and possible locations of these records has actually made it harder to find the proverbial needles within the digital haystacks.  The resulting rapid increase in the cost of discovery and exchange of relevant evidence, if left unchecked,   raises concerns about access to justice. Hence, there is an urgent need for demonstrably accurate and cost-effective technologies to support  \"e-discovery\" of the relevant records.\r\n\r\nProfessor Douglas W. Oard and colleagues of the University of Maryland are developing techniques to automatically decide within minutes the responsiveness of more documents than one person could examine in a lifetime.  These techniques use \"semi-supervised learning\" algorithms for \"training\" the software to replicate the kinds of decisions that people make on representative examples. Using Finite Population Annotation, a new framework for integrating learning with evaluation, novel methods are being developed to achieve and measure the highest possible effectiveness for any specified level of human effort.  These learning methods draw on rich approaches to representing the content of both born-digital structured documents and scanned paper.  Measures for rigorously assessing the effectiveness of the resulting automated review  techniques are being developed both to support decisions by legal professionals and by the courts about which methods to use, and to help developers further improve their algorithms.\r\n  \r\nThe legal system demands technology whose effectiveness has been demonstrated on collections that are representative of what is actually expected in a real case.  For that reason, this project is creating real world benchmarks in collaboration with the National Institute of Standards and Technology's Text Retrieval Conference (TREC).  The project's results are expected to help to shape professional practice through workshops for legal and technical stakeholders, and through university courses to prepare the next generation of attorneys and information professionals to employ these new capabilities.  \"E-discovery\" technologies resulting from this effort are likely to be broadly applicable in domains beyond the law practice, including preparation of systematic reviews of scientific literature, scholarly access to digital archives, and government responses to public information requests from citizens.  Additional information is available at http://ediscovery.umiacs.umd.edu.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Douglas",
   "pi_last_name": "Oard",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Douglas W Oard",
   "pi_email_addr": "oard@glue.umd.edu",
   "nsf_id": "000161387",
   "pi_start_date": "2011-03-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Doermann",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "David S Doermann",
   "pi_email_addr": "doermann@buffalo.edu",
   "nsf_id": "000230523",
   "pi_start_date": "2011-03-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Kirsch",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Kirsch",
   "pi_email_addr": "dkirsch@umd.edu",
   "nsf_id": "000449123",
   "pi_start_date": "2011-03-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Maryland, College Park",
  "inst_street_address": "3112 LEE BUILDING",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE PARK",
  "inst_state_code": "MD",
  "inst_state_name": "Maryland",
  "inst_phone_num": "3014056269",
  "inst_zip_code": "207425100",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "MD04",
  "org_lgl_bus_name": "UNIVERSITY OF MARYLAND, COLLEGE PARK",
  "org_prnt_uei_num": "NPU8ULVAAS23",
  "org_uei_num": "NPU8ULVAAS23"
 },
 "perf_inst": {
  "perf_inst_name": "University of Maryland, College Park",
  "perf_str_addr": "3112 LEE BUILDING",
  "perf_city_name": "COLLEGE PARK",
  "perf_st_code": "MD",
  "perf_st_name": "Maryland",
  "perf_zip_code": "207425100",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "MD04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 313258.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 421678.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 363991.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 101069.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Researchers at the University of Maryland worked with colleagues from around the world to develop new capabilities for identifying relevant evidence in large collections of digital records for use in civil litigation, a task known as \"e-discovery.\" &nbsp;Close collaboration was established with legal professionals, both directly and through two international venues: the Text Retrieval Conference (TREC) Legal Track and a biennial workshop series on the Discovery of Electronically Stored Information (DESI). &nbsp;</p>\n<p><br />Manual review of large document collections to find relevant evidence that must be disclosed to other parties in a lawsuit can be extremely time-consuming and expensive. &nbsp;Machine learning techniques for text classification can be used to find such documents, but costs can still be high because substantial numbers of documents must be manually labeled, both for training a classifier and for evaluating its accuracy. &nbsp;This project developed new ways of minimizing those labeling costs by considering both training and evaluation costs together in a flexible cost-minimization framework. &nbsp;</p>\n<p>A second source of costs when using current e-discovery methods is that every relevant document that is found must then be reviewed to determine whether it may be withheld based on some legal privilege (for example, attorney-client privilege). &nbsp;That second \"privilege review\" can be a substantial cost driver because specialized legal expertise is needed to make the determination. &nbsp;This project developed and demonstrated the effectiveness of techniques for machine-assisted privilege review, either as a tool to assist legal experts or (for resource-constrained cases in which less is at stake) as a fully automated process. &nbsp;</p>\n<p>The fact that reviews for relevance and for privilege are typically performed sequentially also offers the potential for minimizing costs by optimally allocating human effort between those two review stages. &nbsp;This project developed an integrated model of review costs and decision risks, and then used that model to create a novel two-stage cost-sensitive and risk-sensitive system for allocating review effort. &nbsp;This was the project's third major technical contribution. &nbsp;</p>\n<p>The project also developed research infrastructure to support work on e-discovery and also research in other fields such as management, organizational behavior, and social network analysis. The project's principal contribution to research infrastructure was the creation of a new collection of corporate email. &nbsp;A project partner subsequently granted a license to the Linguistic Data Consortium to distribute the collection for research use, and that collection (the \"Avocado\" collection) has since been used by research groups around the world.</p>\n<p>Law firms, companies that provide e-discovery services, and the courts have rapidly advanced their understanding and use of advanced search technology for e-discovery over the course of this project, resulting in a robust dialogue in the legal community on what has come to be called \"Technology Assisted Review\" (TAR). &nbsp;Techniques developed in this project can also be used to improve the effectiveness, efficiency, and timeliness of responses to Freedom of Information Act requests when, as is increasingly common, digital content is sought from government agencies. &nbsp;Indeed, the increasing ubiquity of digital content in every sector of our society makes it imperative that techniques such as those developed in this project be made widely available in order to identify, categorize and ultimately make the best use of the voluminous and complex digital heritage that we are so rapidly producing.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/03/2017<br>\n\t\t\t\t\tModified by: Douglas&nbsp;W&nbsp;Oard</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nResearchers at the University of Maryland worked with colleagues from around the world to develop new capabilities for identifying relevant evidence in large collections of digital records for use in civil litigation, a task known as \"e-discovery.\"  Close collaboration was established with legal professionals, both directly and through two international venues: the Text Retrieval Conference (TREC) Legal Track and a biennial workshop series on the Discovery of Electronically Stored Information (DESI).  \n\n\nManual review of large document collections to find relevant evidence that must be disclosed to other parties in a lawsuit can be extremely time-consuming and expensive.  Machine learning techniques for text classification can be used to find such documents, but costs can still be high because substantial numbers of documents must be manually labeled, both for training a classifier and for evaluating its accuracy.  This project developed new ways of minimizing those labeling costs by considering both training and evaluation costs together in a flexible cost-minimization framework.  \n\nA second source of costs when using current e-discovery methods is that every relevant document that is found must then be reviewed to determine whether it may be withheld based on some legal privilege (for example, attorney-client privilege).  That second \"privilege review\" can be a substantial cost driver because specialized legal expertise is needed to make the determination.  This project developed and demonstrated the effectiveness of techniques for machine-assisted privilege review, either as a tool to assist legal experts or (for resource-constrained cases in which less is at stake) as a fully automated process.  \n\nThe fact that reviews for relevance and for privilege are typically performed sequentially also offers the potential for minimizing costs by optimally allocating human effort between those two review stages.  This project developed an integrated model of review costs and decision risks, and then used that model to create a novel two-stage cost-sensitive and risk-sensitive system for allocating review effort.  This was the project's third major technical contribution.  \n\nThe project also developed research infrastructure to support work on e-discovery and also research in other fields such as management, organizational behavior, and social network analysis. The project's principal contribution to research infrastructure was the creation of a new collection of corporate email.  A project partner subsequently granted a license to the Linguistic Data Consortium to distribute the collection for research use, and that collection (the \"Avocado\" collection) has since been used by research groups around the world.\n\nLaw firms, companies that provide e-discovery services, and the courts have rapidly advanced their understanding and use of advanced search technology for e-discovery over the course of this project, resulting in a robust dialogue in the legal community on what has come to be called \"Technology Assisted Review\" (TAR).  Techniques developed in this project can also be used to improve the effectiveness, efficiency, and timeliness of responses to Freedom of Information Act requests when, as is increasingly common, digital content is sought from government agencies.  Indeed, the increasing ubiquity of digital content in every sector of our society makes it imperative that techniques such as those developed in this project be made widely available in order to identify, categorize and ultimately make the best use of the voluminous and complex digital heritage that we are so rapidly producing.\n\n\t\t\t\t\tLast Modified: 09/03/2017\n\n\t\t\t\t\tSubmitted by: Douglas W Oard"
 }
}