{
 "awd_id": "1065106",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Medium: Theory and Practice of Optimal Meshing",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Jack S. Snoeyink",
 "awd_eff_date": "2011-04-01",
 "awd_exp_date": "2015-03-31",
 "tot_intn_awd_amt": 772857.0,
 "awd_amount": 772857.0,
 "awd_min_amd_letter_date": "2011-03-31",
 "awd_max_amd_letter_date": "2011-03-31",
 "awd_abstract_narration": "Meshing has been a cornerstone for simulations using the finite\r\nelement method.  But more recently it has had applications wherever\r\none needs to define a function over a domain, such as, graphics,\r\ncomputer aided design, robotics, and even machine learning. Algorithms\r\nwill be designed to efficiently work in any fixed dimension with\r\nguarantees on output size and quality.\r\n\r\nAt present, no theory exists to formulate and produce optimal meshes\r\nin the presence of small input angles even for the 2D case.  The\r\nresearch will find efficient algorithms 2D and higher dimension that\r\ngenerate optimal size Delaunay triangulations using only simplices\r\nwith no angles approaching 180 degrees.\r\n\r\nMachine learning applications need a meshing algorithm that runs in\r\npolynomial time for meshing n points in log n dimensions.\r\nHistorically, meshing algorithms return a set of space-filling\r\nsimplices.  Even good aspect ratio simplices have too small a volume\r\nand return a mesh that is of super polynomial size.  Thus, new\r\nalgorithms will be developed that handle atomic objects that are have\r\nmuch larger volume than simplices.\r\n\r\nThe results from this project are eminently practical and have broad\r\nimpact on the Sciences, Engineering, Manufacturing, and Machine\r\nLearning.  In particular, meshing is an enabling technology for\r\ndesigning efficient windmills and cars, and simulations of earth\r\nquakes and medial devices.  One goal is to incorporated techniques\r\nfrom this research into our first generation 3D code that we made\r\navailable on the web. In addition, this material will be incorporated\r\ninto classes taught at CMU, lectures, and papers presented at\r\nconferences.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Gary",
   "pi_last_name": "Miller",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Gary Miller",
   "pi_email_addr": "glmiller@cs.cmu.edu",
   "nsf_id": "000462221",
   "pi_start_date": "2011-03-31",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7929",
   "pgm_ref_txt": "COMPUTATIONAL GEOMETRY"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 772857.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The primary focus of the project was to design more efficient<br />algorithms for fundamental problems that arise in geometry, graph<br />theory, and linear algebra.&nbsp; Our motivation and interest for studying<br />these algorithmic problems arises partly by their application to<br />problems in the sciences, industry, medicine, and military.<br /><br />One of the applications studied in more detail by this project were<br />problems in image processing.&nbsp; We demonstrated how these new<br />algorithms can be applied to problems as medical image processing.<br />Other fast algorithms were found for the solution to numeric problems<br />arising from electro-magnetic radiation and 3D fluids problems such as<br />those involving Maxwell's equations and radar imaging.<br /><br />To efficiently solve these applications required us to study and solve<br />more fundamental questions in algorithm design. Included in our list<br />were problems in computational geometry, graph theory,<br />and linear algebra.&nbsp; The main approach undertaken to solve a problem<br />in one of these three areas was to include the tools developed in the<br />other two areas.&nbsp; A now classic algorithmic design technique is to use<br />graph theory to solve linear algebra problems and linear algebra to<br />solve graph problems in what now is known as spectral graph theory.<br />The project also developed new algorithms for problems in<br />computational geometry and numerical analysis.<br /><br />One of the simplest problems that was studied by this project and is<br />reasonably easy to describe is the problem of computing the Voronoi<br />Diagram of a set of points given in some fixed dimensional space. The<br />problem is to partition the space according to the distance to the<br />nearest input point. This will decompose the space into a polytope<br />around each input point. Unfortunately the number of corners of these<br />polytopes may be exponential in the number of points and the<br />dimension. Thus, even in three dimensions the number of corners may be<br />quadratic in the number of points.&nbsp; But quite often the number of<br />corners may only be linear in the number of points.&nbsp; We showed how to<br />use ideas from finite element mesh generation to improve the runtime<br />to compute the Voronoi diagram when the output size is small. In this<br />case we obtained a much faster algorithm then was known.<br /><br />Another very well-studied problem is finding what is known as a low<br />diameter decomposition of a graph. That is given an unweighted<br />undirected graph G with n vertices remove a small number of edges such<br />that each connected component of the remaining graph has small<br />diameter, O(log n).&nbsp; We found a very simple linear-work parallel algorithm for<br />this classic problem.&nbsp; This algorithm is now used in the fastest known<br />algorithm for solving linear systems that are symmetric and diagonally<br />dominate SDD.&nbsp; We have also used this decomposition algorithm to<br />improve the run time for parallel approximate shortest-path algorithms<br />as well as approximate graph spanners.<br /><br />The algorithm is so simple it can be explained to a general audience.<br />The input to the algorithm is an unweighted undirected graph and we<br />think of each node as having an agent or processor on each node.<br /><br />The algorithm is as follows:<br /><br />1) Each agent picks independently a value from an exponential<br />distribution, say the ith node picks Xi.<br /><br />2) The agents determine the maximum value picked, say, Xm.<br /><br />3) The ith agent comes alive at time Xm - Xi and if no one owns the<br />agent's node, it gets to start its own cluster by owning its own node, otherwise it quits.<br /><br />4) At each successive time the agent looks to see if any nodes<br />neighboring the nodes it owns are not owned and i...",
  "por_txt_cntn": "\nThe primary focus of the project was to design more efficient\nalgorithms for fundamental problems that arise in geometry, graph\ntheory, and linear algebra.  Our motivation and interest for studying\nthese algorithmic problems arises partly by their application to\nproblems in the sciences, industry, medicine, and military.\n\nOne of the applications studied in more detail by this project were\nproblems in image processing.  We demonstrated how these new\nalgorithms can be applied to problems as medical image processing.\nOther fast algorithms were found for the solution to numeric problems\narising from electro-magnetic radiation and 3D fluids problems such as\nthose involving Maxwell's equations and radar imaging.\n\nTo efficiently solve these applications required us to study and solve\nmore fundamental questions in algorithm design. Included in our list\nwere problems in computational geometry, graph theory,\nand linear algebra.  The main approach undertaken to solve a problem\nin one of these three areas was to include the tools developed in the\nother two areas.  A now classic algorithmic design technique is to use\ngraph theory to solve linear algebra problems and linear algebra to\nsolve graph problems in what now is known as spectral graph theory.\nThe project also developed new algorithms for problems in\ncomputational geometry and numerical analysis.\n\nOne of the simplest problems that was studied by this project and is\nreasonably easy to describe is the problem of computing the Voronoi\nDiagram of a set of points given in some fixed dimensional space. The\nproblem is to partition the space according to the distance to the\nnearest input point. This will decompose the space into a polytope\naround each input point. Unfortunately the number of corners of these\npolytopes may be exponential in the number of points and the\ndimension. Thus, even in three dimensions the number of corners may be\nquadratic in the number of points.  But quite often the number of\ncorners may only be linear in the number of points.  We showed how to\nuse ideas from finite element mesh generation to improve the runtime\nto compute the Voronoi diagram when the output size is small. In this\ncase we obtained a much faster algorithm then was known.\n\nAnother very well-studied problem is finding what is known as a low\ndiameter decomposition of a graph. That is given an unweighted\nundirected graph G with n vertices remove a small number of edges such\nthat each connected component of the remaining graph has small\ndiameter, O(log n).  We found a very simple linear-work parallel algorithm for\nthis classic problem.  This algorithm is now used in the fastest known\nalgorithm for solving linear systems that are symmetric and diagonally\ndominate SDD.  We have also used this decomposition algorithm to\nimprove the run time for parallel approximate shortest-path algorithms\nas well as approximate graph spanners.\n\nThe algorithm is so simple it can be explained to a general audience.\nThe input to the algorithm is an unweighted undirected graph and we\nthink of each node as having an agent or processor on each node.\n\nThe algorithm is as follows:\n\n1) Each agent picks independently a value from an exponential\ndistribution, say the ith node picks Xi.\n\n2) The agents determine the maximum value picked, say, Xm.\n\n3) The ith agent comes alive at time Xm - Xi and if no one owns the\nagent's node, it gets to start its own cluster by owning its own node, otherwise it quits.\n\n4) At each successive time the agent looks to see if any nodes\nneighboring the nodes it owns are not owned and if it finds any it\ngets to own them.\n\n5) The process continues until all the nodes belong to a cluster, which\nwill be after at most Xm time steps.\n\nThe algorithm analysis follows from basic proprieties of the exponential\ndistribution:  The diameter bounds follows by showing that Xm cannot\nbe too large; while bounding the number of edges between clusters\nfollows by the memory-less property of the exponential distribution.\n\n\n\n..."
 }
}