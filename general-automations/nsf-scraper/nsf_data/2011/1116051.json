{
 "awd_id": "1116051",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC: Small: MobileAccessibility: Bridge to the World for Blind, Low-Vision, and Deaf-Blind People",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 516000.0,
 "awd_min_amd_letter_date": "2011-07-28",
 "awd_max_amd_letter_date": "2013-06-27",
 "awd_abstract_narration": "More than 160 million blind, low-vision, and deaf-blind people worldwide have not realized the full potential of the mobile revolution.  People in these groups often use special-purpose portable devices to solve specific accessibility problems, such as obtaining product information from bar codes, finding location information via GPS, and accessing printed text using optical character recognition (OCR).  Unfortunately, devices targeted at these groups are specialized for one or few functions, usually not networked, and expensive.  Devices also target one disability, thereby preventing a deaf-blind person from, for instance, using a device designed for a low-vision person.  Blind, low-vision, and deaf-blind people who can afford it must carry multiple devices with varying interfaces.  This is despite the fact that many mainstream mobile devices already have the necessary sensors, such as a camera, microphone, GPS locator, accelerometer, and compass, to provide all of these functions on one device.  MobileAccessibility is the PI's approach to providing useful mobile accessible functionality to blind, low-vision, and deaf-blind users.  This approach leverages a smart phone's sensors, multi-modal output, and access to remote services to reduce the cost of existing accessibility solutions and enable completely new ones to be created.  Some key user interaction problems for these groups of users that will be addressed in this project include: (i) how can a blind, low-vision, or deaf-blind person effectively use the camera on a smart phone to achieve an accessibility goal, (ii) how can enlarged presentations be effectively navigated by a low-vision person on the small screen of a smart phone, (iii) how can vibration be effectively used to convey information to a blind or deaf-blind person, (iv) how can valuable network services be best utilized by these communities, (v) how can the knowledge of one person about their environment be effectively captured, stored, and used among these communities.  The user-centered design of these applications will involve blind, low-vision, and deaf-blind people throughout their development.  Prototype applications to provide context to the research questions will be built for all three groups. Input will use speech recognition, the touch screen, and the keyboard.  Output will be audio for blind users, enlargement for low-vision users, and vibration and tethering to Braille devices for deaf-blind and blind users.  The resulting interfaces will be evaluated both in the lab and in the field.  There will a focus on identifying common interaction techniques that can be employed by multiple applications.\r\n\r\nBroader Impacts:  This research represents a new paradigm in mobile assistive technologies where a single programmable device can serve a multitude of accessibility needs.  Rather than using separate devices for different needs, accessibility solutions can be downloaded to a single device.  The research challenge is to design, build, and evaluate novel accessibility solutions in this new paradigm.  A mobile phone that can accomplish multiple accessibility tasks has the potential to provide the target communities with more independence than they have currently.  Furthermore, the MobileAccessibility solution has the potential to be inexpensive and more sustainable than current accessibility solutions.  Qualified students with disabilities will be recruited as researchers, giving them a chance to participate in work directly affecting them. New project-oriented curricula based on MobileAccessibility will be created.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Richard",
   "pi_last_name": "Ladner",
   "pi_mid_init": "E",
   "pi_sufx_name": "",
   "pi_full_name": "Richard E Ladner",
   "pi_email_addr": "ladner@cs.washington.edu",
   "nsf_id": "000438222",
   "pi_start_date": "2011-07-28",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jeffrey",
   "pi_last_name": "Bigham",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jeffrey Bigham",
   "pi_email_addr": "jbigham@cmu.edu",
   "nsf_id": "000541549",
   "pi_start_date": "2011-11-15",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 BROOKLYN AVE NE",
  "perf_city_name": "SEATTLE",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981951016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The goals of MobileAccessibility are to design, implement, and evaluate new interaction techniques and applications on touchscreen smartphones that support the accessibility needs of blind, low-vision, and deaf-blind people. &nbsp;&nbsp; User actions can be touchscreen gestures, speech entry, or taking photographs.&nbsp; Output to users can be speech, other sounds, or vibration. &nbsp;Ten representative projects contributed to these goals.</p>\n<p>1. Perkinput, a Braille-based text entry system for touchscreen that uses finger calibration instead of buttons.&nbsp; Once calibrated, users enter Braille using standard chording techniques found with Perkins Braillers.&nbsp;&nbsp; Studies show that using Perkinput is much faster than using a touchscreen keyboard with a screen reader.</p>\n<p>2. PassChords, a multi-touch gesture system to enter PINs on a touchscreens without using a keyboard.&nbsp; Studies show that PassChords are easy to remember, fast to enter, and have similar security to standard four-digit PINs. &nbsp;</p>\n<p>3. Digitaps, multi-touch gesture systems to enter numeric data on touchscreens without a keyboard.&nbsp; The systems were developed to be easy to learn and minimize the number of gestures per digit.&nbsp; Studies show that the gesture systems are indeed easy to learn and provide an accurate way to enter numeric data.</p>\n<p>4. Tactile Graphics with a Voice (TGV), an alternative method to make the text in embossed figures accessible using QR codes instead of embossed Braille.&nbsp;&nbsp; Several self-voicing smartphone applications were developed that help guide the user toward taking a good pictures.&nbsp; Studies show that TGV is an effective means to access the text in embossed figures.</p>\n<p>5. Blind photography enhancement, techniques that enable blind photographers to take better pictures.&nbsp; Studies show that methods to automatically choose good frames from streaming video is an effective way to take good pictures.</p>\n<p>6. VizWiz, a technique where blind users can take a picture and ask a question that is answered by crowd workers.&nbsp; Studies of more than 50,000 photographs and questions by blind VizWiz users reveal what blind people actually want to know.</p>\n<p>7. BraillePlay, a family of smartphone games that promote Braille literacy in young blind children and their families.&nbsp; The games implement Braille access using speech and vibration.&nbsp; Studies show that the blind children are can play the games effectively and enjoy them.</p>\n<p>8. JustSpeak, &nbsp;a universal voice control solution for non-visual access to the Android operating system.&nbsp; JustSpeak is available for application developers as a mechanism to avoid finding button on a touchscreen.&nbsp; This ability to use speech instead of searching for buttons is particularly useful for blind users.</p>\n<p>9. StopInfo, a crowdsourced method of obtaining information about bus stops that is part of the popular OneBusAway system for real-time bus arrival information.&nbsp; Studies show that bus riders are willing to provide accurate information about bus stops with various incentives. &nbsp;&nbsp;Studies also show that the information provided by StopInfo is useful for blind transit riders.</p>\n<p>10. Eyes-free speech-based dictation, a system to enter and edit text using speech and touchscreen gestures only.&nbsp;&nbsp; Speech is the primary mode for entry and editing, and gestures are only used for navigating the entered text.&nbsp; Preliminary studies show that eyes-free dictation on smartphones can be effective for many users both blind and sighted users.&nbsp; This project is still in early development.</p>\n<p>Most of the projects reinforce the concept that eyes-free interaction techniques for smartphones that use touch gestures and speech can be very valuable for blind users.&nbsp;&nbsp; Most of the projects build on abilities that many blind users already ha...",
  "por_txt_cntn": "\nThe goals of MobileAccessibility are to design, implement, and evaluate new interaction techniques and applications on touchscreen smartphones that support the accessibility needs of blind, low-vision, and deaf-blind people.    User actions can be touchscreen gestures, speech entry, or taking photographs.  Output to users can be speech, other sounds, or vibration.  Ten representative projects contributed to these goals.\n\n1. Perkinput, a Braille-based text entry system for touchscreen that uses finger calibration instead of buttons.  Once calibrated, users enter Braille using standard chording techniques found with Perkins Braillers.   Studies show that using Perkinput is much faster than using a touchscreen keyboard with a screen reader.\n\n2. PassChords, a multi-touch gesture system to enter PINs on a touchscreens without using a keyboard.  Studies show that PassChords are easy to remember, fast to enter, and have similar security to standard four-digit PINs.  \n\n3. Digitaps, multi-touch gesture systems to enter numeric data on touchscreens without a keyboard.  The systems were developed to be easy to learn and minimize the number of gestures per digit.  Studies show that the gesture systems are indeed easy to learn and provide an accurate way to enter numeric data.\n\n4. Tactile Graphics with a Voice (TGV), an alternative method to make the text in embossed figures accessible using QR codes instead of embossed Braille.   Several self-voicing smartphone applications were developed that help guide the user toward taking a good pictures.  Studies show that TGV is an effective means to access the text in embossed figures.\n\n5. Blind photography enhancement, techniques that enable blind photographers to take better pictures.  Studies show that methods to automatically choose good frames from streaming video is an effective way to take good pictures.\n\n6. VizWiz, a technique where blind users can take a picture and ask a question that is answered by crowd workers.  Studies of more than 50,000 photographs and questions by blind VizWiz users reveal what blind people actually want to know.\n\n7. BraillePlay, a family of smartphone games that promote Braille literacy in young blind children and their families.  The games implement Braille access using speech and vibration.  Studies show that the blind children are can play the games effectively and enjoy them.\n\n8. JustSpeak,  a universal voice control solution for non-visual access to the Android operating system.  JustSpeak is available for application developers as a mechanism to avoid finding button on a touchscreen.  This ability to use speech instead of searching for buttons is particularly useful for blind users.\n\n9. StopInfo, a crowdsourced method of obtaining information about bus stops that is part of the popular OneBusAway system for real-time bus arrival information.  Studies show that bus riders are willing to provide accurate information about bus stops with various incentives.   Studies also show that the information provided by StopInfo is useful for blind transit riders.\n\n10. Eyes-free speech-based dictation, a system to enter and edit text using speech and touchscreen gestures only.   Speech is the primary mode for entry and editing, and gestures are only used for navigating the entered text.  Preliminary studies show that eyes-free dictation on smartphones can be effective for many users both blind and sighted users.  This project is still in early development.\n\nMost of the projects reinforce the concept that eyes-free interaction techniques for smartphones that use touch gestures and speech can be very valuable for blind users.   Most of the projects build on abilities that many blind users already have, Braille knowledge, familiarity with smartphones, and clear speech. As such these projects will benefit society by improving the accessibility to smartphones that are widely used by blind people.  Some of the techniques developed in these projects will also be useful for non..."
 }
}