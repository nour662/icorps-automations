{
 "awd_id": "1116384",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Small: Building Audio Interfaces with Crowdsourced Concept Maps and Active Transfer Learning",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 499804.0,
 "awd_amount": 499804.0,
 "awd_min_amd_letter_date": "2011-08-16",
 "awd_max_amd_letter_date": "2011-08-16",
 "awd_abstract_narration": "The United States is a world-leader in software and in multimedia content (e.g. music, film). To remain so, we must continually raise the bar in both software and media production. Software tools for media production (e.g. the audio production suite Protools) often have complex interfaces, conceptualized in ways that makes it difficult for any but the most expert to realize the power of these tools. Complex interfaces and steep learning curves can discourage creative people from doing their best work with such tools. Here, we focus on audio production tools. We propose a user-centered approach to remove the great disconnect between existing audio production tools and the conceptual frameworks within which many people work, both expert musicians and the broader public. The tools we develop will automatically adapt to the user's conceptual framework, rather than forcing the user to adapt to the tools. Where appropriate, the tools will speed and enhance their adaptation using active learning informed by interaction with previous users (transfer learning). The tools will also automatically build a crowdsourced audio concept map. This will help provide facilities for computer-aided, directed learning, so that tool users can expand their conceptual frameworks and abilities. By letting people manipulate audio on their own terms and enhancing their knowledge of such tools with directed learning, we expect to transform the interaction experience, making the computer a device that supports and enhances creativity, rather than an obstacle.\r\n\r\nThis work will have a number of broader impacts. The tools developed will be directly usable by practicing musicians and will also facilitate learning and creativity for the general public. These techniques will also be applicable to personalization of hearing aids and new diagnostic systems for audiologists. Our approach to tool personalization is core work in human-computer interaction and should generalize to other creative activities (e.g. image manipulation). Resulting advances in active and transfer learning will be of great value to machine learning researchers. Finding the relationships between quantifiable parameters of audio and the language and metaphors used by practicing musicians to describe sound is central to this work. This is of great interest to cognitive scientists, linguists, artificial intelligence researchers, and engineers. Concept maps for audio terms should also prove useful for machine translation. Broad application of techniques to map human descriptive terms on to machine-manipulable parameters will change expectations for both artists and scientists. Artists will be able to explore new lines of creativity that currently require significant investments of time in vastly disparate fields (e.g. signal processing and painting). This has the potential to transform information science and lead to new cognitive models of creativity, forming the basis for new approaches to education and research in both technology and in art.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Bryan",
   "pi_last_name": "Pardo",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Bryan A Pardo",
   "pi_email_addr": "pardo@northwestern.edu",
   "nsf_id": "000275342",
   "pi_start_date": "2011-08-16",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Darren",
   "pi_last_name": "Gergle",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Darren R Gergle",
   "pi_email_addr": "dgergle@northwestern.edu",
   "nsf_id": "000327409",
   "pi_start_date": "2011-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "633 CLARK ST",
  "perf_city_name": "EVANSTON",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 499804.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Audio production is essential to many forms of media, including music recordings, podcasts, radio dramas, television programs and film. Audio production tools for effects like reverberation, equalization, and dynamic range compression, are used to process audio after it is recorded, transforming raw recordings into polished final products. These tools are often difficult to use, as they are parameterized and controlled in terms (e.g. spectral tilt, and &ldquo;Q&rdquo;) that are non-intuitive to many people. On the other hand, many potential users of audio production tools (e.g. acoustic musicians, podcast creators) have sonic ideas that they cannot express in technical terms. As a result, there is a cognitive gap between the tools and those who would use them. &nbsp;</p>\n<p>&nbsp;</p>\n<p>To bridge this gap we developed a new interaction paradigm for audio production tools that allows users to control audio production tools using evaluative feedback and natural language. For example, the user specifies a word describing the quality of the sound they seek, such as making a recording &ldquo;brighter&rdquo; or &ldquo;warmer.&rdquo; We created SocialEQ (see image), an audio production tool that lets the user teach the tool the meaning of a sound adjective (e.g &ldquo;tinny&rdquo; sound) by presenting alternative manipulations of a sound and letting the user rate how well each manipulation embodies the desired goal (e.g. how &ldquo;tinny&rdquo; the sound is now). We created Audealize, a tool that bridges the gap between low-level parameters of existing audio production tools and programmatic goals (e.g. &ldquo;make my guitar sound &lsquo;underwater&rdquo;&rsquo;). Users modify the audio by clicking on the word in a 2-D word map that best describes how they want the sound to change. This example audio production tool (see the primary image) is available for the public to try and download.</p>\n<p>We performed the first user study comparing a word map interface to traditional audio production interfaces. A study on a population of 432 non-experts found they favored the word map over traditional interfaces. Absolute performance measures show those using the word map interface produced results that equaled or exceeded results using traditional interfaces. This indicates that language, in concert with a meaningful word map, is an effective interaction paradigm for audio production by non-experts. This points the way for similar interfaces for media production in other domains. For example, one could make a version of Photoshop that lets the user specify in language how they want the image to look.</p>\n<p>To develop our word map we required a large vocabulary of descriptive terms with known associations to audio effects created by production tools. Therefore, we performed the first large-scale collection of a vocabulary describing audio effects that maps these words to specific settings of the three most widely used audio effects tools: equalization, reverberation, and dynamic range compression. Data on the strength of association between words and the actual setting of these audio effects was collected for a set of 4297 words drawn from 1233 people. This dataset is two orders of magnitude larger than any previous similar data collection. This data has been made available to the public.</p>\n<p>We collected this word association data in both English and Spanish, allowing a new kind of automated translation of descriptive adjectives for audio between these languages. Resources such as the Oxford English Dictionary (OED) typically list the &ldquo;audio sense&rdquo; for only a small subset of the words commonly used to describe sound. For example, &ldquo;warm&rdquo; is a very commonly used sound adjective and the OED does not mention the audio sense. Directly translating the predominant (i.e. first) sense of a sound adjective into another language often results in an incorrect translation. We developed a system that builds a translation map between sound adjectives of two languages: English and Spanish. When an English word and a Spanish word are both used to describe the same audio effect, they are considered a translation pair. The more frequently a pairing between two words occurs, the more certain the translation. This work points the way to a new kind of machine translation that, rather than relying on paired texts, relies on common associations to the same media object (e.g. a sound file or image).</p>\n<p class=\"CreativeITregulartext\">The audio production tools we developed will facilitate creativity for the general public. Our approach to tool building is generalizable to a variety of activities and disciplines. For example, audiologists could use this approach to translate between lay vocabulary and actionable hearing aid adjustments. Finding the relationships between quantifiable parameters of audio and the language people use to describe sound is at the core of this work. This is of great interest to cognitive scientists, linguists, artificial intelligence researchers, and engineers. Our work also demonstrates techniques that can be used to develop more natural user interfaces, and is of value to human-computer interaction researchers.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/05/2016<br>\n\t\t\t\t\tModified by: Bryan&nbsp;A&nbsp;Pardo</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1116384/1116384_10123100_1480922415712_Audealize--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1116384/1116384_10123100_1480922415712_Audealize--rgov-800width.jpg\" title=\"Audealize Interface\"><img src=\"/por/images/Reports/POR/2016/1116384/1116384_10123100_1480922415712_Audealize--rgov-66x44.jpg\" alt=\"Audealize Interface\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Audealize: a new way of controlling audio production tools. Instead of a traditional interface made of controls with hard-to-understand labels,we provide a language-based interface. Either type the kind of change to the sound you seek in the search box, or click on the word describing what want.</div>\n<div class=\"imageCredit\">Bryan Pardo</div>\n<div class=\"imageSubmitted\">Bryan&nbsp;A&nbsp;Pardo</div>\n<div class=\"imageTitle\">Audealize Interface</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2016/1116384/1116384_10123100_1480922304313_SociaEQ--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1116384/1116384_10123100_1480922304313_SociaEQ--rgov-800width.jpg\" title=\"SocialEQ\"><img src=\"/por/images/Reports/POR/2016/1116384/1116384_10123100_1480922304313_SociaEQ--rgov-66x44.jpg\" alt=\"SocialEQ\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">The welcome screen to SocialEQ. You can teach SocialEQ the meaning of a descriptive word for sound (e.g. \"bright\") using this tool. The learned word is used in the Audealize interface.</div>\n<div class=\"imageCredit\">Bryan Pardo</div>\n<div class=\"imagePermisssions\">Copyrighted</div>\n<div class=\"imageSubmitted\">Bryan&nbsp;A&nbsp;Pardo</div>\n<div class=\"imageTitle\">SocialEQ</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nAudio production is essential to many forms of media, including music recordings, podcasts, radio dramas, television programs and film. Audio production tools for effects like reverberation, equalization, and dynamic range compression, are used to process audio after it is recorded, transforming raw recordings into polished final products. These tools are often difficult to use, as they are parameterized and controlled in terms (e.g. spectral tilt, and \"Q\") that are non-intuitive to many people. On the other hand, many potential users of audio production tools (e.g. acoustic musicians, podcast creators) have sonic ideas that they cannot express in technical terms. As a result, there is a cognitive gap between the tools and those who would use them.  \n\n \n\nTo bridge this gap we developed a new interaction paradigm for audio production tools that allows users to control audio production tools using evaluative feedback and natural language. For example, the user specifies a word describing the quality of the sound they seek, such as making a recording \"brighter\" or \"warmer.\" We created SocialEQ (see image), an audio production tool that lets the user teach the tool the meaning of a sound adjective (e.g \"tinny\" sound) by presenting alternative manipulations of a sound and letting the user rate how well each manipulation embodies the desired goal (e.g. how \"tinny\" the sound is now). We created Audealize, a tool that bridges the gap between low-level parameters of existing audio production tools and programmatic goals (e.g. \"make my guitar sound ?underwater\"?). Users modify the audio by clicking on the word in a 2-D word map that best describes how they want the sound to change. This example audio production tool (see the primary image) is available for the public to try and download.\n\nWe performed the first user study comparing a word map interface to traditional audio production interfaces. A study on a population of 432 non-experts found they favored the word map over traditional interfaces. Absolute performance measures show those using the word map interface produced results that equaled or exceeded results using traditional interfaces. This indicates that language, in concert with a meaningful word map, is an effective interaction paradigm for audio production by non-experts. This points the way for similar interfaces for media production in other domains. For example, one could make a version of Photoshop that lets the user specify in language how they want the image to look.\n\nTo develop our word map we required a large vocabulary of descriptive terms with known associations to audio effects created by production tools. Therefore, we performed the first large-scale collection of a vocabulary describing audio effects that maps these words to specific settings of the three most widely used audio effects tools: equalization, reverberation, and dynamic range compression. Data on the strength of association between words and the actual setting of these audio effects was collected for a set of 4297 words drawn from 1233 people. This dataset is two orders of magnitude larger than any previous similar data collection. This data has been made available to the public.\n\nWe collected this word association data in both English and Spanish, allowing a new kind of automated translation of descriptive adjectives for audio between these languages. Resources such as the Oxford English Dictionary (OED) typically list the \"audio sense\" for only a small subset of the words commonly used to describe sound. For example, \"warm\" is a very commonly used sound adjective and the OED does not mention the audio sense. Directly translating the predominant (i.e. first) sense of a sound adjective into another language often results in an incorrect translation. We developed a system that builds a translation map between sound adjectives of two languages: English and Spanish. When an English word and a Spanish word are both used to describe the same audio effect, they are considered a translation pair. The more frequently a pairing between two words occurs, the more certain the translation. This work points the way to a new kind of machine translation that, rather than relying on paired texts, relies on common associations to the same media object (e.g. a sound file or image).\nThe audio production tools we developed will facilitate creativity for the general public. Our approach to tool building is generalizable to a variety of activities and disciplines. For example, audiologists could use this approach to translate between lay vocabulary and actionable hearing aid adjustments. Finding the relationships between quantifiable parameters of audio and the language people use to describe sound is at the core of this work. This is of great interest to cognitive scientists, linguists, artificial intelligence researchers, and engineers. Our work also demonstrates techniques that can be used to develop more natural user interfaces, and is of value to human-computer interaction researchers.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 12/05/2016\n\n\t\t\t\t\tSubmitted by: Bryan A Pardo"
 }
}