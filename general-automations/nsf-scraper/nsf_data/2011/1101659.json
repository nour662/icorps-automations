{
 "awd_id": "1101659",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "ICES: Small: Mechanism Design for Highly Anonymous Environments",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tracy Kimbrel",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2011-07-06",
 "awd_max_amd_letter_date": "2014-10-02",
 "awd_abstract_narration": "In mechanism design, the goal is to design rules that lead to desirable outcomes even when agents behave strategically.  This project focuses on a specific type of strategic behavior: a single agent can pretend to be multiple agents, and thereby participate in the mechanism more than once. One approach to address this is to design the mechanism so that this behavior is always suboptimal for the agent.  Other approaches include: making it costly to obtain additional identifiers; doing some limited verification of identities; detecting false identifiers based on the social-network structure among the agents; and designing methods to sample from the agents in an unbiased way.  These approaches require a blend of techniques from computer science and economics.\r\n\r\nInternet-based mechanisms such as online rating systems and online elections, as well as mechanisms for electronic commerce, play an increasingly important role in the economy and social infrastructure.  It is often easy to participate multiple times in such mechanisms, leading to distorted results and economic inefficiencies.  Hence, the research under this project has the potential to make Internet-based mechanisms more meaningful and efficient.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vincent",
   "pi_last_name": "Conitzer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vincent Conitzer",
   "pi_email_addr": "conitzer@cs.duke.edu",
   "nsf_id": "000487289",
   "pi_start_date": "2011-07-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W MAIN ST",
  "perf_city_name": "DURHAM",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054640",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "805200",
   "pgm_ele_name": "Inter Com Sci Econ Soc S (ICE)"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Computer systems are increasingly used by multiple parties with different objectives.&nbsp; We cannot simply specify the behavior of these diverse participants; what we can do, however, is to provide incentives for desirable behavior.&nbsp; This is where mechanism design comes in.<br /><br />In mechanism design, the goal is to design rules that result in desirable outcomes even when used by strategic agents that each pursue their own self-interest.&nbsp; Most of mechanism design has focused on the problem of the strategic misreporting of preferences.&nbsp; For example, in an election, a voter may not vote for her most-preferred alternative because she does not believe it has a serious chance of winning; in an auction, a bidder may bid less than she thinks the item is worth because she hopes to win it at a lower price.&nbsp; Mechanisms unlike these, where it is in fact always optimal to report truthfully, are called strategy-proof.<br /><br />However, the applications that motivate computer scientists are often run in highly anonymous environments, such as the Internet.&nbsp; Online voting, rating, and auction systems are pervasive.&nbsp; In these contexts, other types of strategic manipulation are possible: for example, an agent is often able to participate multiple times, under different identities.&nbsp; Strategy-proof mechanisms are generally still vulnerable to such false-name manipulation; mechanisms that are robust to this type of manipulation are called false-name-proof.<br /><br />In accordance with the proposal, we have focused our research on the design of mechanisms for highly anonymous environments.&nbsp; Among other contributions, we have provided a statistical technique for analyzing how confident we can be in the outcome of a vote when false-name manipulation may have taken place; analyzed which matching mechanisms (such as the mechanism used to match residents to hospitals) are false-name-proof, and in what precise sense; designed mechanisms for allocating tasks to agents when it cannot be directly observed whether the agent puts effort into the task; characterized a voting mechanism by its robustness to the insertion of clone alternatives; and analyzed the computational complexity of designing mechanisms when agents can pretend to be of a different type, at some cost.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/30/2015<br>\n\t\t\t\t\tModified by: Vincent&nbsp;Conitzer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nComputer systems are increasingly used by multiple parties with different objectives.  We cannot simply specify the behavior of these diverse participants; what we can do, however, is to provide incentives for desirable behavior.  This is where mechanism design comes in.\n\nIn mechanism design, the goal is to design rules that result in desirable outcomes even when used by strategic agents that each pursue their own self-interest.  Most of mechanism design has focused on the problem of the strategic misreporting of preferences.  For example, in an election, a voter may not vote for her most-preferred alternative because she does not believe it has a serious chance of winning; in an auction, a bidder may bid less than she thinks the item is worth because she hopes to win it at a lower price.  Mechanisms unlike these, where it is in fact always optimal to report truthfully, are called strategy-proof.\n\nHowever, the applications that motivate computer scientists are often run in highly anonymous environments, such as the Internet.  Online voting, rating, and auction systems are pervasive.  In these contexts, other types of strategic manipulation are possible: for example, an agent is often able to participate multiple times, under different identities.  Strategy-proof mechanisms are generally still vulnerable to such false-name manipulation; mechanisms that are robust to this type of manipulation are called false-name-proof.\n\nIn accordance with the proposal, we have focused our research on the design of mechanisms for highly anonymous environments.  Among other contributions, we have provided a statistical technique for analyzing how confident we can be in the outcome of a vote when false-name manipulation may have taken place; analyzed which matching mechanisms (such as the mechanism used to match residents to hospitals) are false-name-proof, and in what precise sense; designed mechanisms for allocating tasks to agents when it cannot be directly observed whether the agent puts effort into the task; characterized a voting mechanism by its robustness to the insertion of clone alternatives; and analyzed the computational complexity of designing mechanisms when agents can pretend to be of a different type, at some cost.\n\n\t\t\t\t\tLast Modified: 11/30/2015\n\n\t\t\t\t\tSubmitted by: Vincent Conitzer"
 }
}