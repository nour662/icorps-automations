{
 "awd_id": "1115665",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: DSA-Cloud: Data Semantics Aware Clouds for High Performance Analytics",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 374973.0,
 "awd_amount": 390973.0,
 "awd_min_amd_letter_date": "2011-07-07",
 "awd_max_amd_letter_date": "2012-04-17",
 "awd_abstract_narration": "This project is motivated by successful deployment of eScience applications\r\non clouds: how to deploy HPC analytics applications on the cloud? Both eScience applications\r\nand HPC analytics applications manipulate with tera-scale or peta-scale data and require \r\naccess to expensive computing resources. However, HPC analytics applications bear several \r\ndistinct characteristics such as complex data access patterns and interest locality, which \r\npose new challenges to its adoption in clouds. \r\n\r\nThe goal of this project is to develop a data semantics \r\naware framework to enable HPC analytics at clouds. Such a framework is composed of three components; \r\n1) a MapReduce API with data semantics awareness used to develop high-performance\r\nanalysis applications, 2) a translation layer equipped with data-semantics aware HPC interfaces, \r\nand 3) a data-affinity-aware data placement scheme. It is anticipated that high productivity on the\r\neconomic impact is significantly improved through the cost-effective scientific data processing.\r\nDelivering an open source software to the community speeds up the 21st century scientific \r\ndiscovery process in any HPC analytics areas such as cosmology, astrophysics, chromodynamics, \r\nbioinformatics, etc. Numerous educational benefits are expected to be generated from collaborative \r\neffort with several UCF educational projects and external collaboration and community ties \r\nthrough the integration into the FutureGrid, and scientific computing cloud at Department of Energy.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jun",
   "pi_last_name": "Wang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jun Wang",
   "pi_email_addr": "Jun.Wang@ucf.edu",
   "nsf_id": "000277527",
   "pi_start_date": "2011-07-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "The University of Central Florida Board of Trustees",
  "inst_street_address": "4000 CENTRAL FLORIDA BLVD",
  "inst_street_address_2": "",
  "inst_city_name": "ORLANDO",
  "inst_state_code": "FL",
  "inst_state_name": "Florida",
  "inst_phone_num": "4078230387",
  "inst_zip_code": "328168005",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "FL10",
  "org_lgl_bus_name": "THE UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RD7MXJV7DKT9"
 },
 "perf_inst": {
  "perf_inst_name": "The University of Central Florida Board of Trustees",
  "perf_str_addr": "4000 CENTRAL FLORIDA BLVD",
  "perf_city_name": "ORLANDO",
  "perf_st_code": "FL",
  "perf_st_name": "Florida",
  "perf_zip_code": "328168005",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "FL10",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 374973.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project is motivated by successful deployment of eScience applications on clouds: how to deploy scientific analytics applications on the cloud? Many scientists are exploring the possibilities of deploying applications with large scale of data on cloud computing platforms such as Amazon EC2 and Windows Azure. Recently, the successful deployment of eScience applications on clouds motivates us to deploy HPC analytics applications to the cloud. The reason behind this lies in a fact that eScience applications and HPC analytics applications share some important features: tera-scale or peta-scale data size and high cost to run on single or several supercomputers or large platforms. Both eScience applications and HPC analytics applications manipulate with tera-scale or peta-scale data and require access to expensive computing resources. However, HPC analytics applications bear several distinct characteristics such as complex data access patterns and interest locality, which pose new challenges to its adoption in clouds.</p>\n<p>In order to optimize the performance of HPC analytics on the cloud or Hadoop infrastructure, we construct a private cloud platform in Marmot cluster. Marmot is part of PRObE, an NSF-sponsored project providing a large-scale, low-level systems research facility. In this private cloud, we employ Virtual Machines (VMs) via various virtualization technologies, such as Xen, KVM, Linux Containers. From application users' perspectives, such VMs act as independent computing nodes, which resources, such as CPU cores, Memory, block I/O etc., can be adjusted at runtime and pay as it goes. In addition to VMs, the network facilities are also virtualized via state-of-the-art techniques. In our private cloud, the VMs are connected by Open vSwitch (OVS), which is an open-source implementation of a distributed virtual multilayer switch. Under the private cloud, we deployed two distinct Distributed File Systems (DFSs), Hadoop File Systemand Lustre file system. These DFSs are mainly used to provide storage services for running HPC analytics on the private cloud. &nbsp;</p>\n<p>Facilitated by the private cloud and storage infrastructure, the investigators develop a set of frameworks and middlewares to enable fast execution of HPC analytics applications on cloud. Firstly, we develop a translation layer framework based on a Unified I/O System (UNIO) to avoid data movement overhead for public cloud infrastructures. Our main idea is to enable both HPC simulation programs and analytics programs to run atop one cloud file system, e.g. Hadoop file system, a data-intensive file system (DIFS in brief). Secondly, we develop a new Data-gRouping-AWare (DRAW) data placement scheme for cloud storage and data intensive file system to address an interest locality issue. DRAW dynamically scrutinizes data access from system log files. It extracts optimal data groupings and re-organizes data layouts to achieve the maximum parallelism per group subjective to load balance. Thirdly, We propose a novel method to Optimize Parallel Data Access on Distributed File Systems referred to as Opass to reduce remote parallel data accesses and achieve a higher balance of data read requests between cluster nodes.</p>\n<p>In conclusion, this research project delivers indirect key outcomes for accelerating running eScience applications and HPC analytics on cloud environments. The outcomes include: 1) To solve the data migration problem in small-medium sized HPC clusters, we propose to construct a sided I/O path, named as SideIO, to explicitly direct analysis data to Hadoop file system that co-locates computing with data. In contrast, checkpoint data may not be read back later, it is written to the dedicated parallel file system to maximize I/O throughput. 2) We develop a new HPC analytics framework called NOHAA, to provide a semantics-aware intelligent data upload interface and a locality-aware hierarch...",
  "por_txt_cntn": "\nThis project is motivated by successful deployment of eScience applications on clouds: how to deploy scientific analytics applications on the cloud? Many scientists are exploring the possibilities of deploying applications with large scale of data on cloud computing platforms such as Amazon EC2 and Windows Azure. Recently, the successful deployment of eScience applications on clouds motivates us to deploy HPC analytics applications to the cloud. The reason behind this lies in a fact that eScience applications and HPC analytics applications share some important features: tera-scale or peta-scale data size and high cost to run on single or several supercomputers or large platforms. Both eScience applications and HPC analytics applications manipulate with tera-scale or peta-scale data and require access to expensive computing resources. However, HPC analytics applications bear several distinct characteristics such as complex data access patterns and interest locality, which pose new challenges to its adoption in clouds.\n\nIn order to optimize the performance of HPC analytics on the cloud or Hadoop infrastructure, we construct a private cloud platform in Marmot cluster. Marmot is part of PRObE, an NSF-sponsored project providing a large-scale, low-level systems research facility. In this private cloud, we employ Virtual Machines (VMs) via various virtualization technologies, such as Xen, KVM, Linux Containers. From application users' perspectives, such VMs act as independent computing nodes, which resources, such as CPU cores, Memory, block I/O etc., can be adjusted at runtime and pay as it goes. In addition to VMs, the network facilities are also virtualized via state-of-the-art techniques. In our private cloud, the VMs are connected by Open vSwitch (OVS), which is an open-source implementation of a distributed virtual multilayer switch. Under the private cloud, we deployed two distinct Distributed File Systems (DFSs), Hadoop File Systemand Lustre file system. These DFSs are mainly used to provide storage services for running HPC analytics on the private cloud.  \n\nFacilitated by the private cloud and storage infrastructure, the investigators develop a set of frameworks and middlewares to enable fast execution of HPC analytics applications on cloud. Firstly, we develop a translation layer framework based on a Unified I/O System (UNIO) to avoid data movement overhead for public cloud infrastructures. Our main idea is to enable both HPC simulation programs and analytics programs to run atop one cloud file system, e.g. Hadoop file system, a data-intensive file system (DIFS in brief). Secondly, we develop a new Data-gRouping-AWare (DRAW) data placement scheme for cloud storage and data intensive file system to address an interest locality issue. DRAW dynamically scrutinizes data access from system log files. It extracts optimal data groupings and re-organizes data layouts to achieve the maximum parallelism per group subjective to load balance. Thirdly, We propose a novel method to Optimize Parallel Data Access on Distributed File Systems referred to as Opass to reduce remote parallel data accesses and achieve a higher balance of data read requests between cluster nodes.\n\nIn conclusion, this research project delivers indirect key outcomes for accelerating running eScience applications and HPC analytics on cloud environments. The outcomes include: 1) To solve the data migration problem in small-medium sized HPC clusters, we propose to construct a sided I/O path, named as SideIO, to explicitly direct analysis data to Hadoop file system that co-locates computing with data. In contrast, checkpoint data may not be read back later, it is written to the dedicated parallel file system to maximize I/O throughput. 2) We develop a new HPC analytics framework called NOHAA, to provide a semantics-aware intelligent data upload interface and a locality-aware hierarchical storage system in support of co-located computation and storage on Windows Azure..."
 }
}