{
 "awd_id": "1134872",
 "agcy_id": "NSF",
 "tran_type": "CoopAgrmnt",
 "awd_istr_txt": "Cooperative Agreement",
 "awd_titl_txt": "Enabling, Enhancing, and Extending Petascale Computing for Science and Engineering",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032922247",
 "po_email": "rchadduc@nsf.gov",
 "po_sign_block_name": "Robert Chadduck",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 27500000.0,
 "awd_amount": 56000001.0,
 "awd_min_amd_letter_date": "2011-09-15",
 "awd_max_amd_letter_date": "2017-03-06",
 "awd_abstract_narration": "The current award is to The University of Texas at Austin to deploy and support Stampede, a HPC Linux cluster with an initial peak performance of 10 petaflops.  The system will have 6,400 Dell Stallion servers, with the servers connected by FDR Infiniband. Each server will have dual processors based on Intel?s forthcoming Sandy Bridge architecture with 32 GB of memory. The system will also include a pre-release version of Intel's forthcoming \"Knights Corner\" co-processors based on the Intel\u00ae Many Integrated Core ( Intel\u00ae MIC) Architecture: highly parallel co-processors that utilize the x86 instruction set. Stampede will also offer 128 next-generation NVIDIA GPUs for remote visualization, 16 nodes with 1TB of shared memory for large data analysis, and a high-performance file system with 14 petabytes of storage for data-intensive computing. All components will be integrated with a FDR InfiniBand network for extreme scalability. Altogether, Stampede will have a peak performance of over 10 petaflops and over 250 terabytes of memory. Second generation co-processors based on the Intel\u00ae MIC Architecture will be added when they become available, increasing Stampede's peak performance to at least 15 petaflops. \r\n\r\nThis national resource will be available in early 2013 through the NSF Cyberinfrastructure to enable basic research in science and engineering, and will be operated and supported for four years. The project will advance methods for petascale computing including Intel\u00ae MIC Architecture performance optimization, and will develop new expertise in data-intensive computing. The award will enable 1000+ projects in computational and data-driven science and engineering projects to advance knowledge in their fields.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Former Principal Investigator",
   "pi_first_name": "John",
   "pi_last_name": "Boisseau",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "John R Boisseau",
   "pi_email_addr": "boisseau@tacc.utexas.edu",
   "nsf_id": "000240621",
   "pi_start_date": "2011-09-15",
   "pi_end_date": "2014-03-24"
  },
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Stanzione",
   "pi_mid_init": "",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Daniel Stanzione",
   "pi_email_addr": "dan@tacc.utexas.edu",
   "nsf_id": "000193108",
   "pi_start_date": "2017-03-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Stanzione",
   "pi_mid_init": "",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Daniel Stanzione",
   "pi_email_addr": "dan@tacc.utexas.edu",
   "nsf_id": "000193108",
   "pi_start_date": "2011-09-15",
   "pi_end_date": "2014-03-24"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Karl",
   "pi_last_name": "Schulz",
   "pi_mid_init": "W",
   "pi_sufx_name": "",
   "pi_full_name": "Karl W Schulz",
   "pi_email_addr": "karl@ices.utexas.edu",
   "nsf_id": "000370250",
   "pi_start_date": "2011-09-15",
   "pi_end_date": "2014-03-24"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Tommy",
   "pi_last_name": "Minyard",
   "pi_mid_init": "K",
   "pi_sufx_name": "",
   "pi_full_name": "Tommy K Minyard",
   "pi_email_addr": "minyard@tacc.utexas.edu",
   "nsf_id": "000371000",
   "pi_start_date": "2011-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "William",
   "pi_last_name": "Barth",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "William L Barth",
   "pi_email_addr": "bbarth@tacc.utexas.edu",
   "nsf_id": "000596862",
   "pi_start_date": "2011-09-15",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Niall",
   "pi_last_name": "Gaffney",
   "pi_mid_init": "I",
   "pi_sufx_name": "",
   "pi_full_name": "Niall I Gaffney",
   "pi_email_addr": "ngaffney@tacc.utexas.edu",
   "nsf_id": "000668354",
   "pi_start_date": "2014-03-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "PO Box 7726",
  "perf_city_name": "Austin",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787137726",
  "perf_ctry_code": "US",
  "perf_cong_dist": "37",
  "perf_st_cong_dist": "TX37",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "761900",
   "pgm_ele_name": "Innovative HPC"
  },
  {
   "pgm_ele_code": "778100",
   "pgm_ele_name": "Leadership-Class Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7619",
   "pgm_ref_txt": "EQUIPMENT ACQUISITIONS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0117",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001718DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 27500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 24000000.0
  },
  {
   "fund_oblg_fiscal_yr": 2017,
   "fund_oblg_amt": 4500001.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project deployed the Stampede Supercomputer, which debuted as the sixth fastest machine in the world in late 2012, and became the workhorse of the NSF XSEDE computing ecosystem from 2013 through 2017.&nbsp; &nbsp;In it's initial configuration, Stampede delivered 9.6PF of peak performance, and was upgraded to more than 11PF in 2015.&nbsp; &nbsp;Stampede was deployed at the Texas Advanced Computing Center at the University of Texas at Austin, in partnership with Dell Technologies, Intel, and Mellanox.&nbsp; Operations support was also provided by academic partners at Clemson University, Cornell University, Indiana University, The Ohio State University, the University of Colorado at Boulder, and the University of Texas at El Paso.&nbsp;</p>\n<p>Over it's lifetime, Stampede served tens of thousands of researchers.&nbsp; By the time the project was finished, Stampede had run more than 8.4 million simulation and data analysis jobs, and had delivered more than 3.4 billion core hours of computation.&nbsp; Nearly 4,000 different projects made use of Stampede; almost 13,000 researchers directly ran jobs on the machine, and tens of thousands more used the machine indirectly through web-based \"Science Gateway\" interfaces.&nbsp; &nbsp;While Stampede delivered enormous capacity, demand was even higher, with requests for time on the machine from the research community each quarter exceeded capacity by a ratio of more than five to one.&nbsp; &nbsp;User's requesting renewal time on the machine cited over 9,500 papers as being results of their computational research.&nbsp;</p>\n<p>Stampede was involved in the solution of many of the largest computational challenges of the last five years.&nbsp; &nbsp;Among the most notable was the role the project played in assisting the LIGO team in their Nobel-prize winning observation of gravitational waves.&nbsp; Stampede performed some of the data analysis calculations for this discovery, and the Stampede team helped to boost performance of the LIGO software stack on both Stampede and other systems.&nbsp; Stampede was also used in support of the scientific computations for the Gordon Bell winning mantle convection run in 2015, which led to groundbreaking results in understanding the connections between mantle convection and continental drift.&nbsp; Stampede was used by industry to develop small satellite launch rockets and offshore oil platforms. Stampede was a key resource in natural hazard response, used extensively in hurricane, tornado, and earthquake forecasting. Stampede was a key enabler in numerous other key science, engineering, and health applications, from analyzing data from the Large Hadron Collider at CERN, to simulating blood flow in capillaries, simulating galaxy formation in the early universe, and development of new nanomaterials.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/03/2018<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Stanzione</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2018/1134872/1134872_10135438_1525233874482_Stampede--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2018/1134872/1134872_10135438_1525233874482_Stampede--rgov-800width.jpg\" title=\"The Stampede Supercomputer\"><img src=\"/por/images/Reports/POR/2018/1134872/1134872_10135438_1525233874482_Stampede--rgov-66x44.jpg\" alt=\"The Stampede Supercomputer\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Front row of the Stampede Supercomputer at the time of original deployment</div>\n<div class=\"imageCredit\">Texas Advanced Computing Center</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Daniel&nbsp;Stanzione</div>\n<div class=\"imageTitle\">The Stampede Supercomputer</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nThis project deployed the Stampede Supercomputer, which debuted as the sixth fastest machine in the world in late 2012, and became the workhorse of the NSF XSEDE computing ecosystem from 2013 through 2017.   In it's initial configuration, Stampede delivered 9.6PF of peak performance, and was upgraded to more than 11PF in 2015.   Stampede was deployed at the Texas Advanced Computing Center at the University of Texas at Austin, in partnership with Dell Technologies, Intel, and Mellanox.  Operations support was also provided by academic partners at Clemson University, Cornell University, Indiana University, The Ohio State University, the University of Colorado at Boulder, and the University of Texas at El Paso. \n\nOver it's lifetime, Stampede served tens of thousands of researchers.  By the time the project was finished, Stampede had run more than 8.4 million simulation and data analysis jobs, and had delivered more than 3.4 billion core hours of computation.  Nearly 4,000 different projects made use of Stampede; almost 13,000 researchers directly ran jobs on the machine, and tens of thousands more used the machine indirectly through web-based \"Science Gateway\" interfaces.   While Stampede delivered enormous capacity, demand was even higher, with requests for time on the machine from the research community each quarter exceeded capacity by a ratio of more than five to one.   User's requesting renewal time on the machine cited over 9,500 papers as being results of their computational research. \n\nStampede was involved in the solution of many of the largest computational challenges of the last five years.   Among the most notable was the role the project played in assisting the LIGO team in their Nobel-prize winning observation of gravitational waves.  Stampede performed some of the data analysis calculations for this discovery, and the Stampede team helped to boost performance of the LIGO software stack on both Stampede and other systems.  Stampede was also used in support of the scientific computations for the Gordon Bell winning mantle convection run in 2015, which led to groundbreaking results in understanding the connections between mantle convection and continental drift.  Stampede was used by industry to develop small satellite launch rockets and offshore oil platforms. Stampede was a key resource in natural hazard response, used extensively in hurricane, tornado, and earthquake forecasting. Stampede was a key enabler in numerous other key science, engineering, and health applications, from analyzing data from the Large Hadron Collider at CERN, to simulating blood flow in capillaries, simulating galaxy formation in the early universe, and development of new nanomaterials. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/03/2018\n\n\t\t\t\t\tSubmitted by: Daniel Stanzione"
 }
}