{
 "awd_id": "1117000",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Collaborative Research: Learning to perform consistently in human/multi-robot teams",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Weng-keen Wong",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 191412.0,
 "awd_amount": 223412.0,
 "awd_min_amd_letter_date": "2011-07-07",
 "awd_max_amd_letter_date": "2013-04-26",
 "awd_abstract_narration": "This project focuses on practical deployment of human/multi-robot teams in situations where robots can explore regions that are unsuitable for humans. For example, a team of \"rescue\" robots can sweep through a collapsed building searching for victims and transmit their positions to human first-responders outside. Managing a human/multi-robot team in a dynamic environment is a challenging problem. Not only is the world mutable, but also the team can experience altered membership because a robot gets lost or a human operator needs rest---the world is changing, and so is the team that is exploring that world.\r\n\r\nThe goal of this research is to develop strategies for human/multi-robot teams to learn to perform consistently and effectively. Three primary aims will be pursued: first, to mitigate changes in team composition via a practical framework for institutional memory that remembers and uses past experiences; second, to model and record expertise for later use by learning behaviors performed by a human operator; and third, to distribute tasks among team members efficiently by providing a balanced mechanism for social choice. The novel approach of this project is applicable to a broad spectrum of human/multi-robot, and human/multi-agent teams, by integrating institutional memory, learning from human teammates, and resolving conflict among differing perspectives. The strategies will be evaluated using a human/multi-robot testbed comprised of one human operator plus a heterogeneous set of inexpensive, limited-function robots. Although each individual robot has restricted mobility and sensing capabilities, together the team members constitute a multi-function, human/multi-robot facility.\r\n\r\nThis project addresses important challenges in robust intelligence, including behavior modeling, learning from experience, making coordinated decisions, and reasoning under uncertainty. Expected outcomes include strategies for human/multi-robot teams that learn to collaborate effectively under a variety of conditions and can maintain their performance despite run-time changes in team membership, as well as knowledge about how people interact with robot teams. Broader impacts include providing access to a networked experimental testbed for remote collaborators; publishing proven curricular materials on multi-robot teams addressed to graduate, undergraduate and high school students; involving undergraduates in research activities; and working with existing contacts at local museums to demonstrate results to the general public.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Susan",
   "pi_last_name": "Epstein",
   "pi_mid_init": "L",
   "pi_sufx_name": "",
   "pi_full_name": "Susan L Epstein",
   "pi_email_addr": "susan.epstein@hunter.cuny.edu",
   "nsf_id": "000173560",
   "pi_start_date": "2011-07-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "CUNY Hunter College",
  "inst_street_address": "695 PARK AVE",
  "inst_street_address_2": "",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2127724020",
  "inst_zip_code": "100655024",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NY12",
  "org_lgl_bus_name": "RESEARCH FOUNDATION OF THE CITY UNIVERSITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "EK93EZLLBSC4"
 },
 "perf_inst": {
  "perf_inst_name": "CUNY Hunter College",
  "perf_str_addr": "695 PARK AVE",
  "perf_city_name": "NEW YORK",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100655024",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NY12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 191412.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><h1>Project Outcomes Report</h1>\n<p><strong>Learning to perform consistently in human/multi-robot teams</strong></p>\n<p>&nbsp;</p>\n<p>Imagine a person and a group of autonomous robots that must travel efficiently together through a large, complex, or rapidly changing unknown indoor space. The team could be explorers in an unknown environment, for example, or emergency service providers with specific targets (e.g., victims whose cellphones&rsquo; signals identify their locations). If the environment is dangerous or the team is in a hurry, it may be impractical to have the robot map the space first. This scenario inspired <em>HRTeam, </em>a collaborative project on robot navigation that replaces a few expensive, teleoperated robots with multiple, autonomous, inexpensive ones. Together with our collaborators, we formulated a robot environment and a vision of how a human-robot team might interact and share its knowledge. The portion of the joint project addressed by this grant produced <em>SemaFORR</em>, a decision-making component for an autonomous robot that navigates to indoor targets without reliance on a map.</p>\n<p>SemaFORR has been tested extensively both on three different robot platforms and in simulation. It supports travel both in physical environments and in simulation of large, complex two-dimensional spaces. SemaFORR has supported as many as eight robots in the same space simultaneously as they travel to their separate lists of targets without collisions. SemaFORR&rsquo;s decisions are based on what it senses and on heuristics that respond to fundamental navigational principles and what the system learns.</p>\n<p>SemaFORR learns a spatial model of an indoor environment as it travels through it. This model is readily understandable both by people and by any robot that uses SemaFORR to make decisions. An example of such a model constructed by a robot appears in the accompanying figure. The model captures what the robot has perceived and learned from its travel experience in the environment delineated by the black walls. A pink circle in the spatial model is a <em>region</em>, an open space with black dots along its perimeter that record experienced entry and exit points. A green square in this model is a <em>conveyor, </em>a transit point that was often used successfully in earlier travel.&nbsp; A sequence of blue lines connected by red dots is a <em>trail</em>, a reasonably efficient revision of a past route, with more perceptual detail at its (red dot) <em>trail markers.</em> The model grows and changes as the robot travels, in real time. (Changes in the walls are recorded in the model as the robot observes them.)</p>\n<p>In some spaces a SemaFORR-based robot that simply reacts to its sensor data performs just as well as if it had planned its path in an accurate map. Even in simulation of an entire floor in a large office building, the approach proves resilient. Furthermore, any robot that decides with SemaFORR can replace another SemaFORR-based robot seamlessly &mdash; the spatial model provides institutional memory.</p>\n<p><strong>Intellectual merit:</strong> SemaFORR is a single robust, consistent, reactive decision maker that is flexible enough to operate autonomously on a variety of robot platforms, both in built spaces and in simulation. This purely reactive component is compact and fast enough to make decisions and learn in real time on inexpensive hardware. It supports teams of multiple robots individually, and learns a spatial model that can be shared with a person, along with the reasons for its decisions.</p>\n<p><strong>Broader impacts: </strong>Our results have been reported in academic journals and at conferences, as well as at an invited, extended talk with a full day&rsquo;s discussion at Rensselaer Polytechnic Institute. The broader impacts of this work include the potential for a spatial model like SemaFORR&rsquo;s to become the foundation of communication about navigation within human-robot teams. We believe that the model is so comfortable for people because it has (unintentional) analogs in the recent neuroscience literature, specifically in recent the Nobel Prize work on evidence that the human hippocampus has cells specialized for quantitative representation of places, routes, and behavioral experience. This grant also supported the education of five undergraduate students and the training of three Ph.D. students, two of whom are now engaged in thesis work in cognitive robotics.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/19/2016<br>\n\t\t\t\t\tModified by: Susan&nbsp;L&nbsp;Epstein</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls onePhoto\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation onePhoto\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1117000/1117000_10106154_1471627927058_NSFpicture--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1117000/1117000_10106154_1471627927058_NSFpicture--rgov-800width.jpg\" title=\"Spatial model\"><img src=\"/por/images/Reports/POR/2016/1117000/1117000_10106154_1471627927058_NSFpicture--rgov-66x44.jpg\" alt=\"Spatial model\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Learned by an autonomous robot as it navigated to 40 targets. A full explanation appears in the accompanying text.</div>\n<div class=\"imageCredit\">Springer-Verlag</div>\n<div class=\"imagePermisssions\">Copyright owner is an institution with an existing agreement allowing use by NSF</div>\n<div class=\"imageSubmitted\">Susan&nbsp;L&nbsp;Epstein</div>\n<div class=\"imageTitle\">Spatial model</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "Project Outcomes Report\n\nLearning to perform consistently in human/multi-robot teams\n\n \n\nImagine a person and a group of autonomous robots that must travel efficiently together through a large, complex, or rapidly changing unknown indoor space. The team could be explorers in an unknown environment, for example, or emergency service providers with specific targets (e.g., victims whose cellphones? signals identify their locations). If the environment is dangerous or the team is in a hurry, it may be impractical to have the robot map the space first. This scenario inspired HRTeam, a collaborative project on robot navigation that replaces a few expensive, teleoperated robots with multiple, autonomous, inexpensive ones. Together with our collaborators, we formulated a robot environment and a vision of how a human-robot team might interact and share its knowledge. The portion of the joint project addressed by this grant produced SemaFORR, a decision-making component for an autonomous robot that navigates to indoor targets without reliance on a map.\n\nSemaFORR has been tested extensively both on three different robot platforms and in simulation. It supports travel both in physical environments and in simulation of large, complex two-dimensional spaces. SemaFORR has supported as many as eight robots in the same space simultaneously as they travel to their separate lists of targets without collisions. SemaFORR?s decisions are based on what it senses and on heuristics that respond to fundamental navigational principles and what the system learns.\n\nSemaFORR learns a spatial model of an indoor environment as it travels through it. This model is readily understandable both by people and by any robot that uses SemaFORR to make decisions. An example of such a model constructed by a robot appears in the accompanying figure. The model captures what the robot has perceived and learned from its travel experience in the environment delineated by the black walls. A pink circle in the spatial model is a region, an open space with black dots along its perimeter that record experienced entry and exit points. A green square in this model is a conveyor, a transit point that was often used successfully in earlier travel.  A sequence of blue lines connected by red dots is a trail, a reasonably efficient revision of a past route, with more perceptual detail at its (red dot) trail markers. The model grows and changes as the robot travels, in real time. (Changes in the walls are recorded in the model as the robot observes them.)\n\nIn some spaces a SemaFORR-based robot that simply reacts to its sensor data performs just as well as if it had planned its path in an accurate map. Even in simulation of an entire floor in a large office building, the approach proves resilient. Furthermore, any robot that decides with SemaFORR can replace another SemaFORR-based robot seamlessly &mdash; the spatial model provides institutional memory.\n\nIntellectual merit: SemaFORR is a single robust, consistent, reactive decision maker that is flexible enough to operate autonomously on a variety of robot platforms, both in built spaces and in simulation. This purely reactive component is compact and fast enough to make decisions and learn in real time on inexpensive hardware. It supports teams of multiple robots individually, and learns a spatial model that can be shared with a person, along with the reasons for its decisions.\n\nBroader impacts: Our results have been reported in academic journals and at conferences, as well as at an invited, extended talk with a full day?s discussion at Rensselaer Polytechnic Institute. The broader impacts of this work include the potential for a spatial model like SemaFORR?s to become the foundation of communication about navigation within human-robot teams. We believe that the model is so comfortable for people because it has (unintentional) analogs in the recent neuroscience literature, specifically in recent the Nobel Prize work on evidence that the human hippocampus has cells specialized for quantitative representation of places, routes, and behavioral experience. This grant also supported the education of five undergraduate students and the training of three Ph.D. students, two of whom are now engaged in thesis work in cognitive robotics.\n\n \n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 08/19/2016\n\n\t\t\t\t\tSubmitted by: Susan L Epstein"
 }
}