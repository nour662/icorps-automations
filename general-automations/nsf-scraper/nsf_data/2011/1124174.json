{
 "awd_id": "1124174",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Non-Standard Asymptotic Theory for Semiparametric Estimators",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Georgia Kosmopoulou",
 "awd_eff_date": "2011-08-15",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 129747.0,
 "awd_amount": 129747.0,
 "awd_min_amd_letter_date": "2011-08-11",
 "awd_max_amd_letter_date": "2011-08-11",
 "awd_abstract_narration": "Modern statistical and econometric models in social and natural sciences are complex and typically include many unknown parameters. Some of these parameters are of particular interest to the researchers and policymakers (e.g., the mean effect of a treatment), while others are not (e.g., the exact form of a regression function or the probability law of the observed covariates). The latter parameters are usually called nuisance parameters because their values are needed in order to conduct valid statistical inference on the parameters of interest, even though the researchers are not interested in them. An important class of these models are the so-called semiparametric models, which have the distinctive feature that the nuisance parameters are functions (as opposed to numbers). A modern, well-established approach in statistics and econometrics to conduct inference in semiparametric models is to estimate in a flexible, non-parametric way the nuisance parameters first (using the available data), and then employ these estimates as a preliminary guess of their true value in order to conduct inference on the parameters of interest. This procedure is generically referred to as semiparametric inference, and is particularly useful because of its flexibility and lack of sensitivity to biases generated by model misspecification.\r\n\r\nSemiparametric inference procedures are very popular among theoretical researchers, partially because of their nice and well understood large sample properties (approximations that assume a large amount of data). However, these inference procedures are considerably less popular among empirical researchers and policymakers, mainly because they are known to be highly sensitive to the way that they are implemented in practice. Specifically, an important drawback of most semiparametric inference procedures is that they rely on non-parametric techniques for the estimation of the nuisance parameters, which in turn require the selection of tuning and smoothing parameters. These additional parameters are artificially introduced in the inference procedure to flexibly approximate the unknown functions (the nuisance parameters). The large sample approximations employed in the literature ignore the effect of these additional parameters that are artificially introduced in the construction of the inference procedure. This fact, in turn, leads to an important lack of robustness of semiparametric inference procedures, that is, small changes in the choice of tuning and smoothing parameters lead to dramatically different empirical results, making applied work unreliable in general. In other words, this lack of robustness usually translates in incorrect statistical inference that may lead researchers and policymakers to draw flawed conclusions from empirical work that employs these semiparametric inference procedures.\r\n\r\nThe main goal of the proposed research agenda is to develop new, alternative large sample approximations to commonly used semiparametric inference procedures that (at least partially) account for the effect of the specific user-defined choices of tuning and smoothing parameters involved in the inference procedure. This alternative asymptotic theory leads to more \"robust\" statistical inference procedures because it captures the effect of certain terms that are assumed away by the conventional large sample approximations. This project will proceed in two main stages. First, alternative large sample approximations will be developed for specific semiparametric examples, including weighted averaged derivatives and partially linear model. Not only these models are of interest in their own right, but also they will provide some of the key ingredients to understand the new theoretical features emerging from the non-standard large sample approximations studied in this proposal. Among other problems, the goal is to establish an alternative first-order large sample distribution, derive valid standard-error estimators, develop new ways of selecting the value of the tuning and smoothing parameters, study the validity of commonly used resampling procedures, and explore the higher-order implications of the alternative asymptotic approximations. Once the study of these particular semiparametric procedures is well understood, the second stage of the investigation will be to develop a generalization and unification of the theoretical results outlined for the special examples, which will cover many other problems of interest.\r\n\r\nThe results of this research are expected to benefit several fields of study, ranging from Economics or Political Science to Biostatistics or Public Health, allowing researchers to conduct \"robust\" inference in semiparametric models, and making semiparametric inference more attractive to researchers and policymakers. To further increase the impact of this research proposal, a key goal is to provide computer code for commonly used platforms, and to write a non-technical survey with a discussion on theory and implementation of both the classical results and the new results emerging from the research proposed.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Jansson",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Jansson",
   "pi_email_addr": "mjansson@econ.berkeley.edu",
   "nsf_id": "000522209",
   "pi_start_date": "2011-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "Sponsored Projects Office",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947045940",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "132000",
   "pgm_ele_name": "Economics"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 129747.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Conducting credible empirical work in Economics, and other social sciences, is one of the most important and difficult tasks in both academic and policy work. Academic researchers and policy-makers prefer statistical inference procedures that are both flexible and reliable when used in empirical work. Unfortunately, flexibility often comes at the price of less robustness: many econometric and statistical procedures in Economics and other sciences require the choice of tuning and other parameters that make these procedures quite sensitive in applications. The main goal of this grant was to introduce and develop a new large-sample distribution theory for econometric procedures that can be used to construct new econometric and statistical procedures which are more more credible (that is, more robust) in applications. Specifically, the grant developed new interval estimators for parameters related to treatment effects (i.e., the effect of a policy or intervention on some outcome of interest) for two classes of models: (1) weighted average derivatives and (2) linear regression models with many covariates. These models are commonly used in empirical work related to program evaluation, and thus results from this grant may be useful for a variety of fields ranging from Economics or Political Science to Public Policy or Public Health.</p>\n<p>While the research work underlying this grant was mostly theoretical in nature, several practical results of importance for empirical work were also obtained. These are: (1) new standard errors and confidence intervals formulas for weighted average derivatives, (2) new standard errors and confidence intervals formulas for linear regression models with many covariates, (3) new inference procedures based on resampling techniques (also known as Bootstrap) for weighted average derivatives estimators, (4) new tuning parameter selectors for different estimators in the models mentioned previously, and (5) new software for estimation (and simulation evidence) for the models mentioned previously.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/07/2014<br>\n\t\t\t\t\tModified by: Michael&nbsp;Jansson</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nConducting credible empirical work in Economics, and other social sciences, is one of the most important and difficult tasks in both academic and policy work. Academic researchers and policy-makers prefer statistical inference procedures that are both flexible and reliable when used in empirical work. Unfortunately, flexibility often comes at the price of less robustness: many econometric and statistical procedures in Economics and other sciences require the choice of tuning and other parameters that make these procedures quite sensitive in applications. The main goal of this grant was to introduce and develop a new large-sample distribution theory for econometric procedures that can be used to construct new econometric and statistical procedures which are more more credible (that is, more robust) in applications. Specifically, the grant developed new interval estimators for parameters related to treatment effects (i.e., the effect of a policy or intervention on some outcome of interest) for two classes of models: (1) weighted average derivatives and (2) linear regression models with many covariates. These models are commonly used in empirical work related to program evaluation, and thus results from this grant may be useful for a variety of fields ranging from Economics or Political Science to Public Policy or Public Health.\n\nWhile the research work underlying this grant was mostly theoretical in nature, several practical results of importance for empirical work were also obtained. These are: (1) new standard errors and confidence intervals formulas for weighted average derivatives, (2) new standard errors and confidence intervals formulas for linear regression models with many covariates, (3) new inference procedures based on resampling techniques (also known as Bootstrap) for weighted average derivatives estimators, (4) new tuning parameter selectors for different estimators in the models mentioned previously, and (5) new software for estimation (and simulation evidence) for the models mentioned previously.\n\n\t\t\t\t\tLast Modified: 08/07/2014\n\n\t\t\t\t\tSubmitted by: Michael Jansson"
 }
}