{
 "awd_id": "1118122",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Small: Generalized Anytime Probabilistic Inference",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032924481",
 "po_email": "hmunoz@nsf.gov",
 "po_sign_block_name": "Hector Munoz-Avila",
 "awd_eff_date": "2011-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 449426.0,
 "awd_amount": 449426.0,
 "awd_min_amd_letter_date": "2011-08-16",
 "awd_max_amd_letter_date": "2011-08-16",
 "awd_abstract_narration": "Probabilistic reasoning is now routinely used in many fields of science and engineering, where it underlies systems that perform tasks such as text classification, social network analysis, medical diagnosis, information extraction, probabilistic planning, vision and robotics. This project aims to develop an anytime and generalized probabilistic inference engine that targets a wide range of probabilistic representations, including classical, propositional representations --- such as Bayesian and Markov networks --- in addition to more expressive representations based on first order logic. The project will also investigate probabilistic queries in complexity classes that have not received much attention in the literature, yet can be used to study the robustness of inferences and decisions based on probabilistic reasoning systems. The targeted inference engine is planned to put the state of the art in exact inference at the service of approximate inference, allowing it to resign to approximations only when exact inference yields. Moreover, the engine is planned to smoothly and incrementally improve its approximations over time. A key emphasis of the project is to accomplish these objectives while using the most general probabilistic representation as an input, to allow for the widest possible adoption of the developed inference engine. Our anticipated results will impact many fields by allowing users to perform more accurate probabilistic inference, on larger models and in different contexts. Through scientific articles, research seminars, conference presentations, and graduate teaching, we expect the obtained results to be widely disseminated so as to maximize the attained benefits by various communities. Moreover, we plan to publicly release software systems that embed our results, continuing with a long tradition of publicly releasing award-winning software systems for probabilistic reasoning.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Adnan",
   "pi_last_name": "Darwiche",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Adnan Darwiche",
   "pi_email_addr": "darwiche@cs.ucla.edu",
   "nsf_id": "000440042",
   "pi_start_date": "2011-08-16",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 WILSHIRE BLVD STE 700",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900244200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 449426.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project investigated a general framework for approximate probabilistic inference that reduces approximate inference to exact inference on an approximate model. The framework is known as Reduce-Compensate-Recover (RCR). The project focused on two main objectives relating to RCR. First, providing a more formal treatment of first-order probabilistic inference to allow the application of RCR in a first-order setting. Second, developing&nbsp;logic-based compilation techniques to support exact inference for RCR, through reduction to weighted model counting. The project also investigated algorithms and applications of PP^PP-complete tasks, including the computation of Same-Decision Probability (SDP), which is PP^PP-complete.</p>\n<div>On first-order probabilistic inference: We pursued an approach for quantifier elimination (universal and existential) to standardize the process of first-order probabilistic inference using weighted model counting. In particular, we developed&nbsp;a skolimization techniques for first-order logic that preservers the weighted model count. To our knowledge, this has provided the first systematic method for full quantifier elimination in the context of first-order probabilistic inference.&nbsp;The proposed technique can be thought of as generalizing the classical and influential skolimization technique that preserves satisfiability in first-order logic, and that forms the basis for performing first-order resolution. We also developed a first-order formulation and algorithm of RCR, known as lifted RCR.</div>\n<div>\n<p>On compilation techniques: We developed new algorithms and complexity results for knowledge compilation, which is a main approach for performing weighted model counting and, hence, exact probabilistic inference. In particular, we identified two new complexity measures, CV-width and Decision-width, which can be used to provide upper bounds on the complexity of knowledge compilation. We also developed algorithms for knowledge compilation, whose complexity can be bounded by the newly identified complexity measures. We also developed a new method for compiling Bayesian networks into Sentential Decision Diagrams (SDDs), to further the reach of exact probabilistic inference using weighted model counting.</p>\n<div>On PP^PP-complete tasks: We developed algorithms for computing the SDP in Naive Bayes classifiers and Bayesian networks more generally. We also pursued some applications of the SDP, including feature selection (i.e., gathering information) where we proposed maximizing the expected SDP, leading to robust decisions and classifications. Another application we pursued is using the SDP for computer adaptive testing, where we conducted a case study for classifying the competence of dentistry volunteers.</div>\n</div>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/16/2016<br>\n\t\t\t\t\tModified by: Adnan&nbsp;Darwiche</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project investigated a general framework for approximate probabilistic inference that reduces approximate inference to exact inference on an approximate model. The framework is known as Reduce-Compensate-Recover (RCR). The project focused on two main objectives relating to RCR. First, providing a more formal treatment of first-order probabilistic inference to allow the application of RCR in a first-order setting. Second, developing logic-based compilation techniques to support exact inference for RCR, through reduction to weighted model counting. The project also investigated algorithms and applications of PP^PP-complete tasks, including the computation of Same-Decision Probability (SDP), which is PP^PP-complete.\nOn first-order probabilistic inference: We pursued an approach for quantifier elimination (universal and existential) to standardize the process of first-order probabilistic inference using weighted model counting. In particular, we developed a skolimization techniques for first-order logic that preservers the weighted model count. To our knowledge, this has provided the first systematic method for full quantifier elimination in the context of first-order probabilistic inference. The proposed technique can be thought of as generalizing the classical and influential skolimization technique that preserves satisfiability in first-order logic, and that forms the basis for performing first-order resolution. We also developed a first-order formulation and algorithm of RCR, known as lifted RCR.\n\n\nOn compilation techniques: We developed new algorithms and complexity results for knowledge compilation, which is a main approach for performing weighted model counting and, hence, exact probabilistic inference. In particular, we identified two new complexity measures, CV-width and Decision-width, which can be used to provide upper bounds on the complexity of knowledge compilation. We also developed algorithms for knowledge compilation, whose complexity can be bounded by the newly identified complexity measures. We also developed a new method for compiling Bayesian networks into Sentential Decision Diagrams (SDDs), to further the reach of exact probabilistic inference using weighted model counting.\nOn PP^PP-complete tasks: We developed algorithms for computing the SDP in Naive Bayes classifiers and Bayesian networks more generally. We also pursued some applications of the SDP, including feature selection (i.e., gathering information) where we proposed maximizing the expected SDP, leading to robust decisions and classifications. Another application we pursued is using the SDP for computer adaptive testing, where we conducted a case study for classifying the competence of dentistry volunteers.\n\n\n \n\n\t\t\t\t\tLast Modified: 05/16/2016\n\n\t\t\t\t\tSubmitted by: Adnan Darwiche"
 }
}