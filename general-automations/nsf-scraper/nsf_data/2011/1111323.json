{
 "awd_id": "1111323",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-08-15",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 385310.0,
 "awd_amount": 385310.0,
 "awd_min_amd_letter_date": "2011-08-11",
 "awd_max_amd_letter_date": "2011-08-11",
 "awd_abstract_narration": "This research involves collaboration among investigators at three institutions.  The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks.  To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot.  Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs.  In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.\r\n\r\nThe Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space.  The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions.  Speech is a natural though demanding way to use natural language to communicate with a robot.  To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information.  This project will answer three scientific questions.\r\n\r\n(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? \r\n(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? \r\n3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?\r\n\r\nTo these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely.  To inform the design process, the PIs will conduct focus groups with potential users.  They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.\r\n\r\nBroader Impacts:  To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively.  It must also be able to communicate effectively with other agents, and particularly with people.  This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research.  Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability.  This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age).  It will also support telepresence applications such as telecommuting, telemedicine and search and rescue.  The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Matthias",
   "pi_last_name": "Scheutz",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Matthias J Scheutz",
   "pi_email_addr": "matthias.scheutz@tufts.edu",
   "nsf_id": "000289027",
   "pi_start_date": "2011-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Tufts University",
  "inst_street_address": "80 GEORGE ST",
  "inst_street_address_2": "",
  "inst_city_name": "MEDFORD",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6176273696",
  "inst_zip_code": "021555519",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "MA05",
  "org_lgl_bus_name": "TRUSTEES OF TUFTS COLLEGE",
  "org_prnt_uei_num": "WL9FLBRVPJJ7",
  "org_uei_num": "WL9FLBRVPJJ7"
 },
 "perf_inst": {
  "perf_inst_name": "Tufts University",
  "perf_str_addr": "80 GEORGE ST",
  "perf_city_name": "MEDFORD",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021555519",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "MA05",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 385310.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>We anticipate a future in which humans and intelligent robots will collaborate on shared tasks. To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between the human and the robot. Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs as in the case of a robotic wheelchair.&nbsp; According to the 2011 census, the US Centers for Disease Control estimates approximated 3.4 million Americans are living with a mobility disability and roughly half of these require a wheelchair.&nbsp; And even for those who are able to self propel their wheelchair, the ability to have a robotic wheelchair, particularly as they age, would provide a key benefit.</p>\n<p>The goal of this project was to enable natural verbal human-robot interactions&nbsp; with a robotic wheelchair which allow human users to instruct the robot in natural language rather than using a joy stick control interface.&nbsp; We developed several novel algorithms that support various aspects of natural language understanding necessary for such a robotic platform, from integrating visual information with natural language, to resolving references to locations unknown to the robot based on its map, to understanding requests that are hidden in indirect speech acts.&nbsp; We also developed a data set from human subject experiments that can be used to further study the interaction between human gestures and human natural language interactions when humans give route instructions sitting in the wheelchair.</p>\n<p>Both the algorithms and the empirical datasets are prerequisite for developing the next generation of intelligent autonomous wheelchairs that can be instructed&nbsp; verbally in natural ways.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/01/2014<br>\n\t\t\t\t\tModified by: Matthias&nbsp;J&nbsp;Scheutz</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWe anticipate a future in which humans and intelligent robots will collaborate on shared tasks. To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between the human and the robot. Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs as in the case of a robotic wheelchair.  According to the 2011 census, the US Centers for Disease Control estimates approximated 3.4 million Americans are living with a mobility disability and roughly half of these require a wheelchair.  And even for those who are able to self propel their wheelchair, the ability to have a robotic wheelchair, particularly as they age, would provide a key benefit.\n\nThe goal of this project was to enable natural verbal human-robot interactions  with a robotic wheelchair which allow human users to instruct the robot in natural language rather than using a joy stick control interface.  We developed several novel algorithms that support various aspects of natural language understanding necessary for such a robotic platform, from integrating visual information with natural language, to resolving references to locations unknown to the robot based on its map, to understanding requests that are hidden in indirect speech acts.  We also developed a data set from human subject experiments that can be used to further study the interaction between human gestures and human natural language interactions when humans give route instructions sitting in the wheelchair.\n\nBoth the algorithms and the empirical datasets are prerequisite for developing the next generation of intelligent autonomous wheelchairs that can be instructed  verbally in natural ways.\n\n\t\t\t\t\tLast Modified: 09/01/2014\n\n\t\t\t\t\tSubmitted by: Matthias J Scheutz"
 }
}