{
 "awd_id": "1111407",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Large: Collaborative Research: Kali:  A System for Sequential Programming of Multicore Processors",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Anita La Salle",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2017-07-31",
 "tot_intn_awd_amt": 905887.0,
 "awd_amount": 905887.0,
 "awd_min_amd_letter_date": "2011-08-06",
 "awd_max_amd_letter_date": "2011-08-06",
 "awd_abstract_narration": "Most computers today are 'multicore' parallel computers that are capable of executing several independent threads of computation simultaneously. Unfortunately, most existing programs are not parallel and cannot take advantage of this hardware capability. Furthermore, writing parallel programs using current notations like OpenMP is more difficult than writing sequential programs and, as a result, increases development costs and the likelihood of program defects.\r\n\r\nThe Kali project is building a software system that will permit most application programmers to write sequential programs and still obtain good performance on multicore processors. Parallelism will be hidden within object-oriented class libraries written by expert parallel programmers and it will be managed by a sophisticated runtime system that uses a range of parallel execution strategies customized to the needs of the application. Applications programmers can take advantage of the benefits of sequential programming such as familiarity, readability, maintainability, and debuggability. They will also be able to tune program performance and power without having to drop down to a lower abstraction level. In addition, the Kali project is studying the use of innovative hardware to facilitate the development of efficient programs. Finally, the project is producing a suite of application benchmarks that will be useful for performance evaluation of similar systems.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "David",
   "pi_last_name": "Padua",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "David A Padua",
   "pi_email_addr": "padua@uiuc.edu",
   "nsf_id": "000317715",
   "pi_start_date": "2011-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Maria",
   "pi_last_name": "Garzaran",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Maria J Garzaran",
   "pi_email_addr": "garzaran@cs.uiuc.edu",
   "nsf_id": "000297030",
   "pi_start_date": "2011-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Illinois at Urbana-Champaign",
  "inst_street_address": "506 S WRIGHT ST",
  "inst_street_address_2": "",
  "inst_city_name": "URBANA",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "2173332187",
  "inst_zip_code": "618013620",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "IL13",
  "org_lgl_bus_name": "UNIVERSITY OF ILLINOIS",
  "org_prnt_uei_num": "V2PHZ2CSCH63",
  "org_uei_num": "Y8CWNJRCNN91"
 },
 "perf_inst": {
  "perf_inst_name": "University of Illinois at Urbana-Champaign",
  "perf_str_addr": "506 S WRIGHT ST",
  "perf_city_name": "URBANA",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "618013620",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "IL13",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 905887.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Although parallel computers are everywhere from data-centers to cell-phones, writing efficient parallel programs remains a complex and error-prone task. Existing notations, compilers, and runtime systems for parallel programming were developed mostly for computational science applications concerned with numerical solutions of partial differential equations, and they are not appropriate for applications in emerging disciplines like machine learning and graph analytics, which now constitute the bulk of the applications that need to run on parallel computers.</p>\n<p>The goals of the Kali project,&nbsp; a collaboration between researchers at the University of Texas at Austin (UTA) and the University of Illinois, Urbana-Champaign (UIUC), were (i) to study applications in emerging disciplines like machine learning and graph analytics, (ii) to identify patterns of parallelism and locality in these applications, and (iii) to design and implement programming notations, compilers, and runtime systems that make it easier to write such applications and execute them efficiently on parallel computers. Since most programmers in these emerging disciplines are not expert parallel programmers, simplifying parallel programming was another important goal.</p>\n<p>To accomplish these goals, the team studied diverse applications including web-page ranking algorithms, product recommendation systems, and network analysis systems. The studies revealed that these kinds of applications exhibit a parallelism pattern named amorphous data-parallelism, which is not supported well by existing parallel programming systems. Therefore, the team implemented a system called Galois, which permits application programmers to code applications in sequential C++, leaving it to the system software to find amorphous data-parallelism and exploit it efficiently on parallel computing systems. The team also extended the HTA library with array operations suitable for expressing amorphous parallelism.</p>\n<p>Major outcomes of the Kali project include (i) the Galois system, which is available publicly for download, (ii) the Lonestar and Lonestar GPU benchmark suites of applications that exhibit amorphous data-parallelism, and (iii) the HTA library for exploiting locality. These software products are in use in companies and universities all over the world. In addition, roughly 50 technical papers were published in refereed conferences and journals, and about 10 graduate students were trained with support from this project.</p>\n<p>The project team also developed course material for teaching parallel programming based on the concepts developed in the Kali project. Tutorials using this material were taught in major parallel computing conferences all over the world. This material is also being used in courses conducted in universities all over the world.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/29/2017<br>\n\t\t\t\t\tModified by: David&nbsp;A&nbsp;Padua</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nAlthough parallel computers are everywhere from data-centers to cell-phones, writing efficient parallel programs remains a complex and error-prone task. Existing notations, compilers, and runtime systems for parallel programming were developed mostly for computational science applications concerned with numerical solutions of partial differential equations, and they are not appropriate for applications in emerging disciplines like machine learning and graph analytics, which now constitute the bulk of the applications that need to run on parallel computers.\n\nThe goals of the Kali project,  a collaboration between researchers at the University of Texas at Austin (UTA) and the University of Illinois, Urbana-Champaign (UIUC), were (i) to study applications in emerging disciplines like machine learning and graph analytics, (ii) to identify patterns of parallelism and locality in these applications, and (iii) to design and implement programming notations, compilers, and runtime systems that make it easier to write such applications and execute them efficiently on parallel computers. Since most programmers in these emerging disciplines are not expert parallel programmers, simplifying parallel programming was another important goal.\n\nTo accomplish these goals, the team studied diverse applications including web-page ranking algorithms, product recommendation systems, and network analysis systems. The studies revealed that these kinds of applications exhibit a parallelism pattern named amorphous data-parallelism, which is not supported well by existing parallel programming systems. Therefore, the team implemented a system called Galois, which permits application programmers to code applications in sequential C++, leaving it to the system software to find amorphous data-parallelism and exploit it efficiently on parallel computing systems. The team also extended the HTA library with array operations suitable for expressing amorphous parallelism.\n\nMajor outcomes of the Kali project include (i) the Galois system, which is available publicly for download, (ii) the Lonestar and Lonestar GPU benchmark suites of applications that exhibit amorphous data-parallelism, and (iii) the HTA library for exploiting locality. These software products are in use in companies and universities all over the world. In addition, roughly 50 technical papers were published in refereed conferences and journals, and about 10 graduate students were trained with support from this project.\n\nThe project team also developed course material for teaching parallel programming based on the concepts developed in the Kali project. Tutorials using this material were taught in major parallel computing conferences all over the world. This material is also being used in courses conducted in universities all over the world.\n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 10/29/2017\n\n\t\t\t\t\tSubmitted by: David A Padua"
 }
}