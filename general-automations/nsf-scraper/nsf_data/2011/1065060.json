{
 "awd_id": "1065060",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "TC: Medium: Putting Differential Privacy To Work",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032927841",
 "po_email": "sgreensp@nsf.gov",
 "po_sign_block_name": "Sol Greenspan",
 "awd_eff_date": "2011-03-15",
 "awd_exp_date": "2017-02-28",
 "tot_intn_awd_amt": 1199950.0,
 "awd_amount": 1199950.0,
 "awd_min_amd_letter_date": "2011-03-24",
 "awd_max_amd_letter_date": "2013-08-12",
 "awd_abstract_narration": "A wealth of data about individuals is constantly accumulating in various databases in the form of medical records, social network graphs, mobility traces in cellular networks, search logs, and movie ratings, to name only a few. There are many valuable uses for such datasets, but it is difficult to realize these uses while protecting privacy. Even when data collectors try to protect the privacy of their customers by releasing anonymized or aggregated data, this data often reveals much more information than intended. To reliably prevent such privacy violations, we need to replace the current ad-hoc solutions with a principled data release mechanism that offers strong, provable privacy guarantees. Recent research on DIFFERENTIAL PRIVACY has brought us a big step closer to achieving this goal. Differential privacy allows us to reason formally about what an adversary could learn from released data, while avoiding the need for many assumptions (e.g. about what an adversary might already know), the failure of which have been the cause of privacy violations in the past. However, despite its great promise, differential privacy is still rarely used in practice. Proving that a given computation can be performed in a differentially private way requires substantial manual effort by experts in the field, which prevents it from scaling in practice.  \r\n\r\nThis project aims to put differential privacy to work---to build a system that supports differentially private data analysis, can be used by the average programmer, and is general enough to be used in a wide variety of applications. Such a system could be used pervasively and make strong privacy guarantees a standard feature wherever sensitive data is being released or analyzed. Specific contributions will include ENRICHING THE FUNDAMENTAL MODEL OF DIFFERENTIAL PRIVACY to address practical issues such as data with inherent correlations, increased accuracy, privacy of functions, or privacy for streaming data; DEVELOPING A DIFFERENTIALLY PRIVATE PROGRAMMING LANGUAGE, along with a compiler that can automatically prove programs in this language to be differentially private, and a runtime system that is hardened against side-channel attacks; and SHOWING HOW TO APPLY DIFFERENTIAL PRIVACY IN A DISTRIBUTED SETTING in which the private data is spread across many databases in different administrative domains, with possible overlaps, heterogeneous schemata, and different expectations of privacy.  The long-term goal is to combine ideas from differential privacy, programming languages, and distributed systems to make data analysis techniques with strong, provable privacy guarantees practical for general use. The themes of differential privacy are also being integrated into Penn's new undergraduate curriculum on Market and Social Systems Engineering.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Benjamin",
   "pi_last_name": "Pierce",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Benjamin C Pierce",
   "pi_email_addr": "bcpierce@cis.upenn.edu",
   "nsf_id": "000452070",
   "pi_start_date": "2011-03-24",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Andreas",
   "pi_last_name": "Haeberlen",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Andreas Haeberlen",
   "pi_email_addr": "ahae@cis.upenn.edu",
   "nsf_id": "000562850",
   "pi_start_date": "2011-03-24",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3451 WALNUT ST STE 440A",
  "perf_city_name": "PHILADELPHIA",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046205",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779500",
   "pgm_ele_name": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "7795",
   "pgm_ref_txt": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 263250.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 624450.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 312250.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>A wealth of data can be found in medical records, social network graphs, mobility traces in cellular networks, search logs, movie ratings, etc. There are many valuable uses for such datasets, but it is difficult to realize them while protecting privacy. To reliably prevent violations, we need to replace current ad-hoc solutions with a principled data release mechanism offering strong, provable privacy guarantees. Recent research on DIFFERENTIAL PRIVACY has brought us a big step closer to achieving this goal, allowing us to reason formally about what an adversary could learn from released data while avoiding the need for fragile assumptions about what an adversary does not already know. However, despite its promise, differential privacy has rarely been used in practice, partly because proving that a given computation can be performed privately requires substantial expert effort. &nbsp;The broad aim of this project has been to \"put differential privacy to work\" by designing, analyzing, and implementing systems that support differentially private analysis.</p>\n<p>FOUNDATIONS. &nbsp;We extended the theoretical foundations for conducting analyses of big datasets subject to privacy protections. This includes computing low-rank approximations to matrices (useful for recommendation engines), finding the most frequent elements amongst a large decentralized dataset (similar techniques have been put into practice at Google and Apple), producing high-quality \"synthetic\" datasets of high dimensional data (like the Netflix data set), and controlling over-fitting in machine learning.</p>\n<p>LANGUAGES AND VERIFICATION. &nbsp;We designed and implemented a series of programming languages for differentially private querying. &nbsp;The first language, FUZZ, is a simple functional programming language with a type system for inferring an upper bound on the sensitivity of functions; the results from this analysis can then be used to add the correct amount of noise to the results of the query. &nbsp;The second, DFUZZ, is an extension of Fuzz with a combination of \"linear indexed types\" and \"lightweight dependent types\", allowing a richer sensitivity analysis that is able to certify a larger class of queries as differentially private, including ones whose sensitivity depends on runtime information. Finally, ADAPTIVE FUZZ adds support for adaptive querying where, at each step, both the choice of the next function and its privacy parameters are informed by the results of prior queries.</p>\n<p>We also developed tools for VERIFYING differentially private algorithms. &nbsp;In particular, we implemented an algorithm for automatic sensitivity analysis for Fuzz programs, eliminating annotation burden and making verification less costly. &nbsp;Second, working jointly work with IMDEA software and Dundee University, we have developed a series of RELATIONAL PROGRAM LOGICS, which support checking properties referring to two runs of the same program, naturally capturing differential privacy.</p>\n<p>SYSTEMS. &nbsp;In practice, it is often useful to ask questions about sensitive data sets that are held by someone else. However, this is difficult to do efficiently - particularly when multiple parties are involved - and it creates additional privacy risks. We have developed algorithms that can mitigate these risks, and that can answer common types of distributed queries much faster: in a few minutes or hours, instead of several years. We have also found a way to verify that a question about sensitive data has been answered correctly, without revealing what the underlying data actually was.</p>\n<p>APPLICATIONS. &nbsp;In practice, it is often useful to ask questions about sensitive data sets that are held by someone else. However, this is difficult to do efficiently - particularly when multiple parties are involved - and it creates additional privacy risks. We have developed algorithms that can mitigate these risks, and that can answer common types of distributed queries much faster: in a few minutes or hours, instead of several years. &nbsp;We have also found a way to verify that a question about sensitive data has been answered correctly, without revealing what the underlying data actually was.<br />We applied our solutions to a number of common data mining algorithms, as well as to a larger case study: measuring systemic risk in financial networks. Systemic risk is though to have been a major factor in the financial crisis of 2008, but it is difficult to measure because it depends on sensitive financial data, which is heavily protected. We have developed a way to measure the risk privately, while giving strong guarantees to the banks that their data will not be leaked. Our results could serve as the foundation for an 'early-warning system' for future financial crises.</p>\n<p>EDUCATION AND OUTREACH. &nbsp;With Cynthia Dwork, Co-PI Roth published a monograph on differential privacy that has become a standard reference in the area. &nbsp;The theme of differential privacy was also integrated into a new sophomore-level course on scalable and cloud computing (NETS 212), one of the flagship courses in Penn's new undergraduate NETS curriculum.</p>\n<p>http://privacy.cis.upenn.edu.&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/16/2017<br>\n\t\t\t\t\tModified by: Benjamin&nbsp;C&nbsp;Pierce</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nA wealth of data can be found in medical records, social network graphs, mobility traces in cellular networks, search logs, movie ratings, etc. There are many valuable uses for such datasets, but it is difficult to realize them while protecting privacy. To reliably prevent violations, we need to replace current ad-hoc solutions with a principled data release mechanism offering strong, provable privacy guarantees. Recent research on DIFFERENTIAL PRIVACY has brought us a big step closer to achieving this goal, allowing us to reason formally about what an adversary could learn from released data while avoiding the need for fragile assumptions about what an adversary does not already know. However, despite its promise, differential privacy has rarely been used in practice, partly because proving that a given computation can be performed privately requires substantial expert effort.  The broad aim of this project has been to \"put differential privacy to work\" by designing, analyzing, and implementing systems that support differentially private analysis.\n\nFOUNDATIONS.  We extended the theoretical foundations for conducting analyses of big datasets subject to privacy protections. This includes computing low-rank approximations to matrices (useful for recommendation engines), finding the most frequent elements amongst a large decentralized dataset (similar techniques have been put into practice at Google and Apple), producing high-quality \"synthetic\" datasets of high dimensional data (like the Netflix data set), and controlling over-fitting in machine learning.\n\nLANGUAGES AND VERIFICATION.  We designed and implemented a series of programming languages for differentially private querying.  The first language, FUZZ, is a simple functional programming language with a type system for inferring an upper bound on the sensitivity of functions; the results from this analysis can then be used to add the correct amount of noise to the results of the query.  The second, DFUZZ, is an extension of Fuzz with a combination of \"linear indexed types\" and \"lightweight dependent types\", allowing a richer sensitivity analysis that is able to certify a larger class of queries as differentially private, including ones whose sensitivity depends on runtime information. Finally, ADAPTIVE FUZZ adds support for adaptive querying where, at each step, both the choice of the next function and its privacy parameters are informed by the results of prior queries.\n\nWe also developed tools for VERIFYING differentially private algorithms.  In particular, we implemented an algorithm for automatic sensitivity analysis for Fuzz programs, eliminating annotation burden and making verification less costly.  Second, working jointly work with IMDEA software and Dundee University, we have developed a series of RELATIONAL PROGRAM LOGICS, which support checking properties referring to two runs of the same program, naturally capturing differential privacy.\n\nSYSTEMS.  In practice, it is often useful to ask questions about sensitive data sets that are held by someone else. However, this is difficult to do efficiently - particularly when multiple parties are involved - and it creates additional privacy risks. We have developed algorithms that can mitigate these risks, and that can answer common types of distributed queries much faster: in a few minutes or hours, instead of several years. We have also found a way to verify that a question about sensitive data has been answered correctly, without revealing what the underlying data actually was.\n\nAPPLICATIONS.  In practice, it is often useful to ask questions about sensitive data sets that are held by someone else. However, this is difficult to do efficiently - particularly when multiple parties are involved - and it creates additional privacy risks. We have developed algorithms that can mitigate these risks, and that can answer common types of distributed queries much faster: in a few minutes or hours, instead of several years.  We have also found a way to verify that a question about sensitive data has been answered correctly, without revealing what the underlying data actually was.\nWe applied our solutions to a number of common data mining algorithms, as well as to a larger case study: measuring systemic risk in financial networks. Systemic risk is though to have been a major factor in the financial crisis of 2008, but it is difficult to measure because it depends on sensitive financial data, which is heavily protected. We have developed a way to measure the risk privately, while giving strong guarantees to the banks that their data will not be leaked. Our results could serve as the foundation for an 'early-warning system' for future financial crises.\n\nEDUCATION AND OUTREACH.  With Cynthia Dwork, Co-PI Roth published a monograph on differential privacy that has become a standard reference in the area.  The theme of differential privacy was also integrated into a new sophomore-level course on scalable and cloud computing (NETS 212), one of the flagship courses in Penn's new undergraduate NETS curriculum.\n\nhttp://privacy.cis.upenn.edu. \n\n \n\n \n\n\t\t\t\t\tLast Modified: 05/16/2017\n\n\t\t\t\t\tSubmitted by: Benjamin C Pierce"
 }
}