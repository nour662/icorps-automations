{
 "awd_id": "1131897",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "NCRN-MN: Triangle Census Research Network",
 "cfda_num": "47.075",
 "org_code": "04050000",
 "po_phone": "7032927269",
 "po_email": "ceavey@nsf.gov",
 "po_sign_block_name": "Cheryl Eavey",
 "awd_eff_date": "2011-10-01",
 "awd_exp_date": "2018-09-30",
 "tot_intn_awd_amt": 2997591.0,
 "awd_amount": 4087370.0,
 "awd_min_amd_letter_date": "2011-07-13",
 "awd_max_amd_letter_date": "2016-08-31",
 "awd_abstract_narration": "A primary mission of many federal statistical agencies is to disseminate data to the public for secondary analysis.  However, dissemination is increasingly challenging due to risks of unintended confidentiality breaches, nonresponse and faulty data, and the costs of mounting surveys that collect many detailed attributes.  The Triangle Census Research Network (TCRN) will develop broadly applicable methodologies that will transform and improve data dissemination practice in the federal statistical system.  In particular, the TCRN will advance methodologies and tools for disseminating public use data with high quality and acceptable risks of confidentiality breaches by developing theory and methodology for releasing multiply imputed, synthetic datasets based on flexible, nonparametric Bayesian models built specifically for high-dimensional data with longitudinal and multi-level aspects.  TCRN will develop approaches for including survey weights in redacted data that can improve statistical estimation without leading to confidentiality disclosures.  The project also will develop the framework for computer systems that provide secondary analysts with feedback on the quality of inferences from redacted data, and it will develop theory and methodology for creating synthetic contingency tables based on fusions of linear programming and Bayesian modeling.  The TCRN will improve methodology and practice for handling missing and faulty data by developing frameworks for simultaneous imputation of missing data and editing of faulty data by integrating paradigms from statistics and operations research.  The project also will develop nonparametric Bayesian methodology for multiple imputation of missing data in high dimensions with longitudinal and multi-level aspects.  Finally, to enhance agencies' abilities to integrate information from multiple sources, the TCRN will develop methods that agencies and secondary analysts can use to properly account for uncertainty in inferences in imperfect record linkage settings, as well as to pass on that uncertainty in public use data products via multiply imputed datasets.  TCRN also will develop statistical approaches to combining information from multiple data sources that do not depend on record linkage.\r\n\r\nThe methodological developments of the TCRN will transform the way statistical agencies handle data dissemination with regard to statistical disclosure limitation, missing data, and integrating information.  These developments will offer federal agencies options for releasing data products with increased utility, leading to advances in science and improved policy making. The TCRN will apply the methodologies to major Census Bureau data products, thereby improving the hundreds of secondary analyses of these datasets.  The interdisciplinary team of the TCRN will use these data products to answer questions in aging, economics, and social welfare that have important implications for policy making.  As an integral part of the research, the TCRN will involve and offer educational opportunities to postdoctoral fellows, graduate students, and statisticians at federal agencies, thus developing and training future leaders in data dissemination research and practice.  This activity is supported by the NSF-Census Research Network funding opportunity.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "SES",
 "org_div_long_name": "Division of Social and Economic Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Jerome",
   "pi_last_name": "Reiter",
   "pi_mid_init": "P",
   "pi_sufx_name": "",
   "pi_full_name": "Jerome P Reiter",
   "pi_email_addr": "jreiter@duke.edu",
   "nsf_id": "000168419",
   "pi_start_date": "2011-07-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Alan",
   "pi_last_name": "Karr",
   "pi_mid_init": "F",
   "pi_sufx_name": "",
   "pi_full_name": "Alan F Karr",
   "pi_email_addr": "karr@rti.org",
   "nsf_id": "000320384",
   "pi_start_date": "2011-07-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Duke University",
  "inst_street_address": "2200 W MAIN ST",
  "inst_street_address_2": "",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NC",
  "inst_state_name": "North Carolina",
  "inst_phone_num": "9196843030",
  "inst_zip_code": "277054640",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "NC04",
  "org_lgl_bus_name": "DUKE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "TP7EK8DZV6N5"
 },
 "perf_inst": {
  "perf_inst_name": "Duke University",
  "perf_str_addr": "2200 W. Main St,",
  "perf_city_name": "Durham",
  "perf_st_code": "NC",
  "perf_st_name": "North Carolina",
  "perf_zip_code": "277054010",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "NC04",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "917800",
   "pgm_ele_name": "NSF-Census Research Network"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7434",
   "pgm_ref_txt": "CNCI"
  },
  {
   "pgm_ref_code": "8073",
   "pgm_ref_txt": "NSF-Census Research Network"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0116",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001617RB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 2997591.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 322027.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 199737.0
  },
  {
   "fund_oblg_fiscal_yr": 2016,
   "fund_oblg_amt": 568015.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Primary goals of this award were to develop broadly-applicable methodologies that improve data dissemination practice in the federal statistical system, as well as to train students and postdoctoral fellows in statistical methods directly relevant for data dissemination tasks.&nbsp; Activities focused on three interrelated areas: (i) disseminating public use data with high quality and acceptable disclosure risks, (ii) handling missing data and correcting faulty data in large complex surveys, and (iii) integrating information from multiple data sources.</p>\n<p>With respect to data confidentiality, the team developed a new method for generating synthetic data--i.e., data simulated from statistical models--for people nested within households.&nbsp; This methodology is being tested by the Census Bureau as they redesign the protection strategies for the American Community Survey.&nbsp; The team developed methods for generating synthetic data for economic surveys; these are being tested by the Census Bureau for use in the Economic Census.&nbsp; The team also developed methods that analysts of synthetic data can use to evaluate the quality of analyses based on the synthetic data.&nbsp; The methods satisfy a privacy criterion known as differential privacy.&nbsp; Finally, the team developed open source software package packages that implement both the synthesis models and the verification measures. These are posted in public repositories and are available for anyone to download.</p>\n<p>With respect to missing and faulty data, the team developed methods for simultaneous editing and imputation of faulty respondent data.&nbsp; For example, if the reported data indicate that someone responds with the impossible combination \"pregnant male,\" the methods will leverage relationships among the variables in the data to come up with plausible corrections.&nbsp; The team also developed methods for handling nonignorable missing data, e.g., people with large incomes do not provide their values.&nbsp; These methods enable inferences that account for attrition in longitudinal surveys and general item nonresponse.&nbsp; The team developed several highly flexible methods for multiple imputation of missing data in general contexts.&nbsp; The methods use variants of Bayesian mixture models.&nbsp; Some of these methods are being used by the Census Bureau for imputation of missing values in economic data.&nbsp; Finally, the team developed open source software package packages that implement the edit-imputation and flexible multiple imputation routines. These are posted in public repositories and are available for anyone to download.</p>\n<p>With respect to data integration, the team developed improved methods for record linkage that use Bayesian technique.&nbsp; The methods allow analysts to incorporate uncertainty into their inferences that results from imprecise linkages.&nbsp; Included in these developments are methods for simultaneous record linkage and estimation of regression coefficients.&nbsp; The team also developed approaches for simultaneous causal inference with linked datasets.&nbsp; These methods can result in more accurate estimates than treating linkage and causal inference as separate tasks.&nbsp; The team also worked on methods for data fusion, in which one seeks to combine information from databases without overlapping records.&nbsp; In particular, the team showed that one can improve on standard approaches for data fusion by using measurement error modeling.&nbsp; The methods were applied to adjust reporting errors in educational attainment in data from the American Community Survey.&nbsp;</p>\n<p>With respect to training, the award supported seven postdoctoral associates, thirteen PhD students, six master's students, and three undergraduate students.&nbsp; The award directly supported research that led to eleven PhD dissertations and six Master's theses.&nbsp; The trainees published more than 50 peer-reviewed papers in total.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/17/2018<br>\n\t\t\t\t\tModified by: Jerome&nbsp;P&nbsp;Reiter</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nPrimary goals of this award were to develop broadly-applicable methodologies that improve data dissemination practice in the federal statistical system, as well as to train students and postdoctoral fellows in statistical methods directly relevant for data dissemination tasks.  Activities focused on three interrelated areas: (i) disseminating public use data with high quality and acceptable disclosure risks, (ii) handling missing data and correcting faulty data in large complex surveys, and (iii) integrating information from multiple data sources.\n\nWith respect to data confidentiality, the team developed a new method for generating synthetic data--i.e., data simulated from statistical models--for people nested within households.  This methodology is being tested by the Census Bureau as they redesign the protection strategies for the American Community Survey.  The team developed methods for generating synthetic data for economic surveys; these are being tested by the Census Bureau for use in the Economic Census.  The team also developed methods that analysts of synthetic data can use to evaluate the quality of analyses based on the synthetic data.  The methods satisfy a privacy criterion known as differential privacy.  Finally, the team developed open source software package packages that implement both the synthesis models and the verification measures. These are posted in public repositories and are available for anyone to download.\n\nWith respect to missing and faulty data, the team developed methods for simultaneous editing and imputation of faulty respondent data.  For example, if the reported data indicate that someone responds with the impossible combination \"pregnant male,\" the methods will leverage relationships among the variables in the data to come up with plausible corrections.  The team also developed methods for handling nonignorable missing data, e.g., people with large incomes do not provide their values.  These methods enable inferences that account for attrition in longitudinal surveys and general item nonresponse.  The team developed several highly flexible methods for multiple imputation of missing data in general contexts.  The methods use variants of Bayesian mixture models.  Some of these methods are being used by the Census Bureau for imputation of missing values in economic data.  Finally, the team developed open source software package packages that implement the edit-imputation and flexible multiple imputation routines. These are posted in public repositories and are available for anyone to download.\n\nWith respect to data integration, the team developed improved methods for record linkage that use Bayesian technique.  The methods allow analysts to incorporate uncertainty into their inferences that results from imprecise linkages.  Included in these developments are methods for simultaneous record linkage and estimation of regression coefficients.  The team also developed approaches for simultaneous causal inference with linked datasets.  These methods can result in more accurate estimates than treating linkage and causal inference as separate tasks.  The team also worked on methods for data fusion, in which one seeks to combine information from databases without overlapping records.  In particular, the team showed that one can improve on standard approaches for data fusion by using measurement error modeling.  The methods were applied to adjust reporting errors in educational attainment in data from the American Community Survey. \n\nWith respect to training, the award supported seven postdoctoral associates, thirteen PhD students, six master's students, and three undergraduate students.  The award directly supported research that led to eleven PhD dissertations and six Master's theses.  The trainees published more than 50 peer-reviewed papers in total.\n\n\t\t\t\t\tLast Modified: 11/17/2018\n\n\t\t\t\t\tSubmitted by: Jerome P Reiter"
 }
}