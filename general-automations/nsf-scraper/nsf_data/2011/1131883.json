{
 "awd_id": "1131883",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "US-German Collaboration:  Towards a Neural Theory of 3D Shape Perception",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2011-11-01",
 "awd_exp_date": "2015-10-31",
 "tot_intn_awd_amt": 460004.0,
 "awd_amount": 460004.0,
 "awd_min_amd_letter_date": "2011-09-08",
 "awd_max_amd_letter_date": "2013-07-24",
 "awd_abstract_narration": "How the brain estimates the 3D shape of objects in our surroundings remains one of the most significant challenges in visual neuroscience. The information provided by the retina is fundamentally ambiguous, because many different combinations of 3D shape, illumination and surface reflectance are consistent with any given image.  Despite this ambiguity, the visual system is extremely adept at estimating 3D shape across a wide range of viewing conditions, something that no extant machine vision system can do. The long-term goal of the project is to develop a computational model in neural terms to explain how 3D shape is estimated in the primate visual system. It will build upon the responses of cells early in visual cortex (V1) and develop models of how they can be organized into mid-level configurations that specify 3D shape properties.  Importantly, the project will also measure human perception of 3D shape in a series of psychophysical experiments designed to test specific predictions, bringing together the complementary expertise of Roland W. Fleming (Giessen University: human perception, psychophysics) and Steven W. Zucker (Yale University: computational vision, computational neuroscience). The results should provide a deeper understanding of visual circuit properties in the ventral processing stream; they should provide models for 3D computer vision and graphics; and they may pave the way for the development of rehabilitation strategies for patients with visual deficits.\r\n\r\nThe basic approach starts with populations of neurons tuned to different orientations and seeks to understand how these provide basic information about local shape properties according to the principles of differential geometry.  Specifically, when 3D surfaces are projected onto the retina, the distorted gradients of shading and texture lead to highly structured patterns of local image orientation, or orientation fields, which can be inferred via circuits involving long-range horizontal connections.  The investigators seek to derive formal models showing how these networks can be organized to infer 3D surface properties. The specific approach is involves four stages: (i) modeling how the visual system obtains clean and reliable orientation fields from the outputs of model V1 cells through lateral interactions and feedback; (ii) establishing how local measurements are grouped into specific \"mid-level\" configurations to support the recovery of 3D shape properties (modeling V2 to V4); (iii) modeling how these low- and mid-level 2D measurements can be mapped into representations of 3D shape properties (V4 to IT); and (iv) modeling how grouping and global constraints can convert these shape estimates into global shape reconstructions (again V4 to IT). Targeted psychophysical experiments will complement all of the modeling and test specific predictions from it. The resulting stimuli will support next generation neurophysiological experiments. Although the above stages define a working strategy, dependencies among these stages should also provide a model of the feedforward/feedback projections that link different areas of cortex. The ultimate goal is a model that can correctly predict the errors, the successes, and the limits of human shape perception. \r\n\r\nThis project is jointly funded by Collaborative Research in Computational Neuroscience and the Office of International Science and Engineering.  A companion project is being funded by the German Ministry of Education and Research (BMBF).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Steven",
   "pi_last_name": "Zucker",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Steven Zucker",
   "pi_email_addr": "steven.zucker@yale.edu",
   "nsf_id": "000218453",
   "pi_start_date": "2011-09-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Yale University",
  "inst_street_address": "150 MUNSON ST",
  "inst_street_address_2": "",
  "inst_city_name": "NEW HAVEN",
  "inst_state_code": "CT",
  "inst_state_name": "Connecticut",
  "inst_phone_num": "2037854689",
  "inst_zip_code": "065113572",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "CT03",
  "org_lgl_bus_name": "YALE UNIV",
  "org_prnt_uei_num": "FL6GV84CKN57",
  "org_uei_num": "FL6GV84CKN57"
 },
 "perf_inst": {
  "perf_inst_name": "Yale University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "CT",
  "perf_st_name": "Connecticut",
  "perf_zip_code": "065208247",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "CT03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "729800",
   "pgm_ele_name": "International Research Collab"
  },
  {
   "pgm_ele_code": "732700",
   "pgm_ele_name": "CRCNS-Computation Neuroscience"
  },
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5936",
   "pgm_ref_txt": "GERMANY (F.R.G.)"
  },
  {
   "pgm_ref_code": "5979",
   "pgm_ref_txt": "Europe and Eurasia"
  },
  {
   "pgm_ref_code": "7327",
   "pgm_ref_txt": "CRCNS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 300313.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 159691.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Human perception of the three-dimensional world remains a fascinating challenge for neurobiology. Somehow the brain is able to infer the shape of object from e.g. their shading distribution, a skill that we take to be effortless and that artists learn to simulate. However mathematically this problem is extremely difficult to solve: for any given shading distribution there are an infinite number of possibilities. The lighting, surface material properties, and image formation process yield an image (that's the easy part!); somehow the brain 'inverts' this,working out the lighting, material, and surface geometry from the image. The inversion is hard because the image is two-dimensional while the geometry and lighting are three-dimensional.</p>\n<p>Machine vision makes this problem simpler by knowing about the light source, which can work well for gaming and industrial problems. We considered the general problem: how to do the inference without prior knowledge of the light source in a mathematical framework driven by the approximate constraints of neurobiology. The result allows an exact calculation of the ambiguity and leads to a surprising conjecture: while the problem is in general ill-posed, in certain situations the possibilities are more limited. We call these configurations 'critical features' and suggest that these serve as anchors for the larger problem. By analogy, it is like solving a puzzle: get a few pieces in place and the others then fit in. This provides one (set of) solutions out of many for the geometry, and psychophysical studies show that these are consistent with human perception.</p>\n<p>Beyond the geometry one also needs to infer something about material properties. Are brightness variations due to geometry, as required above, or are they due to pigmentation, like the skin of an apple? Clearly assuming that pigmentation changes are due to geometry will yield an incorrect solution. A surprising discovery in this project was that aspects of color variation can be used to solve this problem, suggesting a new role for color and a new rationale for the color selective cells in visual cortex.</p>\n<p>Finally, for brains to adapt to our visual world, they need to be able to learn these relationships. In related work we were able to derive the first high-order learning algorithm that is both biologically plausible and provably convergent.</p>\n<p>Much more remains to be done. Given the importance of shading and color in our environment; their necessity for technology and biomedical imaging; and the difficulty of unraveling the brain, we believe the above contributions provide a small step in these directions.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 01/13/2016<br>\n\t\t\t\t\tModified by: Steven&nbsp;Zucker</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHuman perception of the three-dimensional world remains a fascinating challenge for neurobiology. Somehow the brain is able to infer the shape of object from e.g. their shading distribution, a skill that we take to be effortless and that artists learn to simulate. However mathematically this problem is extremely difficult to solve: for any given shading distribution there are an infinite number of possibilities. The lighting, surface material properties, and image formation process yield an image (that's the easy part!); somehow the brain 'inverts' this,working out the lighting, material, and surface geometry from the image. The inversion is hard because the image is two-dimensional while the geometry and lighting are three-dimensional.\n\nMachine vision makes this problem simpler by knowing about the light source, which can work well for gaming and industrial problems. We considered the general problem: how to do the inference without prior knowledge of the light source in a mathematical framework driven by the approximate constraints of neurobiology. The result allows an exact calculation of the ambiguity and leads to a surprising conjecture: while the problem is in general ill-posed, in certain situations the possibilities are more limited. We call these configurations 'critical features' and suggest that these serve as anchors for the larger problem. By analogy, it is like solving a puzzle: get a few pieces in place and the others then fit in. This provides one (set of) solutions out of many for the geometry, and psychophysical studies show that these are consistent with human perception.\n\nBeyond the geometry one also needs to infer something about material properties. Are brightness variations due to geometry, as required above, or are they due to pigmentation, like the skin of an apple? Clearly assuming that pigmentation changes are due to geometry will yield an incorrect solution. A surprising discovery in this project was that aspects of color variation can be used to solve this problem, suggesting a new role for color and a new rationale for the color selective cells in visual cortex.\n\nFinally, for brains to adapt to our visual world, they need to be able to learn these relationships. In related work we were able to derive the first high-order learning algorithm that is both biologically plausible and provably convergent.\n\nMuch more remains to be done. Given the importance of shading and color in our environment; their necessity for technology and biomedical imaging; and the difficulty of unraveling the brain, we believe the above contributions provide a small step in these directions.\n\n \n\n\t\t\t\t\tLast Modified: 01/13/2016\n\n\t\t\t\t\tSubmitted by: Steven Zucker"
 }
}