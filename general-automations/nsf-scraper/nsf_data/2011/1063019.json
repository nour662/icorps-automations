{
 "awd_id": "1063019",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "G8 Initiative: Collaborative Research:  ECS:  Enabling Climate Simulation at Extreme Scale",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Daniel Katz",
 "awd_eff_date": "2011-03-01",
 "awd_exp_date": "2014-02-28",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 150000.0,
 "awd_min_amd_letter_date": "2011-02-25",
 "awd_max_amd_letter_date": "2011-09-07",
 "awd_abstract_narration": "This NSF award to the University of Illinois at Urbana-Champaign and the University of Tennessee at Knoxville  funds U.S. researchers participating in a project competitively selected by the G8 Research Councils Initiative on Multilateral Research through the Interdisciplinary Program on Application Software towards Exascale Computing for Global Scale Issues. This is a pilot collaboration among the U.S. National Science Foundation, the Canadian National Sciences and Engineering Research Council (NSERC), the French Agence Nationale de la Recherche (ANR), the German Deutsche Forschungsgemeinschaft (DFG), the Japan Society for the Promotion of Science (JSPS), the Russian Foundation for Basic Research (RFBR),and the United Kingdom Research Councils (RC-UK), supporting collaborative research projects selected on a competitive basis that are comprised of researchers from at least three of the partner countries.\r\n\r\nThis interdisciplinary project across six countries focuses on three research topics that address limitations in numerical modeling of physics, chemistry and biology with the NCAR Community Earth System Model Version 1 (CESM1) and similar codes used by other countries. These research topics include new approaches to handle resilience, node level optimization and system level scalability. This research will enable the development of more scalable model ensembles. These will allow better evaluation of climate sensitivity and climate feedback processes, a better quantification of model uncertainty and a better understanding of the effects of natural variability.\r\n\r\nThe project will provide essential knowledge toward scaling climate codes for exascale and thus reducing current uncertainties on climate evolution; it will foster interactions between computer scientists and climate scientists; will foster international collaborations in the area of climate simulations and exascale computing; and will educate a new generation of researchers that understand both the application domain of climate simulation and high-performance computing",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "George",
   "pi_last_name": "Bosilca",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "George M Bosilca",
   "pi_email_addr": "bosilca@icl.utk.edu",
   "nsf_id": "000348594",
   "pi_start_date": "2011-02-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Tennessee Knoxville",
  "inst_street_address": "201 ANDY HOLT TOWER",
  "inst_street_address_2": "",
  "inst_city_name": "KNOXVILLE",
  "inst_state_code": "TN",
  "inst_state_name": "Tennessee",
  "inst_phone_num": "8659743466",
  "inst_zip_code": "379960001",
  "inst_country_name": "United States",
  "cong_dist_code": "02",
  "st_cong_dist_code": "TN02",
  "org_lgl_bus_name": "UNIVERSITY OF TENNESSEE",
  "org_prnt_uei_num": "LXG4F9K8YZK5",
  "org_uei_num": "FN2YCS2YAUW3"
 },
 "perf_inst": {
  "perf_inst_name": "University of Tennessee Knoxville",
  "perf_str_addr": "201 ANDY HOLT TOWER",
  "perf_city_name": "KNOXVILLE",
  "perf_st_code": "TN",
  "perf_st_name": "Tennessee",
  "perf_zip_code": "379960001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "02",
  "perf_st_cong_dist": "TN02",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "723100",
   "pgm_ele_name": "CYBERINFRASTRUCTURE"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5921",
   "pgm_ref_txt": "JAPAN"
  },
  {
   "pgm_ref_code": "5936",
   "pgm_ref_txt": "GERMANY (F.R.G.)"
  },
  {
   "pgm_ref_code": "5952",
   "pgm_ref_txt": "SPAIN"
  },
  {
   "pgm_ref_code": "7231",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE"
  },
  {
   "pgm_ref_code": "8060",
   "pgm_ref_txt": "SEES Unsolicited"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 150000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p class=\"Default\">Policy decisions for both mitigating and adapting to climate change are subjects of great discussion in the G8 countries and throughout the world. Uninformed decisions, including ones of no action, could have a very high cost in money and lives. Therefore, it is essential to reduce, as soon as possible, the current uncertainties about future climate change. Numerical models of the physics, chemistry, and biology affecting the Earth-atmosphere climate system are key tools to these projections. Today, even as we prepare to run complex models of the Earth&rsquo;s climate system on petascale machines, we realize that despite the extensive capabilities that petascale will enable, a number of critical limitations in modeling the climate system require an exascale capability. One of the areas where research is necessary in order to allow the climate models to produce meaningful results is the area of fault tolerance.&nbsp;</p>\n<p>The size and structure of future large-scale architectures increase the likelihood of a larger number of soft and hard errors impacting an application&rsquo;s execution, especially with long-running applications like the NCAR Community Earth System Model Version 1 (CESM1). However, due to the complexity of this climate model and the large number of mathematical algorithms involved overall, a holistic multi-stage approach is indeed necessary. The expectation is that a single software approach, being system-level (checkpoint/restart) or application-level (algorithm based), is unlikely to succeed in all aspects, highlighting the path toward middle-ground hybrid approaches, where distinct solutions will be envisioned for each execution stage.</p>\n<p class=\"Default\">One of the most critical requirements of the NCAR codes used in the context of this project is a strict prerequisite for bit-wise reproducibility of the scientific result. While such an approach is somehow unexpected in the scientific community where some margin of error is tolerable, in the context of this project this requirement is not to be underestimated. Unfortunately, such a strict limit on the numerical stability of the result automatically disqualifies the use of any type of algorithm based fault tolerance, where a trade-off between numerical stability and performance is a crucial piece of the puzzle. Due to the bit-wise reproducibility requirement, instead of making the application recovery a major component, we loosen the requirements and focused on a simpler, yet important, target: being able to accurately validate partial results, bounded by a predefined accuracy, during the application execution. Such a protection will already be a significant step compared with today&rsquo;s state, where the result is validated only upon completion of the computation. Thus, instead of building algorithms capable of tolerating hard and soft errors, we build validators capable of soft error detection as a first step, supplemented with checkpoint/restart based approaches allowing the application to restart from correct data (one that will lead to a bit-wise reproducible result). To be precise, the interest of this approach is twofold. On one side the validator can stop the execution of the current instance of the climate application, as soon as the mathematical condition is not respected. This ensures that any execution that reached the completion stage is a valid execution, and that the result is indeed usable. On the other side, by stopping the execution early, we would be able to minimize the cost (in terms of energy and time) to the time-to-solution for the target application.</p>\n<p>Continuing from the observation that we can improve the cost of the execution, we started to build models for representing the different fault management approaches. Instantiating these models with a known architecture, we can then estimate the costs and overheads for approaches for resilience, ...",
  "por_txt_cntn": "Policy decisions for both mitigating and adapting to climate change are subjects of great discussion in the G8 countries and throughout the world. Uninformed decisions, including ones of no action, could have a very high cost in money and lives. Therefore, it is essential to reduce, as soon as possible, the current uncertainties about future climate change. Numerical models of the physics, chemistry, and biology affecting the Earth-atmosphere climate system are key tools to these projections. Today, even as we prepare to run complex models of the Earth\u00c6s climate system on petascale machines, we realize that despite the extensive capabilities that petascale will enable, a number of critical limitations in modeling the climate system require an exascale capability. One of the areas where research is necessary in order to allow the climate models to produce meaningful results is the area of fault tolerance. \n\nThe size and structure of future large-scale architectures increase the likelihood of a larger number of soft and hard errors impacting an application\u00c6s execution, especially with long-running applications like the NCAR Community Earth System Model Version 1 (CESM1). However, due to the complexity of this climate model and the large number of mathematical algorithms involved overall, a holistic multi-stage approach is indeed necessary. The expectation is that a single software approach, being system-level (checkpoint/restart) or application-level (algorithm based), is unlikely to succeed in all aspects, highlighting the path toward middle-ground hybrid approaches, where distinct solutions will be envisioned for each execution stage.\nOne of the most critical requirements of the NCAR codes used in the context of this project is a strict prerequisite for bit-wise reproducibility of the scientific result. While such an approach is somehow unexpected in the scientific community where some margin of error is tolerable, in the context of this project this requirement is not to be underestimated. Unfortunately, such a strict limit on the numerical stability of the result automatically disqualifies the use of any type of algorithm based fault tolerance, where a trade-off between numerical stability and performance is a crucial piece of the puzzle. Due to the bit-wise reproducibility requirement, instead of making the application recovery a major component, we loosen the requirements and focused on a simpler, yet important, target: being able to accurately validate partial results, bounded by a predefined accuracy, during the application execution. Such a protection will already be a significant step compared with today\u00c6s state, where the result is validated only upon completion of the computation. Thus, instead of building algorithms capable of tolerating hard and soft errors, we build validators capable of soft error detection as a first step, supplemented with checkpoint/restart based approaches allowing the application to restart from correct data (one that will lead to a bit-wise reproducible result). To be precise, the interest of this approach is twofold. On one side the validator can stop the execution of the current instance of the climate application, as soon as the mathematical condition is not respected. This ensures that any execution that reached the completion stage is a valid execution, and that the result is indeed usable. On the other side, by stopping the execution early, we would be able to minimize the cost (in terms of energy and time) to the time-to-solution for the target application.\n\nContinuing from the observation that we can improve the cost of the execution, we started to build models for representing the different fault management approaches. Instantiating these models with a known architecture, we can then estimate the costs and overheads for approaches for resilience, based not only on the execution environment but also on intrinsic properties of the application. Additionally, we started looking into ad..."
 }
}