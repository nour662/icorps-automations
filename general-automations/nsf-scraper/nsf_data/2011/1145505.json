{
 "awd_id": "1145505",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Using Social Media and Crowdsourcing to Create a New Affect Dictionary",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2013-08-31",
 "tot_intn_awd_amt": 6162.0,
 "awd_amount": 6162.0,
 "awd_min_amd_letter_date": "2011-08-06",
 "awd_max_amd_letter_date": "2011-08-06",
 "awd_abstract_narration": "Research on sentiment and emotion in textual and spoken language relies heavily on human annotation of corpora and human judgments of words in context to provide gold standard data and useful features for prediction.  Current resources such as Whissell's Dictionary of Affect in Language (DAL) are of limited utility, due to their limited coverage. To provide richer gold standards, in recent years computational linguists have been making use se of crowd-sourcing websites such as Amazon Mechanical Turk to accomplish annotation tasks that previously would have been done by trained annotators or human subjects in laboratory experiments.  Recently there has been some research on using social media websites for similar purposes (mining existing social exchanges for information and creating games to elicit new information).  In this project the PI will explore the use of all three of these approaches to augment existing data on human judgments of the emotional content of lexical items to support her ongoing investigation into the classification of emotional text and speech (a major objective of which is to move beyond simple valence judgments relying upon acoustic and prosodic features by taking into account more nuanced aspects of affective text).  She will examine the value of crowd-sourcing, social media mining and games implemented in social websites to ascertain what lessons can be learned about acquiring reliable annotations and judgments of emotional content.\r\n\r\nBroader Impacts:  This exploratory research has the potential to open up new sources of information about the affective connotations of lexical items that will be invaluable to researchers working in text and in speech affect.  Advances in the automatic identification of affect in text and in speech would have major applications in fields such as business and medicine, inter alia.  Business interests in assessing consumer opinion is rapidly moving from focus groups to analyses of product reviews, while medical informatics researchers similarly try to learn patient attitudes to diagnoses, drugs, and treatments by mining online forums.  A major component of such endeavors is the use of affect dictionaries.  New and better sources of affective connotations of lexical items should prove enormously helpful in these efforts, increasing our ability to learn useful, practical information from the data individuals provide freely online.  The lessons learned through these experiments and results of subsequent data collection will be made available to the larger research community in the form of new affect lexicons.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Julia",
   "pi_last_name": "Hirschberg",
   "pi_mid_init": "B",
   "pi_sufx_name": "",
   "pi_full_name": "Julia B Hirschberg",
   "pi_email_addr": "julia@cs.columbia.edu",
   "nsf_id": "000399629",
   "pi_start_date": "2011-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Columbia University",
  "inst_street_address": "615 W 131ST ST",
  "inst_street_address_2": "MC 8741",
  "inst_city_name": "NEW YORK",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "2128546851",
  "inst_zip_code": "100277922",
  "inst_country_name": "United States",
  "cong_dist_code": "13",
  "st_cong_dist_code": "NY13",
  "org_lgl_bus_name": "THE TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK",
  "org_prnt_uei_num": "",
  "org_uei_num": "F4N1QNPB95M4"
 },
 "perf_inst": {
  "perf_inst_name": "Columbia University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "100276902",
  "perf_ctry_code": "US",
  "perf_cong_dist": "13",
  "perf_st_cong_dist": "NY13",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 6162.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our general goal in this project was to bring together research on sentiment analysis from text with studies of emotional speech to automatically identify a wide range of speaker emotions including happiness, sadness, anger, surprise, fear, and disgust from text. &nbsp;These are thought to be the \"classic\" emotions in the psychology literature. &nbsp; We trained a machine learning classifier on data from LiveJournal (a blog site in which people identify the emotion they most closely associate with their blog post) and on data we collected from annotators asked to describe a large set of scenes. &nbsp;Our particular goal was to be able to determine what sort of emotional facial expressions we should generate in a project we have been working on in text-to-scene generation. &nbsp;This system, called WordsEye, automatically creates scenes from simple text input. &nbsp;We use a third-party program called FaceGen to generate faces with different expressions based upon our work in emotion classification from text. &nbsp; &nbsp;We have published a paper on results of our research in which we report 40-63% success rates in predicting which emotion to generate (Ulinski et al, \"Finding Emotion in Image Descriptions,\" WISDOM 2012. &nbsp;Samples of happy and angry faces generated in WordsEye are included with this report. &nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/01/2013<br>\n\t\t\t\t\tModified by: Julia&nbsp;B&nbsp;Hirschberg</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2013/1145505/1145505_10118044_1385942025707_happy--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2013/1145505/1145505_10118044_1385942025707_happy--rgov-800width.jpg\" title=\"Happy Face\"><img src=\"/por/images/Reports/POR/2013/1145505/1145505_10118044_1385942025707_happy--rgov-66x44.jpg\" alt=\"Happy Face\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A face produced by WordsEye from text analysis.</div>\n<div class=\"imageCredit\">Bob Coyne</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Julia&nbsp;B&nbsp;Hirschberg</div>\n<div class=\"imageTitle\">Happy Face</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2013/1145505/1145505_10118044_1385942088269_angry--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2013/1145505/1145505_10118044_1385942088269_angry--rgov-800width.jpg\" title=\"Angry face\"><img src=\"/por/images/Reports/POR/2013/1145505/1145505_10118044_1385942088269_angry--rgov-66x44.jpg\" alt=\"Angry face\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Face created via text analysis in WordsEye</div>\n<div class=\"imageCredit\">Bob Coyne</div>\n<div class=\"imagePermisssions\">Royalty-free (unrestricted use)</div>\n<div class=\"imageSubmitted\">Julia&nbsp;B&nbsp;Hirschberg</div>\n<div class=\"imageTitle\">Angry face</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOur general goal in this project was to bring together research on sentiment analysis from text with studies of emotional speech to automatically identify a wide range of speaker emotions including happiness, sadness, anger, surprise, fear, and disgust from text.  These are thought to be the \"classic\" emotions in the psychology literature.   We trained a machine learning classifier on data from LiveJournal (a blog site in which people identify the emotion they most closely associate with their blog post) and on data we collected from annotators asked to describe a large set of scenes.  Our particular goal was to be able to determine what sort of emotional facial expressions we should generate in a project we have been working on in text-to-scene generation.  This system, called WordsEye, automatically creates scenes from simple text input.  We use a third-party program called FaceGen to generate faces with different expressions based upon our work in emotion classification from text.    We have published a paper on results of our research in which we report 40-63% success rates in predicting which emotion to generate (Ulinski et al, \"Finding Emotion in Image Descriptions,\" WISDOM 2012.  Samples of happy and angry faces generated in WordsEye are included with this report.  \n\n\t\t\t\t\tLast Modified: 12/01/2013\n\n\t\t\t\t\tSubmitted by: Julia B Hirschberg"
 }
}