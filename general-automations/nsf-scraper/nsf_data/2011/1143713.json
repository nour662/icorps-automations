{
 "awd_id": "1143713",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EAGER: Shared Visual Common Ground in Human-Robot Interaction for Small Unmanned Aerial Systems",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 300000.0,
 "awd_amount": 316000.0,
 "awd_min_amd_letter_date": "2011-07-21",
 "awd_max_amd_letter_date": "2012-03-27",
 "awd_abstract_narration": "This project will create a computational theory of visual common ground, allowing users to give directives to a robot (or other team members) and receive confirmation or constraints through visual communication over a shared visual display. The motivating example is an urban search and rescue (US&R) professional tapping, sketching, and annotating on an iPad in order to direct a small unmanned aerial system (sUAS) without training. Previous work in human-robot interaction with common ground has been limited to natural language, but recent work has shown that having all team members see the robot's eye view in unmanned ground robots significantly improved performance and situation awareness.  The proposed work populate the computational theory using the Shared Roles Model to represent the inputs (directives, notations), outputs (display viewpoint, form, size, location, content, etc.),  and transformations (visual communication engine).  The computational theory will be prototyped, refined, and tested by US&R practitioners flying realistic sUAS missions at Texas A&M's Disaster City.\r\n\r\nIntellectual merit: The project will create a computational theory of visual common ground that will enable two-way human-robot interaction using visual communication mechanisms such as tapping, sketching, and annotation on shared visual displays on mobile devices such as iPads, smartphones, and tablet PCs. The results will advance the fields of human-robot interaction, artificial intelligence, and cognitive science. \r\n\r\nBroader impacts:  The results could revolutionize how people use mobile devices to interact with robots (and with each other) using naturalistic visual mechanisms, bypassing extensive training. The project will actively recruit women, Hispanics, and persons with disabilities to participate through REU programs. An open source visual communication toolkit for HRI researchers will be produced. The results will improve robots for public safety, remote medicine, and telecommuting, and could also immediately help save lives through incorporation into Texas Task Force 1.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robin",
   "pi_last_name": "Murphy",
   "pi_mid_init": "R",
   "pi_sufx_name": "",
   "pi_full_name": "Robin R Murphy",
   "pi_email_addr": "robin.r.murphy@tamu.edu",
   "nsf_id": "000511836",
   "pi_start_date": "2011-07-21",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Bob",
   "pi_last_name": "McKee",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Bob McKee",
   "pi_email_addr": "Bob.McKee@TEEXmail.tamu.edu",
   "nsf_id": "000537820",
   "pi_start_date": "2011-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Texas A&M Engineering Experiment Station",
  "inst_street_address": "3124 TAMU",
  "inst_street_address_2": "",
  "inst_city_name": "COLLEGE STATION",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "9798626777",
  "inst_zip_code": "778433124",
  "inst_country_name": "United States",
  "cong_dist_code": "10",
  "st_cong_dist_code": "TX10",
  "org_lgl_bus_name": "TEXAS A&M ENGINEERING EXPERIMENT STATION",
  "org_prnt_uei_num": "QD1MX6N5YTN4",
  "org_uei_num": "QD1MX6N5YTN4"
 },
 "perf_inst": {
  "perf_inst_name": "Texas Engineering Experiment Station",
  "perf_str_addr": "",
  "perf_city_name": "College Station",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "778454645",
  "perf_ctry_code": "US",
  "perf_cong_dist": "10",
  "perf_st_cong_dist": "TX10",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7916",
   "pgm_ref_txt": "EAGER"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 300000.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;This project investigated how a visual common ground&nbsp; can improve human-robot interaction for remote presence applications, such as&nbsp; public safety, medical care, and telecommuting. Remote presence applications are where one or more humans use a robot to project themselves into an environment in order to complete a time-critical mission. One example is the use of small unmanned aerial systems (sUAS) for fire rescue, law enforcement, border patrols, and inspection of critical infrastructure. It is difficult to imagine the elimination of the human from these applications because of the need for human judgment, but at the same time, robots will become increasingly capable and autonomous.</p>\n<p>This project constructed three different interfaces and tested them with over 30 professional fire rescue professionals through monthly flights, five hazardous materials exercises, and one deployment to the SR530 Mudslides. &nbsp;The first two relied on visual common ground for the Mission Specialist and Pilot to see the same things, but both went about their tasks independently. The Dedicated interface allowed the expert (called a Mission Specialist) to see what the robot was seeing but without the artifacts that overlaid on the Pilot&rsquo;s displays. The Dedicated Active Mission Specialist interface} allowed the Mission Specialist to actively control the camera but did not not provide a mechanism for interacting with the Pilot other than verbal directives.&nbsp;&nbsp; The Shared Active interfaces were a major departure, changing both the Mission Specialists and Pilots display. They are are a set of interfaces that allow the Mission Specialist and Pilot to communicate with each other by sketching and spotlighting on the shared display of the robot's camera video. The Shared Active interfaces were more popular with responders, could be used by multiple responders at the same time, and reduced subtle safety risks.</p>\n<p>The project also refined the Shared Roles Model and used it to predict preconditions for unsafe acts and design (or modify) the interfaces to eliminate or mitigate the risk. The model assumes that the safe operation of an unmanned system is a function of the robot, the roles that the agents are expected to perform, and the interfaces and team coordination mechanisms; this is very different from existing cognitive architectures which try to determine the potential for human error independently of the specific hardware and interface.&nbsp; To date, five different categories of preconditions for unsafe acts stemming from role sharing were identified and four were predicted and observed in testing with the SUAS.</p>\n<p>While this project explored the fundamental science of visual common ground, the findings had immediate value to society and the economy. The dedicated interface was used for disaster response during the Center for Robot-Assisted Search and Rescue&rsquo;s deployment to the SR530 Washington State Mudslides. The dedicated interface was set up as a back up should a responder join the team, even though the flights were taskable agent style and no Mission Specialist was needed.. The interface clearly fulfilled its purpose of keeping responders from crowding or jostling the operator.</p>\n<p>Interfaces that allow experts to use SUAS without traveling to a site can revolutionize the economy. The shared active interface has attracted great interest because it can allow telework, potentially increasing the efficiency of inspecting critical infrastructure and responding to accidents and disasters.&nbsp; At the SR530 mudslides, once the CRASAR team had arrived on-site (1 day of travel), scouted landing zones and where to place observers to maintain line of sight (1 day), it still took 2 hours of travel time to reach the landing zone and begin flying. The entire flight time took 48 minutes but had to be conducted over a 3 hour period that included ...",
  "por_txt_cntn": "\n This project investigated how a visual common ground  can improve human-robot interaction for remote presence applications, such as  public safety, medical care, and telecommuting. Remote presence applications are where one or more humans use a robot to project themselves into an environment in order to complete a time-critical mission. One example is the use of small unmanned aerial systems (sUAS) for fire rescue, law enforcement, border patrols, and inspection of critical infrastructure. It is difficult to imagine the elimination of the human from these applications because of the need for human judgment, but at the same time, robots will become increasingly capable and autonomous.\n\nThis project constructed three different interfaces and tested them with over 30 professional fire rescue professionals through monthly flights, five hazardous materials exercises, and one deployment to the SR530 Mudslides.  The first two relied on visual common ground for the Mission Specialist and Pilot to see the same things, but both went about their tasks independently. The Dedicated interface allowed the expert (called a Mission Specialist) to see what the robot was seeing but without the artifacts that overlaid on the Pilot\u00c6s displays. The Dedicated Active Mission Specialist interface} allowed the Mission Specialist to actively control the camera but did not not provide a mechanism for interacting with the Pilot other than verbal directives.   The Shared Active interfaces were a major departure, changing both the Mission Specialists and Pilots display. They are are a set of interfaces that allow the Mission Specialist and Pilot to communicate with each other by sketching and spotlighting on the shared display of the robot's camera video. The Shared Active interfaces were more popular with responders, could be used by multiple responders at the same time, and reduced subtle safety risks.\n\nThe project also refined the Shared Roles Model and used it to predict preconditions for unsafe acts and design (or modify) the interfaces to eliminate or mitigate the risk. The model assumes that the safe operation of an unmanned system is a function of the robot, the roles that the agents are expected to perform, and the interfaces and team coordination mechanisms; this is very different from existing cognitive architectures which try to determine the potential for human error independently of the specific hardware and interface.  To date, five different categories of preconditions for unsafe acts stemming from role sharing were identified and four were predicted and observed in testing with the SUAS.\n\nWhile this project explored the fundamental science of visual common ground, the findings had immediate value to society and the economy. The dedicated interface was used for disaster response during the Center for Robot-Assisted Search and Rescue\u00c6s deployment to the SR530 Washington State Mudslides. The dedicated interface was set up as a back up should a responder join the team, even though the flights were taskable agent style and no Mission Specialist was needed.. The interface clearly fulfilled its purpose of keeping responders from crowding or jostling the operator.\n\nInterfaces that allow experts to use SUAS without traveling to a site can revolutionize the economy. The shared active interface has attracted great interest because it can allow telework, potentially increasing the efficiency of inspecting critical infrastructure and responding to accidents and disasters.  At the SR530 mudslides, once the CRASAR team had arrived on-site (1 day of travel), scouted landing zones and where to place observers to maintain line of sight (1 day), it still took 2 hours of travel time to reach the landing zone and begin flying. The entire flight time took 48 minutes but had to be conducted over a 3 hour period that included stopping for rain. Ignoring the two days of travel to and from the home office and one day of scouting, and considering only the day in th..."
 }
}