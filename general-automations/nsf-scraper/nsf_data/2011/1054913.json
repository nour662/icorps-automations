{
 "awd_id": "1054913",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Building and Searching a Structured Web Database",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2011-03-15",
 "awd_exp_date": "2017-02-28",
 "tot_intn_awd_amt": 488564.0,
 "awd_amount": 504548.0,
 "awd_min_amd_letter_date": "2011-03-09",
 "awd_max_amd_letter_date": "2014-07-16",
 "awd_abstract_narration": "This project investigates techniques for extracting and searching Web-embedded structured datasets.  For example, a manufacturer's site may contain technical product data, and a governmental site may contain economic statistics. Unfortunately, such data can be hard to isolate from surrounding text, and difficult to find using existing search engines that focus exclusively on documents.  The approach for the extraction step is to use current incomplete datasets to induce a large \"portfolio\" of possible extractors, apply all of them to crawled Web content, then test which are most successful.  The approach for the search step is to examine user query logs to find common patterns that describe the relationship between topic words and words that describe the dataset's structure; e.g., \"endangered species near the Mississippi River\" is a prototype for a many-to-many geographic relationship.  The central goal of this work is to eventually construct a working search engine for the structured-data component of the Web.\r\n\r\nThe success of this project is likely to increase access to structured datasets for a very broad population of users.  The project will also yield a large amount of novel extracted data relevant for scientific research, plus useful tools and query logs.  To accompany the research program, this project involves an educational plan that includes revised undergraduate course material, development of online educational material surrounding the datasets and tools, and a course on Web topics taught to a local rural high school.  All project results will be distributed at the project's Web site (http://www.eecs.umich.edu/~michjc/structuredweb/index.html).",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Michael",
   "pi_last_name": "Cafarella",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Michael Cafarella",
   "pi_email_addr": "michjc@umich.edu",
   "nsf_id": "000544946",
   "pi_start_date": "2011-03-09",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Regents of the University of Michigan - Ann Arbor",
  "inst_street_address": "1109 GEDDES AVE STE 3300",
  "inst_street_address_2": "",
  "inst_city_name": "ANN ARBOR",
  "inst_state_code": "MI",
  "inst_state_name": "Michigan",
  "inst_phone_num": "7347636438",
  "inst_zip_code": "481091015",
  "inst_country_name": "United States",
  "cong_dist_code": "06",
  "st_cong_dist_code": "MI06",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF MICHIGAN",
  "org_prnt_uei_num": "",
  "org_uei_num": "GNJ7BBP73WE9"
 },
 "perf_inst": {
  "perf_inst_name": "Regents of the University of Michigan - Ann Arbor",
  "perf_str_addr": "1109 GEDDES AVE STE 3300",
  "perf_city_name": "ANN ARBOR",
  "perf_st_code": "MI",
  "perf_st_name": "Michigan",
  "perf_zip_code": "481091015",
  "perf_ctry_code": "US",
  "perf_cong_dist": "06",
  "perf_st_cong_dist": "MI06",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "104500",
   "pgm_ele_name": "CAREER: FACULTY EARLY CAR DEV"
  },
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 198588.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 15984.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 93844.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 196132.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project focused on tools for building and querying a structured version of the Web. Most web information has taken the form of &ldquo;unstructured&rdquo; documents that can be read by individuals and discovered by search engines, but cannot be used to power modern query tools or analytical applications. In contrast, most commercial databases contain &ldquo;structured&rdquo; information (such as records about bank account holders) that are compatible with query and analytical tools, but which cover just a small fraction of the world&rsquo;s information.&nbsp;</p>\n<p>&nbsp;</p>\n<p>If we could reliably transform Web information into a structured representation, we could combine the Web&rsquo;s topic coverage with all the benefits of modern data tools. This would give Web users far more power than search engines typically offer; users could run queries of the kind usually limited to policy analysts and scientists. For example, a student could easily compute the average age of Nobel prize winners described on Wikipedia, or create a visualization of the start times of all elementary schools found online.</p>\n<p>&nbsp;</p>\n<p>The project comprised two core efforts.</p>\n<p>&nbsp;</p>\n<p>The first effort focused on extraction algorithms, which transform unstructured Web-embedded data into a relational tuple representation, suitable for query processing. In particular, this work focused on extracting data from (a) online spreadsheets, (b) social media texts, such as Twitter, and (c) Web pages that contain rare or unusual &ldquo;one-off&rdquo; data. In all of these efforts, the core criteria for success are (i) how accurately the resulting structured tuple represents the original unstructured information, (ii) how much human effort is required to obtain that level of accuracy, and (iii) the amount of computation needed to obtain it. Where practical, we published the extracted dataset for others to use directly; in some other cases, we published the methods needed for others to reconstruct our datasets.</p>\n<p>&nbsp;</p>\n<p>The second effort focused on tools to manipulate the resulting extracted data. In many cases, extracted structured data is sufficiently high quality that it can be used directly by standard data tools, such as relational databases and visualization suites. We built two tools for cases in which existing systems are not suitable. The first is a tool for easy data transformation, for cases in which an extracted dataset&rsquo;s particular structure does not match the user&rsquo;s intended target application. The second is a query tool designed specially for social media extractions, which exploits the fact that the vast majority of social media information is irrelevant to the user&rsquo;s query, and thus if carefully ignored can dramatically accelerate the user&rsquo;s work.</p>\n<p>&nbsp;</p>\n<p>All of the discovered methods were described and tested in papers published in top-level academic conferences.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/02/2018<br>\n\t\t\t\t\tModified by: Michael&nbsp;Cafarella</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project focused on tools for building and querying a structured version of the Web. Most web information has taken the form of \"unstructured\" documents that can be read by individuals and discovered by search engines, but cannot be used to power modern query tools or analytical applications. In contrast, most commercial databases contain \"structured\" information (such as records about bank account holders) that are compatible with query and analytical tools, but which cover just a small fraction of the world?s information. \n\n \n\nIf we could reliably transform Web information into a structured representation, we could combine the Web?s topic coverage with all the benefits of modern data tools. This would give Web users far more power than search engines typically offer; users could run queries of the kind usually limited to policy analysts and scientists. For example, a student could easily compute the average age of Nobel prize winners described on Wikipedia, or create a visualization of the start times of all elementary schools found online.\n\n \n\nThe project comprised two core efforts.\n\n \n\nThe first effort focused on extraction algorithms, which transform unstructured Web-embedded data into a relational tuple representation, suitable for query processing. In particular, this work focused on extracting data from (a) online spreadsheets, (b) social media texts, such as Twitter, and (c) Web pages that contain rare or unusual \"one-off\" data. In all of these efforts, the core criteria for success are (i) how accurately the resulting structured tuple represents the original unstructured information, (ii) how much human effort is required to obtain that level of accuracy, and (iii) the amount of computation needed to obtain it. Where practical, we published the extracted dataset for others to use directly; in some other cases, we published the methods needed for others to reconstruct our datasets.\n\n \n\nThe second effort focused on tools to manipulate the resulting extracted data. In many cases, extracted structured data is sufficiently high quality that it can be used directly by standard data tools, such as relational databases and visualization suites. We built two tools for cases in which existing systems are not suitable. The first is a tool for easy data transformation, for cases in which an extracted dataset?s particular structure does not match the user?s intended target application. The second is a query tool designed specially for social media extractions, which exploits the fact that the vast majority of social media information is irrelevant to the user?s query, and thus if carefully ignored can dramatically accelerate the user?s work.\n\n \n\nAll of the discovered methods were described and tested in papers published in top-level academic conferences.\n\n\t\t\t\t\tLast Modified: 08/02/2018\n\n\t\t\t\t\tSubmitted by: Michael Cafarella"
 }
}