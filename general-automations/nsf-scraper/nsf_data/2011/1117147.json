{
 "awd_id": "1117147",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Collaborative Research: ShapeShifting and PubSub for Tailoring Memory Accesses and Communication in Heterogeneous Multiprocessors",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 225000.0,
 "awd_amount": 225000.0,
 "awd_min_amd_letter_date": "2011-06-21",
 "awd_max_amd_letter_date": "2011-06-21",
 "awd_abstract_narration": "Over the past decade or more, microprocessors have faced increasing challenges in achieving high-performance for current and emerging software applications while abiding by severe power and thermal limits. In response, industry has turned to approaches that use specialized graphics and computational hardware and complex memory organizations.  The end result is that computer systems have become more heterogeneous and complex, in ways that make it difficult for programmers to write efficient and high-performance software.  Software tuned to run on one implementation will often not run at all or will perform poorly or unpredictably when ported to even a different implementation in the same chip family. \r\n\r\nThe objective of this research effort is to design and evaluate system and hardware support that tailors memory and data access/movements to improve performance and power efficiency, while also easing the issues of programmability and of tuning software for individual chip characteristics.\r\nThe two key themes of this work are Shape Shifting and PubSub data abstractions.  ShapeShifting refers to optimizations and hardware support structures that allow data to be transformed in layout, in order to support faster access, more efficient use of memory, and other attributes that improve power and performance.  In some preliminary experiments, even a software-only implementation of Shape Shifting improves performance by 15%. Pub Sub data abstractions offer methods for individual processors to indicate interest (or disinterest) in updates regarding other program variables.  These abstractions form the underpinning for memory optimizations that are tailored to the application?s memory usage patterns. By mitigating false sharing, encouraging coarse-grained fetches, and reducing coherence broadcasts to uninterested cores, PubSub has the potential to improve the power and performance efficiency of multi-core implementations by a factor of 2X or more.\r\n\r\nThe research program is targeting several types of broad impact.  First, the simulators and tools developed by this project will be released as free, open-source software. Second, the results can enhance performance and energy efficiency of future parallel hardware.  Energy-efficiency is of particular concern from a national economic and strategic standpoint, given the growing electricity consumption of computer systems and the important role of the memory hierarchy in influencing computer power consumption.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Margaret",
   "pi_last_name": "Martonosi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Margaret Martonosi",
   "pi_email_addr": "martonosi@princeton.edu",
   "nsf_id": "000395997",
   "pi_start_date": "2011-06-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Princeton University",
  "inst_street_address": "1 NASSAU HALL",
  "inst_street_address_2": "",
  "inst_city_name": "PRINCETON",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "6092583090",
  "inst_zip_code": "085442001",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "THE TRUSTEES OF PRINCETON UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NJ1YPQXQG7U5"
 },
 "perf_inst": {
  "perf_inst_name": "Princeton University",
  "perf_str_addr": "1 NASSAU HALL",
  "perf_city_name": "PRINCETON",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "085442001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 225000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Over the past decade or more, microprocessors have faced increasing challenges in achieving high-performance for current and emerging applications while abiding by severe power limits. In response, industry has turned to multi-core approaches, often with specialized functional units or heterogeneous cores. In addition, memory hierarchies have also become more heterogeneous and complex, with multiple levels of shared and private caches, software-managed scratchpads, and configurable cache/scratchpad combinations.</p>\n<p>While embracing heterogeneity has allowed CMPs and SoCs to reach application power and performance targets, it generally does so at the expense of programmability and performance portability. Software tuned to run on one implementation will often not run at all or will perform poorly or unpredictably when ported to even a different implementation in the same chip family. The goal of our research is to propose, design, and evaluate system and hardware support that will allow the tailoring of memory and data access/movements for performance and power efficiency, while also easing the issues of programmability and of tuning software for individual chip characteristics.</p>\n<p>Throughout the evolution of the project, a key aspect of it has been identifying and mitigating communication bottlenecks in CPU-GPU systems. In addition, another key goal has been research on performance estimation and design space exploration techniques to identify the hardware and software parameter settings that offer the best performance, the best power dissipation, or the best portability across several platforms. Finally, we also explored verification methods to ascertain the correctness of communication between CPUs, GPUs, and accelerators.</p>\n<p><strong><span style=\"text-decoration: underline;\">Design Space Exploration Tools:</span></strong> We developed the Starchart and Stargazer tools for estimating performance and power across complicated GPU design spaces by using statistical techniques for regression and partitioning. Published at PACT 2013, Starchart automates factor selection and automatically identifies key \"split points\" in a design space. Split points represent points where e.g. performance saturates, power takes a steep increase, etc. Starchart helps hardware designers and programmers identify good operating points for GPU-based systems.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Cross-Platform Performance Estimation:</strong></span> Building on the Starchart and Stargazer work, we have also done research on using the performance results from one platform (Xeon Phi) to estimate the performance of codes if they were ported to another platform (NVIDIA GPU). The accuracy of our performance estimators is very good (within 5%) for compute-bound kernels. Kernels with more data communication are more challenging for this cross-platform estimation, but we have indicator variables (i.e. confidence predictors) that allow us to identify (before porting) the cases where the estimate is not likely to be accurate.</p>\n<p><span style=\"text-decoration: underline;\"><strong>GPU Performance Optimization: </strong></span>Based on our extensive performance estimation and analysis work (above) we also have new papers published regarding ways to reduce the performance impact of memory boundedness in GPUs (MRPB, HPCA 2014) and to properly adjust thread operating points in order to maximize performance while reducing resource usage.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Decoupled Supply-Compute (DeSC) Approaches: </strong></span>While heterogeneous and specialized parallelism shows great leverage improving computation performance at manageable power, its effective use raises additional challenges. As specialized accelerators speed up computations, the communication or memory operations that feed them represent even more of the remaining performance slowdown. In addition, the software-managed communication tailoring used to reduce communication cost often increases software complexity and reduces performance portability. Our DeSC architectures improve the performance, programmer effort, and software portability of accelerator-based systems, by employing automatic compiler techniques to separate data access and address calculations from value computations. Once separated, the code is targeted at either the accelerator itself (for the compute slice) or a data supply unit that feeds it. The data supply unit can either be a general-purpose core or can be another accelerator tailored to this role.</p>\n<p><span style=\"text-decoration: underline;\"><strong>Memory Consistency Model Translation and Verification: </strong></span>Another key thrust of our work was on formal techniques for specifying, verifying, and translating between memory consistency models. This work is central to the heterogeneous communication issues of this research grant, because it allows different compute elements to transfer data in ways that are high-performance, interoperable, and formally verifiable. The key to our approach is the development of microarchitecture-level happens-before graphs (uHBGs). Nodes in these graphs correspond to hardware units, and edges correspond to known event orderings. By considering all possible event orderings by which different values might flow through complex memory hierarchies, memory consistency models can be checked. Our initial work focused on the microarchitecture itself, while subsequent work expanded it to consider cache&nbsp;coherence and address translation interactions.</p>\n<p>Overall, this work has produced numerous publications and open-source software releases.&nbsp; The research has generated innovative ideas that are being further explored by companies and are likely to influence computer systems hardware and software for years to come.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/05/2016<br>\n\t\t\t\t\tModified by: Margaret&nbsp;Martonosi</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOver the past decade or more, microprocessors have faced increasing challenges in achieving high-performance for current and emerging applications while abiding by severe power limits. In response, industry has turned to multi-core approaches, often with specialized functional units or heterogeneous cores. In addition, memory hierarchies have also become more heterogeneous and complex, with multiple levels of shared and private caches, software-managed scratchpads, and configurable cache/scratchpad combinations.\n\nWhile embracing heterogeneity has allowed CMPs and SoCs to reach application power and performance targets, it generally does so at the expense of programmability and performance portability. Software tuned to run on one implementation will often not run at all or will perform poorly or unpredictably when ported to even a different implementation in the same chip family. The goal of our research is to propose, design, and evaluate system and hardware support that will allow the tailoring of memory and data access/movements for performance and power efficiency, while also easing the issues of programmability and of tuning software for individual chip characteristics.\n\nThroughout the evolution of the project, a key aspect of it has been identifying and mitigating communication bottlenecks in CPU-GPU systems. In addition, another key goal has been research on performance estimation and design space exploration techniques to identify the hardware and software parameter settings that offer the best performance, the best power dissipation, or the best portability across several platforms. Finally, we also explored verification methods to ascertain the correctness of communication between CPUs, GPUs, and accelerators.\n\nDesign Space Exploration Tools: We developed the Starchart and Stargazer tools for estimating performance and power across complicated GPU design spaces by using statistical techniques for regression and partitioning. Published at PACT 2013, Starchart automates factor selection and automatically identifies key \"split points\" in a design space. Split points represent points where e.g. performance saturates, power takes a steep increase, etc. Starchart helps hardware designers and programmers identify good operating points for GPU-based systems.\n\nCross-Platform Performance Estimation: Building on the Starchart and Stargazer work, we have also done research on using the performance results from one platform (Xeon Phi) to estimate the performance of codes if they were ported to another platform (NVIDIA GPU). The accuracy of our performance estimators is very good (within 5%) for compute-bound kernels. Kernels with more data communication are more challenging for this cross-platform estimation, but we have indicator variables (i.e. confidence predictors) that allow us to identify (before porting) the cases where the estimate is not likely to be accurate.\n\nGPU Performance Optimization: Based on our extensive performance estimation and analysis work (above) we also have new papers published regarding ways to reduce the performance impact of memory boundedness in GPUs (MRPB, HPCA 2014) and to properly adjust thread operating points in order to maximize performance while reducing resource usage.\n\nDecoupled Supply-Compute (DeSC) Approaches: While heterogeneous and specialized parallelism shows great leverage improving computation performance at manageable power, its effective use raises additional challenges. As specialized accelerators speed up computations, the communication or memory operations that feed them represent even more of the remaining performance slowdown. In addition, the software-managed communication tailoring used to reduce communication cost often increases software complexity and reduces performance portability. Our DeSC architectures improve the performance, programmer effort, and software portability of accelerator-based systems, by employing automatic compiler techniques to separate data access and address calculations from value computations. Once separated, the code is targeted at either the accelerator itself (for the compute slice) or a data supply unit that feeds it. The data supply unit can either be a general-purpose core or can be another accelerator tailored to this role.\n\nMemory Consistency Model Translation and Verification: Another key thrust of our work was on formal techniques for specifying, verifying, and translating between memory consistency models. This work is central to the heterogeneous communication issues of this research grant, because it allows different compute elements to transfer data in ways that are high-performance, interoperable, and formally verifiable. The key to our approach is the development of microarchitecture-level happens-before graphs (uHBGs). Nodes in these graphs correspond to hardware units, and edges correspond to known event orderings. By considering all possible event orderings by which different values might flow through complex memory hierarchies, memory consistency models can be checked. Our initial work focused on the microarchitecture itself, while subsequent work expanded it to consider cache coherence and address translation interactions.\n\nOverall, this work has produced numerous publications and open-source software releases.  The research has generated innovative ideas that are being further explored by companies and are likely to influence computer systems hardware and software for years to come.\n\n \n\n\t\t\t\t\tLast Modified: 09/05/2016\n\n\t\t\t\t\tSubmitted by: Margaret Martonosi"
 }
}