{
 "awd_id": "1117913",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Building Linked Open Services from Online Sources",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2011-10-01",
 "awd_exp_date": "2017-05-31",
 "tot_intn_awd_amt": 500000.0,
 "awd_amount": 532000.0,
 "awd_min_amd_letter_date": "2011-08-06",
 "awd_max_amd_letter_date": "2015-07-09",
 "awd_abstract_narration": "Most of the work on the Semantic Web and more broadly Information Integration assumes that accurate semantic models of sources exist. In practice, although there is a tremendous amount of data available on the Web, there is rarely any semantic description of the sources and the information provided by them. This project will develop a new approach that addresses the problem of automatically discovering and modeling sources by building on the recent development of Linked Open Data within the Semantic Web. The resulting system will be able to learn about any source where there is background knowledge in the Linked Open Data. This work will be a significant advance over what was done previously since it will allow an intelligent system to expand its knowledge to learn models of sources that cover information for which the system has no known sources.  The system will start with all of the data and knowledge in the Linked Open Data as well as semantic descriptions of some related services (through the work on Linked Open Services). The system will then learn how a new source relates to the known sources by exploiting the knowledge already available the the Linked Open Data.  The result will be a rich semantic description of the individual sources that can be used in Semantic Web applications and information integration systems.\r\n\r\nThe ability to automatically discover and learn detailed semantic descriptions across a range of sources that go beyond the current source descriptions will greatly expand the utility of Semantic Web and information integration systems. This capability will allow people and systems to better exploit the massive amount of data available today on the Internet and provide a tool to keep up with its growth.  Within the bioinformatics world, for example, the amount of data continues to grow rapidly, and the ability to find and structure this data will have a significant impact on ability of researchers to fully exploit all of this information to solve biomedical research questions, such as finding more effective treatments for cancer.  More information about the project can be found at http://www.isi.edu/integration/people/knoblock/projects/prj_source_modeling.html",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Craig",
   "pi_last_name": "Knoblock",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Craig A Knoblock",
   "pi_email_addr": "knoblock@isi.edu",
   "nsf_id": "000162603",
   "pi_start_date": "2011-08-06",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Jose Luis",
   "pi_last_name": "Ambite",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Jose Luis Ambite",
   "pi_email_addr": "ambite@isi.edu",
   "nsf_id": "000489745",
   "pi_start_date": "2011-08-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Southern California",
  "inst_street_address": "3720 S FLOWER ST FL 3",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "2137407762",
  "inst_zip_code": "90033",
  "inst_country_name": "United States",
  "cong_dist_code": "34",
  "st_cong_dist_code": "CA34",
  "org_lgl_bus_name": "UNIVERSITY OF SOUTHERN CALIFORNIA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G88KLJR3KYT5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Southern California",
  "perf_str_addr": "3720 S FLOWER ST FL 3",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "90033",
  "perf_ctry_code": "US",
  "perf_cong_dist": "34",
  "perf_st_cong_dist": "CA34",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 500000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 16000.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Most of the work on the Semantic Web and more broadly Information Integration assumes that accurate semantic models of sources exist. &nbsp;These models describe the precise content of a source and are essential to automatically finding and integrating the data.&nbsp; &nbsp;In practice, although there are a tremendous number of sources available, there is rarely any semantic description of the sources and the information provided by them.&nbsp; This project developed a new approach that addresses the problem of creating semantic models of data sources.&nbsp; The resulting system learns a model of a data source by using previously modeled sources or using background knowledge from the Linked Data cloud.&nbsp; &nbsp;The basic approach is to use machine learning algorithms to learn from previously modeled sources and then propose the most likely model of a new source.&nbsp; In cases where the proposed model is not correct, a user can then refine this model using a graphical user interface and the system would then learn from those refinements for processing future sources.&nbsp; This work provides a significant advance over what was done previously since it makes it possible to rapidly construct semantic models of new sources.&nbsp; The research has been applied and shown to be effective in a number of real-world applications, including integrating cultural heritage data, research networking data, and biological data.&nbsp;&nbsp; The work is implemented in a system called Karma, which has been released as open source and is already in widespread use.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 08/30/2017<br>\n\t\t\t\t\tModified by: Craig&nbsp;A&nbsp;Knoblock</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nMost of the work on the Semantic Web and more broadly Information Integration assumes that accurate semantic models of sources exist.  These models describe the precise content of a source and are essential to automatically finding and integrating the data.   In practice, although there are a tremendous number of sources available, there is rarely any semantic description of the sources and the information provided by them.  This project developed a new approach that addresses the problem of creating semantic models of data sources.  The resulting system learns a model of a data source by using previously modeled sources or using background knowledge from the Linked Data cloud.   The basic approach is to use machine learning algorithms to learn from previously modeled sources and then propose the most likely model of a new source.  In cases where the proposed model is not correct, a user can then refine this model using a graphical user interface and the system would then learn from those refinements for processing future sources.  This work provides a significant advance over what was done previously since it makes it possible to rapidly construct semantic models of new sources.  The research has been applied and shown to be effective in a number of real-world applications, including integrating cultural heritage data, research networking data, and biological data.   The work is implemented in a system called Karma, which has been released as open source and is already in widespread use.\n\n \n\n\t\t\t\t\tLast Modified: 08/30/2017\n\n\t\t\t\t\tSubmitted by: Craig A Knoblock"
 }
}