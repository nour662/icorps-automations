{
 "awd_id": "1127228",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SDCI Net: Collaborative Research: An integrated study of datacenter networking and 100 GigE wide-area networking in support of distributed scientific computing",
 "cfda_num": "47.070",
 "org_code": "05090000",
 "po_phone": "7032924220",
 "po_email": "kthompso@nsf.gov",
 "po_sign_block_name": "Kevin Thompson",
 "awd_eff_date": "2011-09-15",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 473351.0,
 "awd_amount": 473351.0,
 "awd_min_amd_letter_date": "2011-09-07",
 "awd_max_amd_letter_date": "2011-09-07",
 "awd_abstract_narration": "As supercomputing speeds increase to peta- and exaflops, scientists are increasing their scale and range of simulations, which are resulting in ever growing datasets that need to be moved to local computers at the scientists' own laboratories. The first goal of this project is to identify bottlenecks that result in poor and/or inconsistent end-to-end application-level throughput using data collection and analysis by working in conjunction with scientists in the Community Earth System Model (CESM) project. With knowledge of the weakest components in the end-to-end chain, we plan to experiment in a controlled environment using a testbed that consists of a high-end cluster at NERSC, which is capable of sourcing/sinking data to disks at close to 100Gbps speeds, and other high-performance computing systems connected via the DOE 100Gbps Advanced Networking Initiative (ANI) prototype network. Multiple datacenter networking technologies such as Remote Direct Memory Access (RDMA) over Converged Ethernet (RoCE) and Internet Wide-Area RDMA Protocol (iWARP) will be combined with high- speed (100Gb/s) wide-area networking solutions, such as dedicated virtual circuits and IP-routed paths, respectively, for a comparative performance study of file transfers and wide-area MPI I/O. A new software module of the Extended-Sockets API (ES-API), which offers RDMA features such as zero-copy operations, will be prototyped and integrated into file transfer applications. Finally, trials will be organized to transfer the best identified solutions to CESM and other scientists. The intellectual merit of the proposed project consists of: i) a systematic scientific approach to determine the reasons for poor end-to-end application-level performance experienced by CESM scientists, ii) development of integrated datacenter and wide-area networking solutions to address the identified problems, and iii) the enabling of these solutions to be utilized by CESM and other science projects. The broader impacts of the proposed activities consist of i) the creation of a course module on datacenter networking, and the involvement of undergraduate students in this research at all three institutions, ii) diversity and outreach programs, and iii) the active promotion of the developed solutions to the CESM project and other scientists.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "OAC",
 "org_div_long_name": "Office of Advanced Cyberinfrastructure (OAC)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Robert",
   "pi_last_name": "Russell",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Robert D Russell",
   "pi_email_addr": "rdr@cs.unh.edu",
   "nsf_id": "000113683",
   "pi_start_date": "2011-09-07",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of New Hampshire",
  "inst_street_address": "51 COLLEGE RD",
  "inst_street_address_2": "BLDG 107",
  "inst_city_name": "DURHAM",
  "inst_state_code": "NH",
  "inst_state_name": "New Hampshire",
  "inst_phone_num": "6038622172",
  "inst_zip_code": "038242620",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NH01",
  "org_lgl_bus_name": "UNIVERSITY SYSTEM OF NEW HAMPSHIRE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GBNGC495XA67"
 },
 "perf_inst": {
  "perf_inst_name": "UNH InterOperability Laboratory",
  "perf_str_addr": "121 Technology Drive, Suite 2",
  "perf_city_name": "Durham",
  "perf_st_code": "NH",
  "perf_st_name": "New Hampshire",
  "perf_zip_code": "038244716",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NH01",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "768300",
   "pgm_ele_name": "SOFTWARE DEVELOPEMENT FOR CI"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7683",
   "pgm_ref_txt": "SOFTWARE DEVELOPEMENT FOR CI"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 473351.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our project was designed to determine the reasons for wide variation in performance when running big applications on large supercomputer clusters. These clusters are constructed by interconnecting thousands of individual compute nodes with very high-speed interconnect networks.&nbsp; One such interconnect is called InfiniBand, which is the hardware installed on the supercomputer installation at the National Center for Atmospheric Research (NCAR), one of our collaborators, and on a small cluster at our facility, the University of New Hampshire's InterOperability Laboratory (IOL).&nbsp; We used both installations in our studies.</p>\n<p>Because it is designed to provide a reliable network transport with low latency and high bandwidth utilization, InfiniBand hardware implements link-level credit-based flow control that prevents packets from being dropped due to lack of memory on the receiving end of each network link between nodes and/or switches.&nbsp; This can lead to congestion in network switches, which is basically caused when traffic arriving at the switch from several links at the same time all needs to be forwarded out a single output link (analogous to congestion that occurs when the number of lanes on a highway decreases from 2 or more down to 1).&nbsp; Because InfiniBand does not drop packets, they enter a \"waiting line\" (traffic jam) in the limited memory of the switch. That memory rapidly fills up, causing the switch to stop accepting new packets from those input links (by not granting them the credits they need to send these packets).&nbsp; This leads a phenomenon known as \"congestion tree spreading\" as congestion starting at one switch triggers traffic that \"backs up\" and causes new congestion at upstream switches.&nbsp; This increases the delays experienced by the packets waiting in the memory of each of these switches in turn.&nbsp; This is one reason supercomputer applications can experience great variability in performance -- it depends on the network traffic generated by other applications sharing the supercomputer and its interconnection network.</p>\n<p>The solution to this in conventional TCP/IP/Ethernet is to simply drop packets at the switch first detecting the congestion, but that in turn requires the TCP layer software, responsible for delivering messages reliably across the network, to detect and retransmit the dropped packets, thereby increasing delay and decreasing bandwidth utilization, sometimes even more so than the congestion delay occurring in&nbsp; Infiniband switches.</p>\n<p>In this project we developed 2 approaches to this problem, one based on redesigning the existing hardware congestion control mechanism currently implemented in InfiniBand hardware, the other based on developing a management application that utilizes the existing InfiniBand hardware congestion control mechanism and counters.&nbsp; The intent of both approaches is to dynamically detect and react quickly to congestion as it arises.&nbsp; The redesign approach does this by developing new mechanisms in the switch hardware that do not need the extensive hard-to-configure parameter settings required by the current hardware.&nbsp; Since we could not change the real hardware on our systems, we studied these new mechanisms by constructing simulation models. The management approach does this by dynamically reconfiguring the parameter settings for existing hardware \"on the fly\".&nbsp; Both approaches show promise for future systems, as documented in our published papers, because the massive size of supercomputer clusters that will be constructed in the Exascale world will require adaptive algorithms that work quickly and effectively without complex manual configuration.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/29/2016<br>\n\t\t\t\t\tModified by: Robert&nbsp;D&nbsp;Russell</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nOur project was designed to determine the reasons for wide variation in performance when running big applications on large supercomputer clusters. These clusters are constructed by interconnecting thousands of individual compute nodes with very high-speed interconnect networks.  One such interconnect is called InfiniBand, which is the hardware installed on the supercomputer installation at the National Center for Atmospheric Research (NCAR), one of our collaborators, and on a small cluster at our facility, the University of New Hampshire's InterOperability Laboratory (IOL).  We used both installations in our studies.\n\nBecause it is designed to provide a reliable network transport with low latency and high bandwidth utilization, InfiniBand hardware implements link-level credit-based flow control that prevents packets from being dropped due to lack of memory on the receiving end of each network link between nodes and/or switches.  This can lead to congestion in network switches, which is basically caused when traffic arriving at the switch from several links at the same time all needs to be forwarded out a single output link (analogous to congestion that occurs when the number of lanes on a highway decreases from 2 or more down to 1).  Because InfiniBand does not drop packets, they enter a \"waiting line\" (traffic jam) in the limited memory of the switch. That memory rapidly fills up, causing the switch to stop accepting new packets from those input links (by not granting them the credits they need to send these packets).  This leads a phenomenon known as \"congestion tree spreading\" as congestion starting at one switch triggers traffic that \"backs up\" and causes new congestion at upstream switches.  This increases the delays experienced by the packets waiting in the memory of each of these switches in turn.  This is one reason supercomputer applications can experience great variability in performance -- it depends on the network traffic generated by other applications sharing the supercomputer and its interconnection network.\n\nThe solution to this in conventional TCP/IP/Ethernet is to simply drop packets at the switch first detecting the congestion, but that in turn requires the TCP layer software, responsible for delivering messages reliably across the network, to detect and retransmit the dropped packets, thereby increasing delay and decreasing bandwidth utilization, sometimes even more so than the congestion delay occurring in  Infiniband switches.\n\nIn this project we developed 2 approaches to this problem, one based on redesigning the existing hardware congestion control mechanism currently implemented in InfiniBand hardware, the other based on developing a management application that utilizes the existing InfiniBand hardware congestion control mechanism and counters.  The intent of both approaches is to dynamically detect and react quickly to congestion as it arises.  The redesign approach does this by developing new mechanisms in the switch hardware that do not need the extensive hard-to-configure parameter settings required by the current hardware.  Since we could not change the real hardware on our systems, we studied these new mechanisms by constructing simulation models. The management approach does this by dynamically reconfiguring the parameter settings for existing hardware \"on the fly\".  Both approaches show promise for future systems, as documented in our published papers, because the massive size of supercomputer clusters that will be constructed in the Exascale world will require adaptive algorithms that work quickly and effectively without complex manual configuration.\n\n\t\t\t\t\tLast Modified: 11/29/2016\n\n\t\t\t\t\tSubmitted by: Robert D Russell"
 }
}