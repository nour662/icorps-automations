{
 "awd_id": "1116372",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "SHF: Small: Directoryless Shared Memory Using Execution Migration",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tao Li",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 400000.0,
 "awd_amount": 400000.0,
 "awd_min_amd_letter_date": "2011-06-23",
 "awd_max_amd_letter_date": "2011-06-23",
 "awd_abstract_narration": "The increase in processor clock frequencies from 1980-2003 has slowed down significantly in recent years. To improve computer performance computer architects are exploring parallel architectures including many-core architectures.  In a many-core or multi-core architecture, processor cores with relatively low complexity are connected to memory and to each other via high-bandwidth on-chip interconnect. The most popular programming model for multi-cores is that of shared memory.  In this memory model, programmers write different threads that can run on different processors all of which can share a single memory space.  This means that the on-chip cache memory on the multi-core chip should behave like a large shared cache. Unfortunately, current schemes for cache coherence either suffer from lack of scalability or require large directories at each core significantly increasing chip area and power.\r\n \r\nA directoryless cache coherence scheme is being investigated in this project that relies on the mechanism of execution migration. In execution migration, a thread?s context or state moves to the processor in whose cache the data resides.  An important advantage of an execution migration architecture is that only a one-way trip is required to access data, since the thread moves to access data.  In conventional data migration architectures, a round-trip is required to access data ? a request is sent to the location where the data resides and then the data is sent to the requesting thread. Further, only one copy of data need be present on chip if execution migration is used, since threads can move.  This means that cache coherence is trivially ensured.  Moreover, the chip can store more distinct data, since data is not replicated and this reduces off-chip access rates.  Finally, an execution migration architecture can exploit the plentiful on-chip bandwidth available to speed up thread migration, thereby reducing data access latency.\r\n\r\nThere are challenges associated with this architecture corresponding to contention for shared data across multiple threads, and the energy required to move thread contexts.  The first challenge is being met through judicious replication of data at the program source level or compiler level.  In particular, limited read copies of data are created across multiple threads. Since these copies only exist in between two writes to the data, coherence is ensured as before without need for complex coherence logic. However, contention for shared data is significantly reduced.  The second challenge of energy consumption is being met through migration of partial thread contexts ? if a stack machine is used as the processor core, energy consumption can be reduced by migrating a subset of the thread context corresponding to the top part of the stack instead of the entire stack.\r\n\r\nIn this project, an Execution Migration Machine with over 100 cores is being designed, and being evaluated using cycle-accurate simulation, and critical elements of the machine are being built on a Field Programmable Gate Array (FPGA). This project has the potential to meet the scalability and programmability challenges that face shared memory multi-core architectures.  The Execution Migration Machine design will shed insight into how best thread migration can be used to enhance multi-core performance, possibly in combination with data migration. If successful, the project will impact the design of future multi-core processors through intelligent use of program and data migration.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Srini",
   "pi_last_name": "Devadas",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Srini Devadas",
   "pi_email_addr": "devadas@mit.edu",
   "nsf_id": "000451511",
   "pi_start_date": "2011-06-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779800",
   "pgm_ele_name": "Software & Hardware Foundation"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7941",
   "pgm_ref_txt": "COMPUTER ARCHITECTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 400000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Current trends in microprocessor design clearly indicate an era of multicores for the 2010s. As transistor density continues to grow geometrically, processor manufacturers are already able to place a hundred cores on a chip, with massive multicore chips on the horizon; many industry pundits are predicting 1000 or more cores by the middle of this decade. It is not clear that current architectures and their memory subsystems will scale to hundreds of cores, and whether these systems will be easy to program.</p>\n<p>The main barrier to scaling current memory architectures is the off-chip memory bandwidth wall: off-chip bandwidth grows with package pin density, which scales much more slowly than on-die transistor density. &nbsp;It is therefore important to have cache memory on chip that can store enough data so programs do not have to access off-chip memory frequently. &nbsp;The power requirements of large caches (which grow quadratically with size) exclude their use in chips on a 1000-core scale. &nbsp;For massive-scale multicores, then, we are left with relatively small per-core caches. In order to achieve high performance, data that a program running on a core requires should with high probability be present in the core's cache. &nbsp;Data that is shared across multiple programs therefore needs to be replicated.</p>\n<p><span style=\"font-size: 12px;\">Replication of data significantly decreases the effective total on-chip cache size because, as the core counts grow, a lot of cache space is taken by replicas and less shared data in total can be cached, which in turn leads to sharply increased off-chip access rates. &nbsp;Further, given that the per-core caches are small, data needs to constantly be moved in and out of these caches.</span></p>\n<p><span style=\"font-size: 12px;\">To address this limitation and take advantage of available data locality in a memory organization where there is only one copy of data, we propose to allow programs to migrate from one core to another at a fine-grained instruction level. When several consecutive accesses are made to data assigned to a given core, migrating the execution context allows the program to make a sequence of local accesses on the destination core rather than pay the performance penalty of moving the data over the original core's cache (and potentially overwriting useful data).&nbsp;</span></p>\n<p>In this project,&nbsp;we created a detailed implementation of hardware-level instruction-granularity program migration in a 110-core chip multiprocessor. Implemented in 45nm ASIC technology, the chip occupies 100mm<sup>2 </sup>and contains over 357 million transistors.&nbsp;We built a board to house the chip (see associated image that shows the chip in a board) tested the underlying hardware implementation through simulation and tested the chip.</p>\n<p>With a custom stack-based Instruction Set Architecture to enable partial context migration, when there is no network congestion, our implementation provides end-to-end migration latency of 4 cycles between neighboring cores with a minimum thread context, and 33 cycles between the farthest cores with a maximum context. Our cores learn a program&rsquo;s data access patterns and migrate programs automatically. We have demonstrated that migration can reduce on-chip data movement by up to 14&times; at a relatively small area increase of 23%. &nbsp;</p>\n<p>To the best of our knowledge, the Execution Migration Machine is the first hardware implementation of program migration in a chip multiprocessor. &nbsp;We have shown that fast hardware-level program migration is viable and useful to reduce on-chip and off-chip data movement.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/02/2015<br>\n\t\t\t\t\tModified by: Srini&nbsp;Devadas</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImage\n\t\t\t\t\t\t\t\t</div>\n<...",
  "por_txt_cntn": "\nCurrent trends in microprocessor design clearly indicate an era of multicores for the 2010s. As transistor density continues to grow geometrically, processor manufacturers are already able to place a hundred cores on a chip, with massive multicore chips on the horizon; many industry pundits are predicting 1000 or more cores by the middle of this decade. It is not clear that current architectures and their memory subsystems will scale to hundreds of cores, and whether these systems will be easy to program.\n\nThe main barrier to scaling current memory architectures is the off-chip memory bandwidth wall: off-chip bandwidth grows with package pin density, which scales much more slowly than on-die transistor density.  It is therefore important to have cache memory on chip that can store enough data so programs do not have to access off-chip memory frequently.  The power requirements of large caches (which grow quadratically with size) exclude their use in chips on a 1000-core scale.  For massive-scale multicores, then, we are left with relatively small per-core caches. In order to achieve high performance, data that a program running on a core requires should with high probability be present in the core's cache.  Data that is shared across multiple programs therefore needs to be replicated.\n\nReplication of data significantly decreases the effective total on-chip cache size because, as the core counts grow, a lot of cache space is taken by replicas and less shared data in total can be cached, which in turn leads to sharply increased off-chip access rates.  Further, given that the per-core caches are small, data needs to constantly be moved in and out of these caches.\n\nTo address this limitation and take advantage of available data locality in a memory organization where there is only one copy of data, we propose to allow programs to migrate from one core to another at a fine-grained instruction level. When several consecutive accesses are made to data assigned to a given core, migrating the execution context allows the program to make a sequence of local accesses on the destination core rather than pay the performance penalty of moving the data over the original core's cache (and potentially overwriting useful data). \n\nIn this project, we created a detailed implementation of hardware-level instruction-granularity program migration in a 110-core chip multiprocessor. Implemented in 45nm ASIC technology, the chip occupies 100mm2 and contains over 357 million transistors. We built a board to house the chip (see associated image that shows the chip in a board) tested the underlying hardware implementation through simulation and tested the chip.\n\nWith a custom stack-based Instruction Set Architecture to enable partial context migration, when there is no network congestion, our implementation provides end-to-end migration latency of 4 cycles between neighboring cores with a minimum thread context, and 33 cycles between the farthest cores with a maximum context. Our cores learn a program\u00c6s data access patterns and migrate programs automatically. We have demonstrated that migration can reduce on-chip data movement by up to 14&times; at a relatively small area increase of 23%.  \n\nTo the best of our knowledge, the Execution Migration Machine is the first hardware implementation of program migration in a chip multiprocessor.  We have shown that fast hardware-level program migration is viable and useful to reduce on-chip and off-chip data movement.\n\n\t\t\t\t\tLast Modified: 09/02/2015\n\n\t\t\t\t\tSubmitted by: Srini Devadas"
 }
}