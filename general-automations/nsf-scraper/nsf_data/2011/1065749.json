{
 "awd_id": "1065749",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Processes Determining the Abundance of Terrestrial Wildlife Communities Across Large Scales",
 "cfda_num": "47.074",
 "org_code": "08040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Elizabeth Blood",
 "awd_eff_date": "2011-07-01",
 "awd_exp_date": "2014-06-30",
 "tot_intn_awd_amt": 374178.0,
 "awd_amount": 374178.0,
 "awd_min_amd_letter_date": "2011-06-23",
 "awd_max_amd_letter_date": "2011-06-23",
 "awd_abstract_narration": "Understanding the processes determining abundance of terrestrial wildlife communities across large scales and estimating the abundance of wildlife across large areas remain a major challenge.  For most species, the factors that regulate their distribution and yearly fluctuations in population size are unknown.  This project takes advantage of the key innovation of using motion sensitive camera traps as a network of sensors for estimating animal abundance. This new approach can produce abundance data for any terrestrial animal >100g, typically ~60% of the terrestrial animals in the eastern USA. Furthermore, the method is amenable to citizen science programs, without the biases or data quality issues typical of other programs, opening the possibility of a sustainable dense sampling effort across large areas. Software will be developed efficiently enter and to manage camera images and associated data. Using standardize field techniques citizen?s groups will sample their local wildlife communities with cameras. The resulting data will be analyzed using new multi-scale statistical models to discover the processes regulating wildlife abundance over large areas. Mapping the local abundance of wildlife populations across broad areas will be key to understanding the mechanisms responsible for changes resulting from land-management decisions and regional climate variation. By involving citizens in data collection this project will be helping local wildlife populations. All data and images will be made freely available online, providing a tool not only for scientists, but also to give the public a new window into the animal communities of their region.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "BIO",
 "org_dir_long_name": "Directorate for Biological Sciences",
 "div_abbr": "EF",
 "org_div_long_name": "Emerging Frontiers",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Zhihai",
   "pi_last_name": "He",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Zhihai He",
   "pi_email_addr": "hezhi@missouri.edu",
   "nsf_id": "000388185",
   "pi_start_date": "2011-06-23",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Joshua",
   "pi_last_name": "Millspaugh",
   "pi_mid_init": "J",
   "pi_sufx_name": "",
   "pi_full_name": "Joshua J Millspaugh",
   "pi_email_addr": "millspaughj@missouri.edu",
   "nsf_id": "000138919",
   "pi_start_date": "2011-06-23",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Missouri-Columbia",
  "inst_street_address": "121 UNIVERSITY HALL",
  "inst_street_address_2": "",
  "inst_city_name": "COLUMBIA",
  "inst_state_code": "MO",
  "inst_state_name": "Missouri",
  "inst_phone_num": "5738827560",
  "inst_zip_code": "652113020",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MO03",
  "org_lgl_bus_name": "UNIVERSITY OF MISSOURI SYSTEM",
  "org_prnt_uei_num": "",
  "org_uei_num": "SZPJL5ZRCLF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of Missouri-Columbia",
  "perf_str_addr": "121 UNIVERSITY HALL",
  "perf_city_name": "COLUMBIA",
  "perf_st_code": "MO",
  "perf_st_name": "Missouri",
  "perf_zip_code": "652113020",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MO03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "795900",
   "pgm_ele_name": "MacroSysBIO & NEON-Enabled Sci"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7959",
   "pgm_ref_txt": "MACROSYSTEM BIOLOGY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 374178.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This NSF grant has allowed us to develop the software and database tools for our eMammal cyber-infrastructure to manage large scale citizen science based surveys of wildlife and use these data to test hypotheses about the determinants of abundance for mammals. eMammal is a biological informatics cyber-infrastructure which brings together citizen scientists and wildlife professionals to collect, analyze, and manage massive camera-trap data. These data allows us to discover the processes regulating wildlife abundance over large areas with sophisticated multi-scale statistical models.</p>\n<p>&nbsp;During the past three years, we have worked with over 300 citizen scientists to deploy camera traps at 2,357 locations in 30 study sites across 6 states generating ~100,000 detections of wildlife and 2.6 million animal image samples, representing more than 130 years of survey work.&nbsp;</p>\n<p>&nbsp;We have developed algorithms to detect animal presence (or, classify camera trap images with and with animals), to segment the animal from the highly cluttered background, to place a bounding box around this animal, and to characterize the identity of this species. During tests on the public available benchmark test datasets, our detection and segmentation algorithm outperforms the state-of-the-arts vision algorithms in the literature. In the past year we have improved the performance of this algorithm so that it could be integrated into the software used by our volunteers. This integration was successful last year, such that the tool is now used by citizen scientists to help them find the animal in the image. These tools and data analysis protocols have been implemented over 2 years producing 52,863 detections of native wildlife in 42,874 camera nights of survey effort across 1953 locations camera sites in 32 parks. These data are now all cleanly annotated with environmental and human data, and combined in a database, which can be queried to enable multiple different statistical analyses.</p>\n<p>&nbsp;We have developed a quantitative modeling framework for the analysis of data returned from the camera trap systems. This framework considers the detection rate and occupancy evaluation for animals as a measure of relative abundance that can be compared across sites (e.g., hunted versus non-hunted, on trail versus off trail). These models can accommodate both categorical and continuous covariates, auto-correlated counts, and nuisance parameters such as imperfect detection rates and year effects. For instance, we will explicitly model week and year effects, to account for temporal variation in relative abundance, and spatial location, which incorporates spatial autocorrelation among camera traps into the models' covariance parameters. &nbsp; &nbsp;&nbsp;</p>\n<p>&nbsp;We have integrated all tools into the Smithsonian cyberinfrastructure. The Smithsonian Data Repository has finished building the workflow that automatically downloads data approved by scientists via the eMammal Expert Review Tool and loads that data into the Smithsonian database. The Smithsonian team has also created an application programming interface (API) so that visitors to the eMammal website may search the data in the database and run pre-set analyses on camera trapping data.</p>\n<p>&nbsp;The success of eMammal as a macro-scale wildlife-monitoring tool hinges on our ability to recruit large numbers of volunteers to run cameras over large areas.&nbsp; Our key innovation and contribution of eMammal has been to standardize field protocols, and create a fun and efficient way to classify, share, and store these amazing photographs and discoveries. We had great success in terms of volunteer recruitment and engagement, working with about 500 different volunteers across 6 states. We had high volunteer turnout with relatively little advertisement, 98% of our volunteers completed the program, and all returned the equipment...",
  "por_txt_cntn": "\nThis NSF grant has allowed us to develop the software and database tools for our eMammal cyber-infrastructure to manage large scale citizen science based surveys of wildlife and use these data to test hypotheses about the determinants of abundance for mammals. eMammal is a biological informatics cyber-infrastructure which brings together citizen scientists and wildlife professionals to collect, analyze, and manage massive camera-trap data. These data allows us to discover the processes regulating wildlife abundance over large areas with sophisticated multi-scale statistical models.\n\n During the past three years, we have worked with over 300 citizen scientists to deploy camera traps at 2,357 locations in 30 study sites across 6 states generating ~100,000 detections of wildlife and 2.6 million animal image samples, representing more than 130 years of survey work. \n\n We have developed algorithms to detect animal presence (or, classify camera trap images with and with animals), to segment the animal from the highly cluttered background, to place a bounding box around this animal, and to characterize the identity of this species. During tests on the public available benchmark test datasets, our detection and segmentation algorithm outperforms the state-of-the-arts vision algorithms in the literature. In the past year we have improved the performance of this algorithm so that it could be integrated into the software used by our volunteers. This integration was successful last year, such that the tool is now used by citizen scientists to help them find the animal in the image. These tools and data analysis protocols have been implemented over 2 years producing 52,863 detections of native wildlife in 42,874 camera nights of survey effort across 1953 locations camera sites in 32 parks. These data are now all cleanly annotated with environmental and human data, and combined in a database, which can be queried to enable multiple different statistical analyses.\n\n We have developed a quantitative modeling framework for the analysis of data returned from the camera trap systems. This framework considers the detection rate and occupancy evaluation for animals as a measure of relative abundance that can be compared across sites (e.g., hunted versus non-hunted, on trail versus off trail). These models can accommodate both categorical and continuous covariates, auto-correlated counts, and nuisance parameters such as imperfect detection rates and year effects. For instance, we will explicitly model week and year effects, to account for temporal variation in relative abundance, and spatial location, which incorporates spatial autocorrelation among camera traps into the models' covariance parameters.     \n\n We have integrated all tools into the Smithsonian cyberinfrastructure. The Smithsonian Data Repository has finished building the workflow that automatically downloads data approved by scientists via the eMammal Expert Review Tool and loads that data into the Smithsonian database. The Smithsonian team has also created an application programming interface (API) so that visitors to the eMammal website may search the data in the database and run pre-set analyses on camera trapping data.\n\n The success of eMammal as a macro-scale wildlife-monitoring tool hinges on our ability to recruit large numbers of volunteers to run cameras over large areas.  Our key innovation and contribution of eMammal has been to standardize field protocols, and create a fun and efficient way to classify, share, and store these amazing photographs and discoveries. We had great success in terms of volunteer recruitment and engagement, working with about 500 different volunteers across 6 states. We had high volunteer turnout with relatively little advertisement, 98% of our volunteers completed the program, and all returned the equipment.  The data was high quality, with only 2% of deployments being rejected due to human error, and 97% accuracy in species identification, which..."
 }
}