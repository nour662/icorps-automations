{
 "awd_id": "1053868",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: Pixel-Based Interpretation and Modification of Graphical User Interfaces",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-03-01",
 "awd_exp_date": "2017-02-28",
 "tot_intn_awd_amt": 565060.0,
 "awd_amount": 589060.0,
 "awd_min_amd_letter_date": "2010-12-06",
 "awd_max_amd_letter_date": "2015-04-23",
 "awd_abstract_narration": "Nearly every graphical user interface (GUI) is implemented using some form of GUI toolkit.  While these toolkits have enabled many successes of the past forty years of research and practice in human-computer interaction (HCI), they have unfortunately also become stifling.  The rigidity and fragmentation of current GUI toolkits makes it difficult or impossible to deploy new ideas in complex existing applications, so researchers are instead often limited to demonstrating and evaluating small toy applications.  This is a major challenge for the field, as it limits both the progress and impact of HCI research.  The PI's goal is to transcend the fragmentation of modern GUIs by building upon their single largest commonality, namely that all GUIs ultimately consist of pixels painted to a display.  He argues that if it were possible to quickly and robustly interpret those pixels, without application source code and independent of its underlying toolkit implementation, we could use that understanding to modify any interface.  The PI's prior work on the Prefab system has shown promising preliminary results, enabled by a strategy of reverse engineering widget appearance.  In the current project the PI will develop methods for pixel-based interpretation of graphical user interfaces, including methods for identifying complex widgets, for extracting widget content, and for recovering widget relationships, and he will build upon these to model widgets state and to address occlusion of portions of interfaces.  He will create methods for modifying graphical user interfaces, including semantic views on pixel-based interpretation, methods for manipulating underlying interfaces, methods for translation between presentations, visual programming methods, and end-user tools for interface customization.  He will implement new tools for informing, deploying, and evaluating interaction research in the field, and he will partner with leading researchers to apply these tools to target-aware pointing and automatic interface generation.  And he will validate his pixel-based methods in real-world datasets collected from broad deployments.\r\n\r\nBroader Impacts:  In this project the PI will pursue a vision of a transformative democratization of every aspect of interaction, in which instead of being limited to a single provided interface anybody can modify any interface of any application.  For example, an HCI researcher might implement and evaluate a new interaction technique in several existing applications; a practitioner or hobbyist who sees the publication might add the technique to their favorite applications.  Web communities might organize around causes, such as translating interfaces into new languages, improving the accessibility of existing applications, or updating interfaces to support gestures, speech access, or other interactions.  End-users might browse libraries of extensions, vote on their favorites, and use visual programming tools to create their own.  Project outcomes, including tools and dataset, will be openly disseminated.  The PI will make special efforts to recruit students from under-represented groups, to integrate the research into existing and new courses, and to use a high school design competition as a channel for recruiting high school students for internships.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "James",
   "pi_last_name": "Fogarty",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "James Fogarty",
   "pi_email_addr": "jfogarty@cs.washington.edu",
   "nsf_id": "000238935",
   "pi_start_date": "2010-12-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Washington",
  "inst_street_address": "4333 BROOKLYN AVE NE",
  "inst_street_address_2": "",
  "inst_city_name": "SEATTLE",
  "inst_state_code": "WA",
  "inst_state_name": "Washington",
  "inst_phone_num": "2065434043",
  "inst_zip_code": "981951016",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "WA07",
  "org_lgl_bus_name": "UNIVERSITY OF WASHINGTON",
  "org_prnt_uei_num": "",
  "org_uei_num": "HD1WMN6945W6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Washington",
  "perf_str_addr": "4333 BROOKLYN AVE NE",
  "perf_city_name": "SEATTLE",
  "perf_st_code": "WA",
  "perf_st_name": "Washington",
  "perf_zip_code": "981951016",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "WA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 114198.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 121542.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 111324.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 117593.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 124403.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">The overall focus of the project is on overcoming the rigidity and fragmentation of current graphical user interface toolkits, which combine to make it difficult or impossible to deploy new ideas in complex existing applications. This is a major challenge for the field, as it limits both the progress and impact of interaction research. We envision a future in which anybody can modify any interface of any application, and are pursuing this future by building upon pixels as a universal representation. At a high level, we aim to develop new methods for pixel-based interpretation and modification of graphical user interfaces, together with new tools for informing, deploying, and evaluating interaction research.</span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Our prior work in this award developed powerful methods for pixel-based identification and interpretation of interface elements. This is partially based in using examples of interface elements to learn prototypes that generalize to allow future pixel-based recognition of those interface elements. It is also based in methods for capturing human annotation of interfaces, as some information is simply not obtainable from the pixels of an existing interface. This ambiguity is pervasive in pixel-based methods. Our prior work in Prefab Layers and Prefab Annotations thus provides a framework for relating code and human annotation in pixel-based identification and interpretation.</span></span></p>\n<p>&nbsp;</p>\n<p><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Our research continues to advance the possibilities for runtime modification of interfaces. This offers great potential as a catalyst for human-computer interaction research and for helping facilitate broader impact and adoption of interaction research. Our focus on mobile application accessibility also offers new opportunities for impact in directly addressing accessibility challenges in mobile applications.&nbsp; <span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">This award is supporting the training of students at the leading edge of user interface technology. Such training is extremely rare and valuable, and the students trained under this award will continue to have significant impact throughout their future careers.&nbsp; <span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\">Public availability of the techniques developed in this research provides the beginning of an information infrastructure to enable future research and broader impact.&nbsp; These techniques can be found at https://github.com/prefab.</span></span></span></span></span></p>\n<p><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><span style=\"color: #000000; font-family: arial, helvetica, sans-serif; font-size: 14px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: left; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; text-decoration-style: initial; text-decoration-color: initial; display: inline !important; float: none;\"><br /></span></span></span></span></span></p><br>\n<p>\n\t\t\t\t      \tLast Modified: 05/31/2017<br>\n\t\t\t\t\tModified by: James&nbsp;Fogarty</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe overall focus of the project is on overcoming the rigidity and fragmentation of current graphical user interface toolkits, which combine to make it difficult or impossible to deploy new ideas in complex existing applications. This is a major challenge for the field, as it limits both the progress and impact of interaction research. We envision a future in which anybody can modify any interface of any application, and are pursuing this future by building upon pixels as a universal representation. At a high level, we aim to develop new methods for pixel-based interpretation and modification of graphical user interfaces, together with new tools for informing, deploying, and evaluating interaction research.\n\n \n\nOur prior work in this award developed powerful methods for pixel-based identification and interpretation of interface elements. This is partially based in using examples of interface elements to learn prototypes that generalize to allow future pixel-based recognition of those interface elements. It is also based in methods for capturing human annotation of interfaces, as some information is simply not obtainable from the pixels of an existing interface. This ambiguity is pervasive in pixel-based methods. Our prior work in Prefab Layers and Prefab Annotations thus provides a framework for relating code and human annotation in pixel-based identification and interpretation.\n\n \n\nOur research continues to advance the possibilities for runtime modification of interfaces. This offers great potential as a catalyst for human-computer interaction research and for helping facilitate broader impact and adoption of interaction research. Our focus on mobile application accessibility also offers new opportunities for impact in directly addressing accessibility challenges in mobile applications.  This award is supporting the training of students at the leading edge of user interface technology. Such training is extremely rare and valuable, and the students trained under this award will continue to have significant impact throughout their future careers.  Public availability of the techniques developed in this research provides the beginning of an information infrastructure to enable future research and broader impact.  These techniques can be found at https://github.com/prefab.\n\n\n\n\n\t\t\t\t\tLast Modified: 05/31/2017\n\n\t\t\t\t\tSubmitted by: James Fogarty"
 }
}