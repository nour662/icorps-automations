{
 "awd_id": "1065125",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "AF: Medium: Taming Masssive Data with Sub-Linear Algorithms",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rahul Shah",
 "awd_eff_date": "2011-03-01",
 "awd_exp_date": "2016-02-29",
 "tot_intn_awd_amt": 1160930.0,
 "awd_amount": 1160930.0,
 "awd_min_amd_letter_date": "2011-02-25",
 "awd_max_amd_letter_date": "2011-02-25",
 "awd_abstract_narration": "The rampant growth of massive data sets presents new challenges for data processing and analysis. To cope with this phenomenon, sublinear time and sublinear space algorithms that are capable of analyzing and extracting value from such immense inputs must be developed. This project aims to study sublinear time and space algorithms from a unified perspective, using the synergies in order to gain a better understanding that will lead to faster, space efficient and more widely applicable algorithms.\r\n\r\nThe proposed research has two core components.  The first component is the study of sparse representations of large data sets.  Sparse representations are useful for quickly analyzing and processing data.  This component itself will have two parts. First, it will lead to a better understanding of sublinear time sampling algorithms for  data  coming from a succinctly described distribution.  The succinct descriptions considered include those defined by a small number of parameters, such as power laws, Gaussians, and histogram distributions. Second, it will improve the current state of knowledge of streaming algorithms, data sketching, compressive sensing and sparse recovery techniques.  Such techniques will for example have an impact on algorithms for acquiring and processing images, audio and network data. \r\n\r\nThe second component aims to design novel statistical techniques for understanding various distributional quantities describing commonly used structured objects such as graphs.  The focus in this component will be on sublinear time and space algorithms that estimate parameters of graphs.  This project will  significantly advance the algorithmic foundations of algorithms with limited resources, and develop highly efficient algorithms for analyzing massive data sets.\r\n\r\nThis project will significantly advance the algorithmic foundations of computation with limited resources.  It will develop highly efficient algorithms for analyzing massive data sets that arise in diverse areas including electronic commerce, health, network security, and scientific data collection.\r\n\r\nThe broader impacts of this project are in the education and mentoring of young researchers including underrepresented groups.  Community outreach activities will introduce primary school children to interesting mathematical and computer science ideas.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ronitt",
   "pi_last_name": "Rubinfeld",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ronitt Rubinfeld",
   "pi_email_addr": "ronitt@csail.mit.edu",
   "nsf_id": "000322655",
   "pi_start_date": "2011-02-25",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Piotr",
   "pi_last_name": "Indyk",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Piotr Indyk",
   "pi_email_addr": "indyk@mit.edu",
   "nsf_id": "000488958",
   "pi_start_date": "2011-02-25",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Massachusetts Institute of Technology",
  "inst_street_address": "77 MASSACHUSETTS AVE",
  "inst_street_address_2": "",
  "inst_city_name": "CAMBRIDGE",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "6172531000",
  "inst_zip_code": "021394301",
  "inst_country_name": "United States",
  "cong_dist_code": "07",
  "st_cong_dist_code": "MA07",
  "org_lgl_bus_name": "MASSACHUSETTS INSTITUTE OF TECHNOLOGY",
  "org_prnt_uei_num": "E2NYLCDML6V1",
  "org_uei_num": "E2NYLCDML6V1"
 },
 "perf_inst": {
  "perf_inst_name": "Massachusetts Institute of Technology",
  "perf_str_addr": "77 MASSACHUSETTS AVE",
  "perf_city_name": "CAMBRIDGE",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "021394301",
  "perf_ctry_code": "US",
  "perf_cong_dist": "07",
  "perf_st_cong_dist": "MA07",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "9218",
   "pgm_ref_txt": "BASIC RESEARCH & HUMAN RESORCS"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 1160930.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The rampant growth of massive data sets presents new challenges for data processing and analysis. To cope with this phenomenon, sublinear time and sublinear space algorithms that are capable of analyzing and extracting value from such immense inputs must be developed.&nbsp; This project studies sublinear time and space algorithms from a unified perspective, using the synergies in order to gain a better understanding that will lead to faster, space efficient and more<br />widely applicable algorithms.<br /><br />The research has two core components. The first component is the study of sparse representations of large data sets. Sparse representations are useful for quickly analyzing and processing data. <br />Results on this part include: Algorithms for using sparsity of data to compress and represent large data sets were developed. Improved sparse recovery algorithms for inputs that have a structured sparsity,<br />namely described as&nbsp; tree and block sparse, have been given.&nbsp;&nbsp; Optimal sparse Fourier Transform algorithms have been developed.<br />The best algorithms for the sparse recovery&nbsp; of matrices under the<br />condition of restricted isometry property are given.<br /><br />The second part of this research is to achieve a better understanding of sublinear time sampling algorithms for very large data. Algorithms that run in time sublinear in the size of the data were discovered:<br />For example, approximation algorithms for the minimum vertex cover size (a classical combinatorial optimization problem on graphs) were found that run in time approaching the average degree of the graph, which is significantly smaller than the description of the entire graph.<br /><br />A new model of local computation was introduced.&nbsp;&nbsp; This model allows one to locally compute just the part of the result that one needs, rather than the whole input/output relationship. For example, if a resident is being matched to a medical residency program, the<br />resident need only compute which hospital the resident has been matched to --&nbsp; not the entire matching of all residents.&nbsp;&nbsp;&nbsp; Within this model, the first results on finding local algorithms for basic problems such as maximal independent set, compression of data,&nbsp; sparse spanning graphs, and resource allocation are given.<br /><br />A large thrust of this project is in learning to understand data coming from samples of a distribution over a very large domain.&nbsp;&nbsp; Such algorithms attempt to understand whether the distributions have certain minimal properties, some examples include being describable as a k-histogram distribution (commonly used in Database<br />applications),&nbsp; monotone distribution, piecewise-polynomial. Surprisingly, one can understand whether the distribution has these properties faster than one can learn the distribution.&nbsp;&nbsp;&nbsp; In this project, the first algorithms for learning and testing k-histogram distributions were given,&nbsp;&nbsp;&nbsp; a framework for using structure to test classes of discrete distributions was introduced which gives near-optimal sample complexity for testing many well studied classes of&nbsp; distributions, including monotone, log-concave, t-modal, piecewise-polynomial, <br />and Poisson Binomial distributions.&nbsp; <br /><br />A new approach to dealing with noisy sample data which introduces ``sampling correctors'' to address corruptions to samples of an unknown distribution. Sampling correctors, acting as filters between the noisy data and the end user, use structure that the distribution is purported to have, in order to allow one to&nbsp; make \"on-the-fly\" corrections to samples drawn from probability distributions.&nbsp; Surprisingly, this work shows that there are settings in which sampling correctors are&nbsp; significantly faster than distribution learning algorithms.<br /><br />Several results are given w...",
  "por_txt_cntn": "\nThe rampant growth of massive data sets presents new challenges for data processing and analysis. To cope with this phenomenon, sublinear time and sublinear space algorithms that are capable of analyzing and extracting value from such immense inputs must be developed.  This project studies sublinear time and space algorithms from a unified perspective, using the synergies in order to gain a better understanding that will lead to faster, space efficient and more\nwidely applicable algorithms.\n\nThe research has two core components. The first component is the study of sparse representations of large data sets. Sparse representations are useful for quickly analyzing and processing data. \nResults on this part include: Algorithms for using sparsity of data to compress and represent large data sets were developed. Improved sparse recovery algorithms for inputs that have a structured sparsity,\nnamely described as  tree and block sparse, have been given.   Optimal sparse Fourier Transform algorithms have been developed.\nThe best algorithms for the sparse recovery  of matrices under the\ncondition of restricted isometry property are given.\n\nThe second part of this research is to achieve a better understanding of sublinear time sampling algorithms for very large data. Algorithms that run in time sublinear in the size of the data were discovered:\nFor example, approximation algorithms for the minimum vertex cover size (a classical combinatorial optimization problem on graphs) were found that run in time approaching the average degree of the graph, which is significantly smaller than the description of the entire graph.\n\nA new model of local computation was introduced.   This model allows one to locally compute just the part of the result that one needs, rather than the whole input/output relationship. For example, if a resident is being matched to a medical residency program, the\nresident need only compute which hospital the resident has been matched to --  not the entire matching of all residents.    Within this model, the first results on finding local algorithms for basic problems such as maximal independent set, compression of data,  sparse spanning graphs, and resource allocation are given.\n\nA large thrust of this project is in learning to understand data coming from samples of a distribution over a very large domain.   Such algorithms attempt to understand whether the distributions have certain minimal properties, some examples include being describable as a k-histogram distribution (commonly used in Database\napplications),  monotone distribution, piecewise-polynomial. Surprisingly, one can understand whether the distribution has these properties faster than one can learn the distribution.    In this project, the first algorithms for learning and testing k-histogram distributions were given,    a framework for using structure to test classes of discrete distributions was introduced which gives near-optimal sample complexity for testing many well studied classes of  distributions, including monotone, log-concave, t-modal, piecewise-polynomial, \nand Poisson Binomial distributions.  \n\nA new approach to dealing with noisy sample data which introduces ``sampling correctors'' to address corruptions to samples of an unknown distribution. Sampling correctors, acting as filters between the noisy data and the end user, use structure that the distribution is purported to have, in order to allow one to  make \"on-the-fly\" corrections to samples drawn from probability distributions.  Surprisingly, this work shows that there are settings in which sampling correctors are  significantly faster than distribution learning algorithms.\n\nSeveral results are given which achieve the best bounds on the rates of  codes for which one can locally correct  any coordinate or locally decode any coordinate in sublinear time.\n\n\n\n\t\t\t\t\tLast Modified: 05/10/2016\n\n\t\t\t\t\tSubmitted by: Ronitt Rubinfeld"
 }
}