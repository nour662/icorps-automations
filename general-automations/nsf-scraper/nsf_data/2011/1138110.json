{
 "awd_id": "1138110",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RAPID: Aerial Robots for Remote Autonomous Exploration and Mapping",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Thyagarajan Nandagopal",
 "awd_eff_date": "2011-07-01",
 "awd_exp_date": "2014-06-30",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2011-06-30",
 "awd_max_amd_letter_date": "2011-06-30",
 "awd_abstract_narration": "Proposal #:\tCNS 11-38110\r\nPI(s):\t\tKumar, R. Vijay; Michael, Nathan D\r\nInstitution:\tUniversity of Pennsylvania\r\nTitle: \tRAPID: Aerial Robots for Rapid Response: Remote Autonomous Exploration and Mapping\r\nProject Proposed:\r\nThis RAPID project, developing and deploying a team of autonomous aerial robots that can enter an unstructured, hazardous environment to explore and map a facility, provides information to human operators in safe, remote locations. The work brings together research groups with complementary expertise in robotics to address the challenging problem of acquiring imagery and three-dimensional maps for post-disaster assessment. Autonomous robots will be deployed without a direct communication link enabling access to areas in the Fukushima that are currently inaccessible.\r\nAddressing an urgent need, the work consists of redesigning aerial robotic systems to perform mapping, localization, and exploration functions in indoor and outdoor environments without prior knowledge of the environment or GPS (Global Positioning System). The system, to be deployed in highly contaminated environments such as the area of Fukushima disasters in Japan, expands the present robotic systems developed by the team. It should be able to build maps and localize, plan, and control autonomously in that map, but requires interactions with a base-station to communicate the relevant information. The paradigm shifting capabilities of aerial robots to act independently and be deployed in critical contaminated areas exhibit novelty. The Japanese-American academic research team will be engaged in some of the following activities: \r\n-\tDevelop methods to acquire information from highly contaminated environments, such as in case of radiation contamination. \r\n-\tEngineer and deploy one or more autonomous robots (i.e., without the link to the base station) equipped with cameras and laser range finders as well as potentially carrying sensors that might reveal new insights about the degree of contamination. \r\n-\tDevelop algorithms and methods for information gathering and map building. \r\n-\tDeploy the robots in Japan through Japanese colleagues.\r\nMost UAVs (Unmanned Aerial Vehicles) are teleoperated with several human operators engaged in the deployment of each UAV. The cross fertilization of technologies for robotics and UAVs has potential to create new small to medium scale autonomous UAVs with a wide range of civilian and defense applications. This project will explore the use of autonomous UAVs for acquiring information from environments that are impossible to access because of radiation contamination. One or more autonomous quad rotor robots equipped with cameras and laser range finders will be deployed to explore the partially-known environment and build 3-D maps of the structure and potentially carrying sensors that might reveal new insights about the degree of contamination. These robots will have to operate without any communication link to the base station. Thus this will represent the first deployment of a truly autonomous robot of its kind.\r\nThis work involves a collaboration with Dr. Satoshi Tadokoro, a researcher in search and rescue robotics, from Tohuku University in Sendai, Japan. A support letter has been submitted by Dr. Tadokoro for the proposed joint research. Another collaborator from the same University, Dr. Kazuya Yoshida, leads the project entitled ?Robotics in Extreme Environment? and brings the ?Extreme Robotics? background to this collaborative research endeavor. The project, expected to lose robots in extreme environment tests, consequently requires building additional autonomous aerial robots for the purpose of the experiments. Funding is also requested to travel to Japan for collaborative research and experimentation.  \r\nBroader Impacts: \r\nThe tragic sequence of events in Sendai and the Fukushima I and II nuclear power plants has resulted in significant contamination due to radioactive iodine, cesium and stronium, making it nearly impossible for humans to enter many areas in the power plants to assess damage. First, the use of robots to acquire information from currently inaccessible areas will have a significant impact on post-disaster recovery operations. Second, the use of autonomous aerial robots will establish an important milestone in robotics and will allow the nuclear power industry to be better positioned to rapidly respond to disasters in the future. This proposal promises an immediate benefit to society by supporting economic recovery efforts in Japan through a participatory research paradigm. Moreover, long term benefits for future disasters are in evidence since emergency response and unmanned systems are both formative domains and the data collected will advance the discovery and understanding of intelligent, human-centered systems in unpredictable situations. Furthermore, the use of autonomous aerial robots will establish an important milestone in robotics allowing the nuclear power industry to be better positioned to rapidly respond to disasters in the future. Finally, the project will train undergraduate and graduate students and expose them to high-impact application areas.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "R. Vijay",
   "pi_last_name": "Kumar",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "R. Vijay Kumar",
   "pi_email_addr": "Kumar@seas.upenn.edu",
   "nsf_id": "000280506",
   "pi_start_date": "2011-06-30",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Nathan",
   "pi_last_name": "Michael",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Nathan D Michael",
   "pi_email_addr": "nmichael@cmu.edu",
   "nsf_id": "000575974",
   "pi_start_date": "2011-06-30",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Pennsylvania",
  "inst_street_address": "3451 WALNUT ST STE 440A",
  "inst_street_address_2": "",
  "inst_city_name": "PHILADELPHIA",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "2158987293",
  "inst_zip_code": "191046205",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "PA03",
  "org_lgl_bus_name": "TRUSTEES OF THE UNIVERSITY OF PENNSYLVANIA, THE",
  "org_prnt_uei_num": "GM1XX56LEP58",
  "org_uei_num": "GM1XX56LEP58"
 },
 "perf_inst": {
  "perf_inst_name": "University of Pennsylvania",
  "perf_str_addr": "3330 Walnut Street",
  "perf_city_name": "Philadelphia",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "191046389",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "PA03",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "164000",
   "pgm_ele_name": "Information Technology Researc"
  },
  {
   "pgm_ele_code": "729800",
   "pgm_ele_name": "International Research Collab"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5921",
   "pgm_ref_txt": "JAPAN"
  },
  {
   "pgm_ref_code": "5978",
   "pgm_ref_txt": "EAST ASIA AND PACIFIC PROGRAM"
  },
  {
   "pgm_ref_code": "7914",
   "pgm_ref_txt": "RAPID"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project explored the possibility of leveraging an autonomous quadrotor in earthquake-damaged environments through field experiments that focus on cooperative mapping using both ground and aerial robots. Aerial robots offer several advantages over ground robots, including the ability to maneuver through complex three-dimensional (3D) environments and gather data from vantages inaccessible to ground robots.</p>\n<p>We considered an earthquake-damaged building with multiple floors that were generally accessible to ground robots. However, various locations in the environment were inaccessible to the ground robots due to debris or clutter. The goal was the generation of 3D maps that capture the layout of the environment and provide insight into the degree of damage inside the building.</p>\n<p>In collaboration with researchers at the Tohuku University at Japan, we designed a field experiment that highlighted the need for heterogeneity. Ground robots do not have the same payload limitations as quadrotors, and they are therefore able to carry larger sensor payloads, maintain tethered communication links, and operate for longer periods of time. However, quadrotors provide mobility and observational capabilities unavailable to ground robots. Hence, to build a rich 3D representation of the environment, we leveraged the advantages of each platform, and in doing so we mitigated the platform limitations.</p>\n<p>During the experiment, we used three different research platforms (Fig. 1). The first platform was a ground robot equipped with an onboard sensing suite that enabled the generation of dense 3D maps. The vehicle was tele-operated through the multi-floor environment while simultaneously collecting sensor data. After the operators identified locations in the environment that were inaccessible to the ground platform, a second ground platform equipped with an automated helipad was tele-operated to these locations and carried a quadrotor robot equipped with onboard sensing that was able to remotely open and close the helipad and autonomously take off and land from the helipad.</p>\n<p>The aerial robot was equipped with a laser scanner and onboard computing for online mapping. It was physically transported by the ground robot to each location of interest, where it autonomously took off before an operator was able to guide the robot to map or observe these inaccessible regions. Upon completion of the mapping and observation phase, the aerial robot was remotely signaled to autonomously land and close the helipad. The quad-rotor was then guided to the next location of interest via the tele-operated ground robot.</p>\n<p>On-site, we realized that in complex environments like the earthquake-damaged buildings in Sendai, the appearance of the environment drastically changed from its original structure. Our earlier aerial navigation approach that utilized certain assumptions that are specific to man-made indoor environments would have failed and hence we designed a new quadrotor platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer and GPS receiver (Fig. 2). The motivation was to utilize the information from multiple sensors such that even if a subset of the sensors were to fail, the performance of the overall system would not be seriously compromised.&nbsp;</p>\n<p>We proposed a novel modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals, and to provide smooth and globally consistent estimates of position in real time for autonomous flight. Through large-scale indoor and outdoor autonomous flight experiments, we demonstrated that the fusion of measurements from multiple sensors increases the system robustness.</p>\n<p>In the collaborative multi-floor mapping environment, we successfully deployed our ground and...",
  "por_txt_cntn": "\nThis project explored the possibility of leveraging an autonomous quadrotor in earthquake-damaged environments through field experiments that focus on cooperative mapping using both ground and aerial robots. Aerial robots offer several advantages over ground robots, including the ability to maneuver through complex three-dimensional (3D) environments and gather data from vantages inaccessible to ground robots.\n\nWe considered an earthquake-damaged building with multiple floors that were generally accessible to ground robots. However, various locations in the environment were inaccessible to the ground robots due to debris or clutter. The goal was the generation of 3D maps that capture the layout of the environment and provide insight into the degree of damage inside the building.\n\nIn collaboration with researchers at the Tohuku University at Japan, we designed a field experiment that highlighted the need for heterogeneity. Ground robots do not have the same payload limitations as quadrotors, and they are therefore able to carry larger sensor payloads, maintain tethered communication links, and operate for longer periods of time. However, quadrotors provide mobility and observational capabilities unavailable to ground robots. Hence, to build a rich 3D representation of the environment, we leveraged the advantages of each platform, and in doing so we mitigated the platform limitations.\n\nDuring the experiment, we used three different research platforms (Fig. 1). The first platform was a ground robot equipped with an onboard sensing suite that enabled the generation of dense 3D maps. The vehicle was tele-operated through the multi-floor environment while simultaneously collecting sensor data. After the operators identified locations in the environment that were inaccessible to the ground platform, a second ground platform equipped with an automated helipad was tele-operated to these locations and carried a quadrotor robot equipped with onboard sensing that was able to remotely open and close the helipad and autonomously take off and land from the helipad.\n\nThe aerial robot was equipped with a laser scanner and onboard computing for online mapping. It was physically transported by the ground robot to each location of interest, where it autonomously took off before an operator was able to guide the robot to map or observe these inaccessible regions. Upon completion of the mapping and observation phase, the aerial robot was remotely signaled to autonomously land and close the helipad. The quad-rotor was then guided to the next location of interest via the tele-operated ground robot.\n\nOn-site, we realized that in complex environments like the earthquake-damaged buildings in Sendai, the appearance of the environment drastically changed from its original structure. Our earlier aerial navigation approach that utilized certain assumptions that are specific to man-made indoor environments would have failed and hence we designed a new quadrotor platform equipped with an IMU, laser scanner, stereo cameras, pressure altimeter, magnetometer and GPS receiver (Fig. 2). The motivation was to utilize the information from multiple sensors such that even if a subset of the sensors were to fail, the performance of the overall system would not be seriously compromised. \n\nWe proposed a novel modular and extensible approach to integrate noisy measurements from multiple heterogeneous sensors that yield either absolute or relative observations at different and varying time intervals, and to provide smooth and globally consistent estimates of position in real time for autonomous flight. Through large-scale indoor and outdoor autonomous flight experiments, we demonstrated that the fusion of measurements from multiple sensors increases the system robustness.\n\nIn the collaborative multi-floor mapping environment, we successfully deployed our ground and aerial robot platforms. We generated maps from both aerial and ground vehicles. Post processing was performe..."
 }
}