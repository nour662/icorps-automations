{
 "awd_id": "1065622",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CIF: Medium: Collaborative Research: Information Theory and Statistical Inference from Large-Alphabet Data",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "John Cozzens",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 418565.0,
 "awd_amount": 418565.0,
 "awd_min_amd_letter_date": "2011-08-08",
 "awd_max_amd_letter_date": "2011-08-08",
 "awd_abstract_narration": "Statistical analysis is key to many challenging applications such as text classification, speech recognition, and DNA analysis. However, often the amount of data available is comparable or even smaller than the set of symbols (alphabet) constituting the data. Unfortunately, not much is known about optimal inference in this so-called large-alphabet domain. Recently, several promising approaches have been developed by different scientific communities, including Bayesian nonparametrics in statistics and machine learning, universal compression in information theory, and the theory of graph limits in mathematics and computer science.\r\n\r\nThe investigators study the problem drawing from these multiple perspectives, but with a particular focus on developing the information theoretic approach. The research studies analytical properties of the \"pattern maximum likelihood'' estimator, which performs well in practice but is not understood theoretically, and also explores computational speedups. Moreover, it attempts to delineate which problem classes are better handled by Bayesian nonparametric techniques and which by the pattern approach, and explores links between these approaches. The investigators use the resulting theory for automatic document classification, allowing for more automation in storing, retrieving, and analyzing data. Furthermore, the investigators use the theory to study genetic variations, whose link with disease diagnosis is a crucial step in the systematic quantification of biology that is playing an increasingly important role in medical advancement. The research also brings new courses to the classroom, with a special outreach effort to involve women and under-represented minorities, including through the Native Hawaiian Science and Engineering Mentorship Program.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alon",
   "pi_last_name": "Orlitsky",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alon Orlitsky",
   "pi_email_addr": "alon@ucsd.edu",
   "nsf_id": "000445517",
   "pi_start_date": "2011-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 418565.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Distribution estimation is an fundamental problem underlying most statistical and machine learning problems. Traditionally, it was studied in the regime where the support size is far smaller number of samples available. For example estimating the bias of a two-sided coin based on hundreds of flips.&nbsp;</p>\n<p><br />Yet many modern applications fall in a vastly different regime, where the support size is comparable or even larger than the number of samples available. For example, natural-language processing often requires estimating the distribution of words following a given context, such as the two previous words. The number of English words is roughly a million, much larger than the number of times a given two-word context appears, even in a large corpus. Similarly, genomic applications typically involve distributions over thousands or millions of nucleotides or bacteria, again often larger than the number of samples, such as patients, available.</p>\n<p><br />Under this grant, we set out to explore new learning methodologies and algorithms for this \"large alphabet\" regime. We considered several basic problems that form the foundation of many applications. They include the estimating: the underlying distribution, properties of the underlying distribution such as entropy or support size, the likelihood that a sample was generated by one of two distributions, and the likelihood that two samples were generated by the same distribution or two different ones. For each of these problems we derived new results that are often nearly optimal, and in some cases we formulated new problems that facilitated a practical solution.&nbsp;</p>\n<p><br />For distribution estimation we considered the L1 distance, an important measure that determines the accuracy of classification and hypothesis testing algorithms. &nbsp;We determined to the first order the rate at which distributions can be estimated in L1 distance. Specifically, we showed that as the number n of samples grows, distributions over k elements can be estimated to L1 distance of roughly square root of (2k over pi n).&nbsp;</p>\n<p><br />Perhaps not surprisingly, the estimation error increases with the alphabet size k. We therefore formulated a new, competitive, definition for evaluating an estimator's accuracy. Intuitively, it is the difference between the accuracy of the estimator and that of an optimal \"genie\" estimator that has some prior knowledge of the underlying distribution. If the difference is small, then the estimator performs nearly as well as a genie-designed estimator, and hence nearly as well as any human-designed estimator. We derived an estimator who accuracy differs from the genie estimator by at most the inverse of the square root of the number of samples, regardless of the alphabet size. Therefore our estimator is near-optimal for any support size. Experimental results validated the superior performance of this estimator over existing methods. &nbsp;One curious fact about the estimator is that it is similar to one used by practitioners yet fundamentally different from an estimator used by theorists.&nbsp;</p>\n<p><br />Following are some of our additional contributions, focusing on a single result per problem. For classification we derived the first discreet-distribution classifier whose error probability relative to that of an optimal natural classifier decreases with the number of samples at a rate independent of the alphabet size. For closeness testing we derived algorithms whose sample complexity closely track the performance of optimal natural testers. For outlier detection, we designed a test that requires a number of samples sublinear in the alphabet size.&nbsp;</p>\n<p><br />Our results were published in several information theory and machine learning venues, including the Information Theory Transactions and the ISIT, NIPS, and COLT conferences. In particular, the distribution-estimation paper received the NIPS 2016 outstanding paper award. We also presented several invited plenary talks, short courses, and tutorials, including one at the 2016 Information Theory Symposium.&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 03/26/2017<br>\n\t\t\t\t\tModified by: Alon&nbsp;Orlitsky</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nDistribution estimation is an fundamental problem underlying most statistical and machine learning problems. Traditionally, it was studied in the regime where the support size is far smaller number of samples available. For example estimating the bias of a two-sided coin based on hundreds of flips. \n\n\nYet many modern applications fall in a vastly different regime, where the support size is comparable or even larger than the number of samples available. For example, natural-language processing often requires estimating the distribution of words following a given context, such as the two previous words. The number of English words is roughly a million, much larger than the number of times a given two-word context appears, even in a large corpus. Similarly, genomic applications typically involve distributions over thousands or millions of nucleotides or bacteria, again often larger than the number of samples, such as patients, available.\n\n\nUnder this grant, we set out to explore new learning methodologies and algorithms for this \"large alphabet\" regime. We considered several basic problems that form the foundation of many applications. They include the estimating: the underlying distribution, properties of the underlying distribution such as entropy or support size, the likelihood that a sample was generated by one of two distributions, and the likelihood that two samples were generated by the same distribution or two different ones. For each of these problems we derived new results that are often nearly optimal, and in some cases we formulated new problems that facilitated a practical solution. \n\n\nFor distribution estimation we considered the L1 distance, an important measure that determines the accuracy of classification and hypothesis testing algorithms.  We determined to the first order the rate at which distributions can be estimated in L1 distance. Specifically, we showed that as the number n of samples grows, distributions over k elements can be estimated to L1 distance of roughly square root of (2k over pi n). \n\n\nPerhaps not surprisingly, the estimation error increases with the alphabet size k. We therefore formulated a new, competitive, definition for evaluating an estimator's accuracy. Intuitively, it is the difference between the accuracy of the estimator and that of an optimal \"genie\" estimator that has some prior knowledge of the underlying distribution. If the difference is small, then the estimator performs nearly as well as a genie-designed estimator, and hence nearly as well as any human-designed estimator. We derived an estimator who accuracy differs from the genie estimator by at most the inverse of the square root of the number of samples, regardless of the alphabet size. Therefore our estimator is near-optimal for any support size. Experimental results validated the superior performance of this estimator over existing methods.  One curious fact about the estimator is that it is similar to one used by practitioners yet fundamentally different from an estimator used by theorists. \n\n\nFollowing are some of our additional contributions, focusing on a single result per problem. For classification we derived the first discreet-distribution classifier whose error probability relative to that of an optimal natural classifier decreases with the number of samples at a rate independent of the alphabet size. For closeness testing we derived algorithms whose sample complexity closely track the performance of optimal natural testers. For outlier detection, we designed a test that requires a number of samples sublinear in the alphabet size. \n\n\nOur results were published in several information theory and machine learning venues, including the Information Theory Transactions and the ISIT, NIPS, and COLT conferences. In particular, the distribution-estimation paper received the NIPS 2016 outstanding paper award. We also presented several invited plenary talks, short courses, and tutorials, including one at the 2016 Information Theory Symposium. \n\n\t\t\t\t\tLast Modified: 03/26/2017\n\n\t\t\t\t\tSubmitted by: Alon Orlitsky"
 }
}