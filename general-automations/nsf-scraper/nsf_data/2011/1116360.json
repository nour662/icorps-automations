{
 "awd_id": "1116360",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "HCC: Small: Assistive Social Situational Awareness Aids for Individuals with Disabilities",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 499286.0,
 "awd_amount": 515284.0,
 "awd_min_amd_letter_date": "2011-08-31",
 "awd_max_amd_letter_date": "2014-05-01",
 "awd_abstract_narration": "The PI's goal in this project is to enable a quantum leap towards next-generation social assistive aids that enrich the lived social experiences of individuals with visual impairments.  Social interaction is a central component of the human experience.  The ability to communicate effectively with fellow individuals is a fundamental necessity for professional success as well as personal fulfillment.  But nonverbal cues (including prosody, environment attributes, the appearance of communicators and their physical movements) account for a substantial and important part of the information conveyed during social interactions.  As a consequence, the more than 1.3 million individuals in the United States (and 37 million worldwide) who are legally blind have only a limited experience of social interaction.  This \"social disability\" often isolates them from their social environments.  Existing assistive technologies are focused on problems such as navigation, reading text, and access to everyday appliances as well as to computers and the Internet, whereas little or no work has been devoted to real-time accessibility to social and behavioral cues.  Providing real-time access to nonverbal communication cues to visually impaired users poses fundamental challenges in several related fields including affective computing, human communication engineering, behavioral modeling, machine learning, human-machine interaction, multimodal interfaces, usability engineering, multimedia computing, and assistive technology design and development\r\n\r\nAs a first step towards practical and viable social assistive solutions, the PI will focus in this project on the design and development of a social situational awareness assistive prototype for dyadic (one-on-one) interactions, with an emphasis on head/face-based nonverbal cues.  The research will be accomplished through the following specific objectives: (1) Design and development of a dyadic interpersonal mediation interface; (2) Extraction and understanding of nonverbal communication cues; (3) Visuo-haptic sensory substitution for delivering high-bandwidth socio-behavioral data; and (4) Evaluation of the social assistive prototype system in dyadic interaction scenarios representing real-world conditions.  The project draws upon intellectual synergies among the team members, who are experts in human-centered multimedia computing, human-computer interfaces and machine intelligence (Panchanathan, Computer Science); assistive technology design and usability engineering (Hedgpeth, Disability Resources Center); and human communication modeling and socio-behavioral analysis (Ramirez, Human Communication).  The work will build on the team's past successes in developing assistive technologies that have been designed, prototyped, deployed and tested for individuals who are visually impaired.  Project outcomes will be evaluated through the Arizona State University Disability Resource Center and the Arizona Center for the Blind and Visually Impaired.\r\n\r\nBroader Impacts:  This project will pioneer the development of next-generation social assistive aids for individuals with visual impairments and thus will have a significant impact on their lives.   The research to these ends will result in the advancement of computational thinking within and at the confluence of the component disciplines, namely socio-behavioral computing (through the introduction of novel methodologies for computational analysis and evaluation of human communication dynamics in general and social behavior in dyadic interactions, specifically), human-computer interfaces (through the design of novel interfaces that deliver high-bandwidth social data), machine intelligence (through the study of algorithms that elicit various levels of interaction semantics) and assistive technology/usability (through the development and evaluation of social assistive prototypes).  The concepts and technologies developed will also provide pathways to technologies for individuals with other disabilities, such as autism, dementia, and (in the most general sense) a very large portion of society.  The methodologies developed will provide a wealth of data and information that will be made publicly accessible to promote and catalyze further research in the component disciplines.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Sethuraman",
   "pi_last_name": "Panchanathan",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Sethuraman Panchanathan",
   "pi_email_addr": "panch@asu.edu",
   "nsf_id": "000485412",
   "pi_start_date": "2011-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Terri",
   "pi_last_name": "Hedgpeth",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Terri M Hedgpeth",
   "pi_email_addr": "terrih@asu.edu",
   "nsf_id": "000422662",
   "pi_start_date": "2011-08-31",
   "pi_end_date": null
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Artemio",
   "pi_last_name": "Ramirez",
   "pi_mid_init": "",
   "pi_sufx_name": "Jr",
   "pi_full_name": "Artemio Ramirez",
   "pi_email_addr": "artemio.ramirezjr.1@asu.edu",
   "nsf_id": "000584016",
   "pi_start_date": "2011-08-31",
   "pi_end_date": "2012-05-25"
  },
  {
   "pi_role": "Former Co-Principal Investigator",
   "pi_first_name": "Vineeth",
   "pi_last_name": "Nallure Balasubramanian",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vineeth Nallure Balasubramanian",
   "pi_email_addr": "vineeth.nb@asu.edu",
   "nsf_id": "000589314",
   "pi_start_date": "2012-05-25",
   "pi_end_date": "2014-05-01"
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Troy",
   "pi_last_name": "McDaniel",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Troy McDaniel",
   "pi_email_addr": "troy.mcdaniel@asu.edu",
   "nsf_id": "000636379",
   "pi_start_date": "2014-05-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Arizona State University",
  "inst_street_address": "660 S MILL AVENUE STE 204",
  "inst_street_address_2": "",
  "inst_city_name": "TEMPE",
  "inst_state_code": "AZ",
  "inst_state_name": "Arizona",
  "inst_phone_num": "4809655479",
  "inst_zip_code": "852813670",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "AZ04",
  "org_lgl_bus_name": "ARIZONA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NTLHJXM55KZ6"
 },
 "perf_inst": {
  "perf_inst_name": "Arizona State University",
  "perf_str_addr": "660 S MILL AVENUE STE 204",
  "perf_city_name": "TEMPE",
  "perf_st_code": "AZ",
  "perf_st_name": "Arizona",
  "perf_zip_code": "852813670",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "AZ04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 155567.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 343719.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 15998.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The National Science Foundation sponsored Human-Centered Computing project, \"Assistive Social Situational Awareness Aids for Individuals with Disabilities,\" was led by Drs. Sethuraman Panchanathan (Principal Investigator), Terri Hedgpeth (Co-PI) and Troy McDaniel (Co-PI) at the Center for Cognitive Ubiquitous Computing (CUbiC, http://cubic.asu.edu/) at Arizona State University. CUbiC is an inter-disciplinary research center focused on cutting edge research in human-centered multimedia computing within assistive, rehabilitative and healthcare applications.</p>\n<p>Social interactions are an essential component of both personal and professional growth amongst individuals. From one's home to workplace, people require a socially conducive environment, the lack of which can lead to psychological stress and potentially an urge for the individual to disconnect from society. Literature in human communication and psychology states that nearly 65% of communication happens through nonverbal cues including facial expressions, hand gestures and eye gaze. Unfortunately, individuals who are blind cannot access these nonverbal aspects of daily interactions, which can often isolate them from their social environments. To date, no device or technology delivers any form of social assistance with one's everyday interactions. The aim of this project was to pioneer the development of social assistive aids that can provide real-time access to nonverbal communication cues in daily interactions for individuals with visual impairments.</p>\n<p>Toward this aim, we developed the Dyadic Interaction Assistant (DIA) to enrich social interactions between an individual who is visually impaired and his or her sighted counterpart (see figure depicting a conceptual drawing of the DIA). The DIA integrates computer vision and machine intelligence components that process data streamed from a tabletop camera to extract an interaction partner's facial expressions. These cues are then delivered to the user through the Haptic Face Display (HFD). The HFD is a custom high bandwidth visuo-haptic sensory substitution system consisting of a two-dimensional vibrotactile display embedded on the back of a mesh chair.</p>\n<p>This project has resulted in fundamental contributions in the fields of machine intelligence (through the study of algorithms to recognize facial expressions and human emotions including significant scientific contributions related to machine learning frameworks in topic models, domain adaptation, deep learning and active learning); human-computer interaction and haptics (through the study of how to deliver facial expressions and emotions using our sense of touch, which involved the construction of a novel visuo-haptic sensory substitution device and evaluations of visual-to-tactile mappings of nonverbal cues); and socio-behavioral computing and assistive technology (through the design, development and evaluation of a novel social interaction assistant, and how it might enrich and be used in social environments). These contributions have been disseminated through conference proceedings, journals, presentations and outreach activities.</p>\n<p>While the contributions made as part of this project will have significant impact on the lives of individuals with all kinds of visual impairments, the concepts and technologies developed will also provide pathways to social assistive aids for individuals with other sensory, perceptual or cognitive impairments affecting communication, such as autism, prosopagnosia and dementia. Even for the broader population, social assistive aids are applicable in many instances where nonverbal cues are inaccessible, such as when speaking to someone over the phone.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/05/2015<br>\n\t\t\t\t\tModified by: Sethuraman&nbsp;Panchanathan</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=...",
  "por_txt_cntn": "\nThe National Science Foundation sponsored Human-Centered Computing project, \"Assistive Social Situational Awareness Aids for Individuals with Disabilities,\" was led by Drs. Sethuraman Panchanathan (Principal Investigator), Terri Hedgpeth (Co-PI) and Troy McDaniel (Co-PI) at the Center for Cognitive Ubiquitous Computing (CUbiC, http://cubic.asu.edu/) at Arizona State University. CUbiC is an inter-disciplinary research center focused on cutting edge research in human-centered multimedia computing within assistive, rehabilitative and healthcare applications.\n\nSocial interactions are an essential component of both personal and professional growth amongst individuals. From one's home to workplace, people require a socially conducive environment, the lack of which can lead to psychological stress and potentially an urge for the individual to disconnect from society. Literature in human communication and psychology states that nearly 65% of communication happens through nonverbal cues including facial expressions, hand gestures and eye gaze. Unfortunately, individuals who are blind cannot access these nonverbal aspects of daily interactions, which can often isolate them from their social environments. To date, no device or technology delivers any form of social assistance with one's everyday interactions. The aim of this project was to pioneer the development of social assistive aids that can provide real-time access to nonverbal communication cues in daily interactions for individuals with visual impairments.\n\nToward this aim, we developed the Dyadic Interaction Assistant (DIA) to enrich social interactions between an individual who is visually impaired and his or her sighted counterpart (see figure depicting a conceptual drawing of the DIA). The DIA integrates computer vision and machine intelligence components that process data streamed from a tabletop camera to extract an interaction partner's facial expressions. These cues are then delivered to the user through the Haptic Face Display (HFD). The HFD is a custom high bandwidth visuo-haptic sensory substitution system consisting of a two-dimensional vibrotactile display embedded on the back of a mesh chair.\n\nThis project has resulted in fundamental contributions in the fields of machine intelligence (through the study of algorithms to recognize facial expressions and human emotions including significant scientific contributions related to machine learning frameworks in topic models, domain adaptation, deep learning and active learning); human-computer interaction and haptics (through the study of how to deliver facial expressions and emotions using our sense of touch, which involved the construction of a novel visuo-haptic sensory substitution device and evaluations of visual-to-tactile mappings of nonverbal cues); and socio-behavioral computing and assistive technology (through the design, development and evaluation of a novel social interaction assistant, and how it might enrich and be used in social environments). These contributions have been disseminated through conference proceedings, journals, presentations and outreach activities.\n\nWhile the contributions made as part of this project will have significant impact on the lives of individuals with all kinds of visual impairments, the concepts and technologies developed will also provide pathways to social assistive aids for individuals with other sensory, perceptual or cognitive impairments affecting communication, such as autism, prosopagnosia and dementia. Even for the broader population, social assistive aids are applicable in many instances where nonverbal cues are inaccessible, such as when speaking to someone over the phone.\n\n\t\t\t\t\tLast Modified: 11/05/2015\n\n\t\t\t\t\tSubmitted by: Sethuraman Panchanathan"
 }
}