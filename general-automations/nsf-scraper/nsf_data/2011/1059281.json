{
 "awd_id": "1059281",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research:  CI-ADDO-EN:  Development of Publicly Available, Easily Searchable, Linguistically Analyzed, Video Corpora for Sign Language and Gesture Research",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-08-01",
 "awd_exp_date": "2014-07-31",
 "tot_intn_awd_amt": 97908.0,
 "awd_amount": 97908.0,
 "awd_min_amd_letter_date": "2011-07-27",
 "awd_max_amd_letter_date": "2011-07-27",
 "awd_abstract_narration": "The goal of this project is to create a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This will constitute an important piece of infrastructure, enabling new kinds of research in both linguistics and vision-based recognition of ASL. In addition, a key goal is to make this corpus easily accessible to the broader ASL community, including users and learners of ASL. As a result of our long-term efforts, we have an extensive collection of linguistically annotated video data from native signers of ASL. However, the potential value of these corpora has been largely untapped, notwithstanding their extensive and productive use by our team and others. Existing limitations in our hardware and software infrastructure make it cumbersome to search and identify data of interest, and to share data among our institutions and with other researchers. In this project, we propose hardware and software innovations that will constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.\r\n \r\nThe enhancement and improved Web-accessibility of these corpora will be invaluable for linguistic research, enabling new kinds of discoveries and the testing of hypotheses that would otherwise have be difficult to investigate. On the computer vision side, the proposed new annotations will provide an extensive public dataset for training and benchmarking a variety of computer vision algorithms. This will facilitate research and expedite progress in gesture recognition, hand pose estimation, human tracking, and large vocabulary, and continuous ASL recognition. Furthermore, this dataset will be useful as training and benchmarking data for algorithms in the broader areas of computer vision, machine learning, and similarity-based indexing. \r\n\r\nThe advances in linguistic knowledge about ASL and in computer-based ASL recognition that will be accelerated by the availability of resources of the kind proposed here will contribute to development of technologies for education and universal access. For example, tools for searching collections of ASL video for occurrences of specific signs, or converting ASL signing to English, are still far from attaining the level of functionality and usability to which users are accustomed for spoken/written languages. Our corpora will enable research that aims to bring such vision-based ASL recognition applications closer to reality. Moreover, these resources will afford important opportunities to individuals who would not otherwise be in a position to conduct such research (e.g., for lack of access to native ASL signers or high-quality synchronized video equipment, or lack of resources/expertise to carry out extensive linguistic annotations). Making our corpora available online will also allow the broader community of ASL users to access our data directly. Students of ASL will be able to retrieve video showing examples of a specific sign used in actual sentences, or examples of a grammatical construction. ASL instructors and teachers of the Deaf will also have easy access to video examples of lexical items and grammatical constructions as used by a variety of native signers for use in language instruction and evaluation. Thus, the proposed web interface to our data collection will be a useful educational resource for users, teachers, and learners of ASL.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Dimitris",
   "pi_last_name": "Metaxas",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Dimitris N Metaxas",
   "pi_email_addr": "dnm@cs.rutgers.edu",
   "nsf_id": "000236186",
   "pi_start_date": "2011-07-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Rutgers University New Brunswick",
  "inst_street_address": "3 RUTGERS PLZ",
  "inst_street_address_2": "",
  "inst_city_name": "NEW BRUNSWICK",
  "inst_state_code": "NJ",
  "inst_state_name": "New Jersey",
  "inst_phone_num": "8489320150",
  "inst_zip_code": "089018559",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "NJ12",
  "org_lgl_bus_name": "RUTGERS, THE STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "M1LVPE5GLSD9"
 },
 "perf_inst": {
  "perf_inst_name": "Rutgers University New Brunswick",
  "perf_str_addr": "3 RUTGERS PLZ",
  "perf_city_name": "NEW BRUNSWICK",
  "perf_st_code": "NJ",
  "perf_st_name": "New Jersey",
  "perf_zip_code": "089018559",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "NJ12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735900",
   "pgm_ele_name": "CCRI-CISE Cmnty Rsrch Infrstrc"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7359",
   "pgm_ref_txt": "COMPUTING RES INFRASTRUCTURE"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 97908.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>&nbsp;</p>\n<p>This project has created a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This constitutes an important piece of infrastructure, enabling new kinds of research in both linguistics and computer vision-based ASL recognition. In addition, a key goal is accessibility to the broader ASL community, including ASL users and learners.</p>\n<p>&nbsp;</p>\n<p>This project encompasses hardware and software innovations that constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.</p>\n<p>&nbsp;</p>\n<p>This project is collaborative among Boston University (BU), Gallaudet University (GU), Rutgers University (RU) and University of Texas at Arlington (UTA). &nbsp;Activities have involved:</p>\n<p>&nbsp;</p>\n<p>&nbsp;a)&nbsp;&nbsp;&nbsp;&nbsp; Development of mirrored file systems at RU and BU; establishment of protocols for copying files across institutions;</p>\n<p>&nbsp;</p>\n<p>b)&nbsp;&nbsp;&nbsp;&nbsp; Creation of Web-based, database search interface for annotated ASL video corpora (GU, BU, RU);</p>\n<p>&nbsp;</p>\n<p>c)&nbsp;&nbsp;&nbsp;&nbsp; Development of software modules to aid with annotation of linguistic properties (BU);</p>\n<p>&nbsp;</p>\n<p>d)&nbsp;&nbsp;&nbsp; Annotation and verification of annotations for ASL video corpora (BU, GU), including hand and face locations (UTA);</p>\n<p>&nbsp;</p>\n<p>e)&nbsp;&nbsp;&nbsp;&nbsp; Research, development, and incorporation of sign lookup into the Web interface (UTA);</p>\n<p>&nbsp;</p>\n<p>f)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Incorporation of computer based sign analysis results into the Corpus (RU, BU)</p>\n<p>&nbsp;</p>\n<p>g)&nbsp;&nbsp;&nbsp;&nbsp; Dissemination of information about corpus (all).</p>\n<p>&nbsp;</p>\n<p>Our corpus of linguistically annotated ASL data is expanding (BU efforts). We have significantly improved the face tracking (now based on 3D modeling) and are now able to generate visualizations of the tracking results, showing changes in facial expressions and head gestures in temporal relation to the production of manual signs (RU).&nbsp; The functionalities to enable such data to be visually browsed and shared through our website are nearly complete.</p>\n<p>&nbsp;</p>\n<p>&nbsp;The storage and dissemination system purchased at Rutgers is an Aberdeen, AberNAS system, configured with multiple volumes exported to both UNIX and Windows machines. Partitions are used for archival storage of the data (BU) and research on ASL analysis from video. It consists of a Linux-based controller box, which in turn runs an iSCSI control device to handle the 67 3TB disks. So we have a 300TB system. Network connectivity is by 2 Gigabit Ethernet connections to researchers at Rutgers. The backup system is a Quantum Ultrium 5 tape drive with 16 bays for robotically moved tapes. These tapes are moved offsite once a month. Disk volumes designated 'archival' are backed up once every six months (we have two complete sets, and swap them back and forth at regular intervals)? disk volumes designated 'current data or research' are backed up once a month. All volumes use RAID6 redundancy with additional hot spares. Detection and recovery of any single disk failure is performed &nbsp;automatically.</p>\n<p>&nbsp;</p>\n<p>&nbsp;For security reasons, the system is not connected to the Internet directly, but has traffic &nbsp;restricted to inside Rutgers (since the system might make an inviting target, and there was no obvious reason to make the server itself accessible from the outside world). When there is a request for data, files can be made available via SFTP (for instance) to the outside world via an NFS mount from a Rutgers-based Internet reachable machine. Beyond that, we do patching as the vendor directs (since it is their own version of Linux). The above design of the system ensures its lon...",
  "por_txt_cntn": "\n \n\nThis project has created a linguistically annotated, publicly available, and easily searchable corpus of video from American Sign Language (ASL). This constitutes an important piece of infrastructure, enabling new kinds of research in both linguistics and computer vision-based ASL recognition. In addition, a key goal is accessibility to the broader ASL community, including ASL users and learners.\n\n \n\nThis project encompasses hardware and software innovations that constitute a major qualitative upgrade in the organization, searchability, and public availability of the existing (and expanding) corpus.\n\n \n\nThis project is collaborative among Boston University (BU), Gallaudet University (GU), Rutgers University (RU) and University of Texas at Arlington (UTA).  Activities have involved:\n\n \n\n a)     Development of mirrored file systems at RU and BU; establishment of protocols for copying files across institutions;\n\n \n\nb)     Creation of Web-based, database search interface for annotated ASL video corpora (GU, BU, RU);\n\n \n\nc)     Development of software modules to aid with annotation of linguistic properties (BU);\n\n \n\nd)    Annotation and verification of annotations for ASL video corpora (BU, GU), including hand and face locations (UTA);\n\n \n\ne)     Research, development, and incorporation of sign lookup into the Web interface (UTA);\n\n \n\nf)      Incorporation of computer based sign analysis results into the Corpus (RU, BU)\n\n \n\ng)     Dissemination of information about corpus (all).\n\n \n\nOur corpus of linguistically annotated ASL data is expanding (BU efforts). We have significantly improved the face tracking (now based on 3D modeling) and are now able to generate visualizations of the tracking results, showing changes in facial expressions and head gestures in temporal relation to the production of manual signs (RU).  The functionalities to enable such data to be visually browsed and shared through our website are nearly complete.\n\n \n\n The storage and dissemination system purchased at Rutgers is an Aberdeen, AberNAS system, configured with multiple volumes exported to both UNIX and Windows machines. Partitions are used for archival storage of the data (BU) and research on ASL analysis from video. It consists of a Linux-based controller box, which in turn runs an iSCSI control device to handle the 67 3TB disks. So we have a 300TB system. Network connectivity is by 2 Gigabit Ethernet connections to researchers at Rutgers. The backup system is a Quantum Ultrium 5 tape drive with 16 bays for robotically moved tapes. These tapes are moved offsite once a month. Disk volumes designated 'archival' are backed up once every six months (we have two complete sets, and swap them back and forth at regular intervals)? disk volumes designated 'current data or research' are backed up once a month. All volumes use RAID6 redundancy with additional hot spares. Detection and recovery of any single disk failure is performed  automatically.\n\n \n\n For security reasons, the system is not connected to the Internet directly, but has traffic  restricted to inside Rutgers (since the system might make an inviting target, and there was no obvious reason to make the server itself accessible from the outside world). When there is a request for data, files can be made available via SFTP (for instance) to the outside world via an NFS mount from a Rutgers-based Internet reachable machine. Beyond that, we do patching as the vendor directs (since it is their own version of Linux). The above design of the system ensures its longevity and reliability for data access and robustness. The existing corpus (including high and resolution videos, annotations, and other source datasets) has now been consolidated and placed on the new server at Rutgers (more than 23TB of data). Multiple portable disks from BU were loaded with copies of the existing ASLLRP corpus and sent to the partner institutions. Rutgers placed a 'mirror' copy of the dataset on its file server.\n\n \n\n Data from t..."
 }
}