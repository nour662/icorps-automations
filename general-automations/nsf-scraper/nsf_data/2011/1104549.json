{
 "awd_id": "1104549",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Any Data, Anytime, Anywhere",
 "cfda_num": "47.049",
 "org_code": "03010000",
 "po_phone": "7032924516",
 "po_email": "jshank@nsf.gov",
 "po_sign_block_name": "James Shank",
 "awd_eff_date": "2011-09-15",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 814448.0,
 "awd_amount": 814448.0,
 "awd_min_amd_letter_date": "2011-09-13",
 "awd_max_amd_letter_date": "2011-09-13",
 "awd_abstract_narration": "The Grid computing model connects computers that are scattered over a wide geographic area, allowing their computing power to be shared. Just as the World Wide Web enables access to information, computer grids enable access to distributed computing resources. These resources include detectors, computer cycles, computer storage, visualization tools, and more. Thus, grids can combine the resources of thousands of different computers that are not fully utilized, and assemble these to create a massively powerful resource, and, with GRID software, this resource can be accessible from a personal computer. \r\n\r\nFor scientists in international collaborations, grid computing provides the power that can enable effective collaborations whose members are widely dispersed geographically. Grids also can enable simulations that might take weeks on a single PC to run in hours on a grid. Further, the development of computing grids also develops new communities. Grids therefore encourage and require people from different countries and cultures to work together to solve problems.\r\n\r\nGrid computing works because people participating in grids opt to share their computer power with others. This opens many questions, both social and technical. Who should be allowed to use each grid? Whose job should get priority in the queue to use grid power? What is the best way to protect user security? How will users pay for grid usage? Answering these questions requires all-new technical solutions, each of which must evolve as other grid and information technologies develop. Since grids involve countries and regions all over the world, these solutions must also suit different technical requirements, limitations and usage patterns.\r\n\r\nThe Large Hadron Collider (LHC), the accelerator facility discussed in this proposal, is a particle accelerator constructed  as a collaboration between more than 50 countries. The world's largest machine, it accelerates particles to nearly the speed of light and then steers these particles into 600 million collisions every second. Data from these collisions is expected to change our basic understanding of antimatter, dark energy and more. The LHC will produce 15 million gigabytes of data a year: the storage capacity of around 20,000,000 CDs. Thousands of physicists all over the world want timely access to this data.\r\n\r\nThe LHC Computing Grid (LCG) combines the computing resources of more than 140 computing centers aiming to harness the power of 100,000 computers to process, analyze and store data produced from the LHC, making it equally available to all partners, regardless of their physical location. \r\n\r\nThrough this proposal, a new LCG computing model for the CMS experiment at the LHC will enable dynamic access to existing world-wide data caches and will provide the capabilities for applications on any laptop, server, or cluster, to access data seamlessly from wherever it is stored.  Data access will no longer require the operation of large scale storage infrastructures local to the participating procesors.. \r\n\r\nThis model will give the distributed physics groups automated access to \"Any Data\" at \"Anytime\" from \"Anywhere\", decreasing data access costs, and reducing application failure. When pre-staging of voluminous datasets into such local storage is not plausible - due to either a lack of local hardware capability or instantaneous demand - it will be replaced with on-demand access of only those data objects the analysis actually requires from any remote site where the data are available. This significantly reduces the overall I/O and storage space requirements outside the CMS computer systems. The total cost of ownership of computer centers at universities throughout the US is thus significantly reduced, as the dominant human as well as hardware costs related to provisioning and operating a storage infrastructure disappear. The project will enable smaller scale university clusters and physicists' desktop computers access to all types of CMS data and thus improve the scientific output of CMS scientists nationwide.\r\n\r\nFurther, this proposed infrastructure could be used for data-driven research in all fields, ranging from other natural sciences to social sciences and the humanities.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "PHY",
 "org_div_long_name": "Division Of Physics",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Frank",
   "pi_last_name": "Wuerthwein",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Frank Wuerthwein",
   "pi_email_addr": "fkw@ucsd.edu",
   "nsf_id": "000144338",
   "pi_start_date": "2011-09-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "724500",
   "pgm_ele_name": "PHYSICS GRID COMPUTING"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 814448.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>With this project we developed, implemented, and deployed a global \"Data Federation\" that allows scientists to access any data, any time, anywhere on disk within the federations.</p>\n<p>We used a global physics collaboration spanning 180 institutions across 40 countries as our exemplar to develop the technology. By the end of the project, the more than 2000 physicists of the CMS collaboration can access any of its tens of petabytes of active data from anywhere with an internet connection.</p>\n<p>A scientist simply points their application to one of the nodes within the federation, and the system we built redirects the application to the appropriate server globally to access the requested data. We implemented intelligence both into the client application as well as the network of federated servers such as to optimize performance of data access.</p>\n<p>At the end of the funded project, we made sure to educate the CMS collaboration's computing operations team to sustain the system's operations long term, well beyond this project's lifetime.</p>\n<p>The system we build is largely based on a software suite called XRootd that is maintained long term by a collaboration of Universities and National laboratories in the US and Europe. To sustain the main software artefacts created as part of this project, we committed them back to the XRootd open source repository, thus making its maintenance sustainable long term as part of the international XRootd collaboration.</p>\n<p>In addition, we wrote a final paper that provides an overview of all aspects of the system we built. The resulting paper was accepted for publication in a peer reviewed IEEE conference proceedings. It references several more detailed papers that describe different aspects of our work, and are also published in peer reviewed proceedings.</p>\n<p>Finally, we helped the Open Science Grid and the Pacific Research Platform, both NSF funded projects that will outlive this project, to adopt some of the core software artefacts we developed in order to support a wide cross section of science beyond the original target audience within our project.</p>\n<p>The outcome of this project is thus a data federation software infrastructure that remains in production use long after the projects end date by multiple science stakeholders well beyond the original target audience. We consider this a perfect example of broad and long term impact for a limited investment by the NSF.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/28/2015<br>\n\t\t\t\t\tModified by: Frank&nbsp;Wuerthwein</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nWith this project we developed, implemented, and deployed a global \"Data Federation\" that allows scientists to access any data, any time, anywhere on disk within the federations.\n\nWe used a global physics collaboration spanning 180 institutions across 40 countries as our exemplar to develop the technology. By the end of the project, the more than 2000 physicists of the CMS collaboration can access any of its tens of petabytes of active data from anywhere with an internet connection.\n\nA scientist simply points their application to one of the nodes within the federation, and the system we built redirects the application to the appropriate server globally to access the requested data. We implemented intelligence both into the client application as well as the network of federated servers such as to optimize performance of data access.\n\nAt the end of the funded project, we made sure to educate the CMS collaboration's computing operations team to sustain the system's operations long term, well beyond this project's lifetime.\n\nThe system we build is largely based on a software suite called XRootd that is maintained long term by a collaboration of Universities and National laboratories in the US and Europe. To sustain the main software artefacts created as part of this project, we committed them back to the XRootd open source repository, thus making its maintenance sustainable long term as part of the international XRootd collaboration.\n\nIn addition, we wrote a final paper that provides an overview of all aspects of the system we built. The resulting paper was accepted for publication in a peer reviewed IEEE conference proceedings. It references several more detailed papers that describe different aspects of our work, and are also published in peer reviewed proceedings.\n\nFinally, we helped the Open Science Grid and the Pacific Research Platform, both NSF funded projects that will outlive this project, to adopt some of the core software artefacts we developed in order to support a wide cross section of science beyond the original target audience within our project.\n\nThe outcome of this project is thus a data federation software infrastructure that remains in production use long after the projects end date by multiple science stakeholders well beyond the original target audience. We consider this a perfect example of broad and long term impact for a limited investment by the NSF.\n\n\t\t\t\t\tLast Modified: 12/28/2015\n\n\t\t\t\t\tSubmitted by: Frank Wuerthwein"
 }
}