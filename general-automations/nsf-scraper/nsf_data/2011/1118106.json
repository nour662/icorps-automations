{
 "awd_id": "1118106",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "III: Small: Parallel Similarity Comparison and Duplicate Detection with Incremental Computing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Maria Zemankova",
 "awd_eff_date": "2011-08-15",
 "awd_exp_date": "2016-07-31",
 "tot_intn_awd_amt": 499732.0,
 "awd_amount": 515732.0,
 "awd_min_amd_letter_date": "2011-08-18",
 "awd_max_amd_letter_date": "2012-03-20",
 "awd_abstract_narration": "All-pairs similarity comparison is one of the core algorithms in many data-intensive mining and search applications such as near duplicate detection among  web pages, spam detection, advertisement click analysis,  similar news/fresh content grouping, and  recommendation for  similar product purchases and search queries. Conducting similarity search on large datasets is time consuming  and becomes more challenging when data are being updated continuously. It is important to develop high performance algorithms and software to meet the increasing speed demands in many consumer and business applications using similarity computation. \r\n\r\nThis project studies efficient and cost-effective parallel algorithms when data are being updated periodically or dynamically. Techniques for partitioning data and balancing computation on a cluster of machines are developed to optimize input/output operations, communication, and computing resource usage. As data are often updated continuously, leveraging previously computed results to handle updated data can eliminate a large amount of unnecessary operations and speedup the entire computation process by an order of magnitude. The project develops efficient software on a cluster of machines. The project starts with incremental duplicate detection for web data analysis and search, and continues to work on similarity comparison in several other applications. Performance of developed software is evaluated in those applications.\r\n\r\nThis research has the potential to develop fully-optimized solutions with significantly reduced cost and increased speed for a variety of big data applications that perform similarity analysis. Developed software will be made available for application developers or data engineers to conduct large-scale computation without involving the complexity of managing parallelism. The project web site (http://www.cs.ucsb.edu/projects/psc/) is used for dissemination of results. The educational plan contains research mentoring, undergraduate and graduate instruction improvement, and outreach activities such as working with high school students.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Tao",
   "pi_last_name": "Yang",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Tao Yang",
   "pi_email_addr": "tyang@cs.ucsb.edu",
   "nsf_id": "000202471",
   "pi_start_date": "2011-08-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Santa Barbara",
  "inst_street_address": "3227 CHEADLE HALL",
  "inst_street_address_2": "",
  "inst_city_name": "SANTA BARBARA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8058934188",
  "inst_zip_code": "931060001",
  "inst_country_name": "United States",
  "cong_dist_code": "24",
  "st_cong_dist_code": "CA24",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SANTA BARBARA",
  "org_prnt_uei_num": "",
  "org_uei_num": "G9QBQDH39DF4"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Santa Barbara",
  "perf_str_addr": "3227 CHEADLE HALL",
  "perf_city_name": "SANTA BARBARA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "931060001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "24",
  "perf_st_cong_dist": "CA24",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 499732.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project investigates efficient parallel algorithms and distributed&nbsp;system support for similarity-based data-intensive computing. It starts with near duplicate &nbsp;detection for web data analysis and search, and continues to work on similarity computing in several applications and develop efficient parallel software on a cluster of machines.&nbsp;</p>\n<p>One of key problems studied is &nbsp;time-consuming all-pairs similarity search used in many web search and data mining applications, and this project has developed &nbsp;a scalable two-stage parallel method called Partition-based Similarity Search. The first stage is to partition the dataset and group potentially similar vectors. The second stage is to run a set of parallel tasks where each task compares a partition of vectors with other candidate partitions. Static partitioning is employed to detect unnecessary comparisons and accomplish &nbsp;early elimination of useless I/O and data communication. Because of data sparsity, accessing irregular feature vectors residing in memory hierarchy during runtime partition comparison incurs significant overhead. The proposed technique optimizes data traversal with a cache-conscious layout to reduce the execution time through size-controlled data splitting and vector coalescing, and provides an analysis to guide the optimal choice for parameter setting. The evaluation results show that the proposed approach leads to one or two orders of magnitude of performance improvement. &nbsp;This project has further considered incremental similarity computing, approximation with locality-sensitive hashing, &nbsp;and &nbsp;mitigating load imbalance &nbsp;among parallel machines caused by the variation in partition sizes and irregular dissimilarity relationship in a large dataset. &nbsp;&nbsp;This project has developed a two-step load balancing algorithm &nbsp;for similarity search and analyzed the optimality and competitiveness of the proposed algorithm.</p>\n<p>The second problem studied is incremental near duplicate detection for web search engines and other data intensive applications. An offline approach to remove redundant content can be important for reducing the engine&rsquo;s cost, but it is challenging to &nbsp;scale such an approach for a large data set which is updated continuously. This project develops &nbsp;a scalable approach with parallel clustering that detects and removes near duplicates incrementally when processing billions of web pages. The parallel clustering scheme is supported by a two-tier architecture for detecting and managing near duplicate groups with incremental computing and multidimensional data mapping. The evaluation shows that removing redundant content in an offline system significantly reduces the overall engine cost while sustaining relevancy quality. Incremental update of duplicate groups greatly speeds up computing time and processing throughput while multidimensional mapping offers a flexibility in improving load balancing and processing throughput efficiency.</p>\n<p>The third problem studied is incremental duplicate data detection in cloud virtual machine (VM) backup. Full source-side deduplication with fingerprint comparison can remove redundant content and reduce network traffic in a 1:10 to 1:20 ratio, but such an operation is expensive in terms of memory and computing resources and can affect other co-located services. This project has developed &nbsp;a fast VM-centric backup service with a tradeoff for a competitive deduplication efficiency while using small computing resources. The design consideration includes the exploitation of popular duplicates following &nbsp;a zipf-like distribution, and VM-centric file system block management for the increased VM snapshot availability. The project has further developed source-side collaborative deduplication when frequent virtual machine snapshot backup is required in a large-scale cloud cluster. The key idea is to separate duplicate detection from the actual storage backup instead of using inline deduplication, and partition global index and duplicate detection requests among machines with buffering control for minimal memory usage.</p>\n<p>More information including software prototype for all-pairs similarity search can be found in the project web site http://www.cs.ucsb.edu/projects/psc/. &nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/20/2016<br>\n\t\t\t\t\tModified by: Tao&nbsp;Yang</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis project investigates efficient parallel algorithms and distributed system support for similarity-based data-intensive computing. It starts with near duplicate  detection for web data analysis and search, and continues to work on similarity computing in several applications and develop efficient parallel software on a cluster of machines. \n\nOne of key problems studied is  time-consuming all-pairs similarity search used in many web search and data mining applications, and this project has developed  a scalable two-stage parallel method called Partition-based Similarity Search. The first stage is to partition the dataset and group potentially similar vectors. The second stage is to run a set of parallel tasks where each task compares a partition of vectors with other candidate partitions. Static partitioning is employed to detect unnecessary comparisons and accomplish  early elimination of useless I/O and data communication. Because of data sparsity, accessing irregular feature vectors residing in memory hierarchy during runtime partition comparison incurs significant overhead. The proposed technique optimizes data traversal with a cache-conscious layout to reduce the execution time through size-controlled data splitting and vector coalescing, and provides an analysis to guide the optimal choice for parameter setting. The evaluation results show that the proposed approach leads to one or two orders of magnitude of performance improvement.  This project has further considered incremental similarity computing, approximation with locality-sensitive hashing,  and  mitigating load imbalance  among parallel machines caused by the variation in partition sizes and irregular dissimilarity relationship in a large dataset.   This project has developed a two-step load balancing algorithm  for similarity search and analyzed the optimality and competitiveness of the proposed algorithm.\n\nThe second problem studied is incremental near duplicate detection for web search engines and other data intensive applications. An offline approach to remove redundant content can be important for reducing the engine?s cost, but it is challenging to  scale such an approach for a large data set which is updated continuously. This project develops  a scalable approach with parallel clustering that detects and removes near duplicates incrementally when processing billions of web pages. The parallel clustering scheme is supported by a two-tier architecture for detecting and managing near duplicate groups with incremental computing and multidimensional data mapping. The evaluation shows that removing redundant content in an offline system significantly reduces the overall engine cost while sustaining relevancy quality. Incremental update of duplicate groups greatly speeds up computing time and processing throughput while multidimensional mapping offers a flexibility in improving load balancing and processing throughput efficiency.\n\nThe third problem studied is incremental duplicate data detection in cloud virtual machine (VM) backup. Full source-side deduplication with fingerprint comparison can remove redundant content and reduce network traffic in a 1:10 to 1:20 ratio, but such an operation is expensive in terms of memory and computing resources and can affect other co-located services. This project has developed  a fast VM-centric backup service with a tradeoff for a competitive deduplication efficiency while using small computing resources. The design consideration includes the exploitation of popular duplicates following  a zipf-like distribution, and VM-centric file system block management for the increased VM snapshot availability. The project has further developed source-side collaborative deduplication when frequent virtual machine snapshot backup is required in a large-scale cloud cluster. The key idea is to separate duplicate detection from the actual storage backup instead of using inline deduplication, and partition global index and duplicate detection requests among machines with buffering control for minimal memory usage.\n\nMore information including software prototype for all-pairs similarity search can be found in the project web site http://www.cs.ucsb.edu/projects/psc/.  \n\n\t\t\t\t\tLast Modified: 09/20/2016\n\n\t\t\t\t\tSubmitted by: Tao Yang"
 }
}