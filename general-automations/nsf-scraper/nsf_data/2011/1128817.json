{
 "awd_id": "1128817",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "Convex optimization methods for system identification and graphical  modeling of time series",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Radhakisan Baheti",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 378761.0,
 "awd_amount": 378761.0,
 "awd_min_amd_letter_date": "2011-08-29",
 "awd_max_amd_letter_date": "2013-07-29",
 "awd_abstract_narration": "Objectives\r\n\r\nThe project aims at developing new methods for dynamical system modeling, based on convex optimization formulations and recent algorithms for large-scale non-smooth optimization.  A first component of the project addresses the estimation and topology selection of graphical models of time series.  A graphical model provides a graph representation of relations between random variables, for example, conditional dependence.  These relations can be translated into sparsity constraints on the parameters of the model.  A fundamental challenge in the estimation of a graphical model is the selection of a sparse graph topology from observed data.  The project aims at developing methods for sparse topology selection via non-smooth convex regularizations.  The main application that motivates this work is connectivity analysis from functional magnetic resonance imaging time series. A second part is concerned with new methods for system identification based on convex algorithms for low-rank approximation of structured matrices.  This work requires the formulation of system identification problems as constrained rank optimization problems and the development of large-scale algorithms for convex relaxations of the rank optimization problems.\r\n\r\nIntellectual Merit\r\n\r\nThe project combines techniques from optimization, system theory, and machine learning to address fundamental problems in the modeling of dynamical systems.\r\nGraphical models, an important topic in machine learning, are not widely studied in system identification.  Conversely, system identification can provide tools for modeling dynamical aspects in machine learning problems. The use of convex formulations and fast first-order algorithms will enable an efficient solution of large instances in practical applications.\r\n\r\nBroader Impacts\r\n\r\nSoftware implementations of the algorithms developed in the project will be made freely available. The outcomes will be integrated in the graduate optimization sequence in the Electrical Engineering Department at UCLA, in particular an advanced course on large-scale optimization.  Research opportunities will be offered to students via individual study courses and summer internships.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lieven",
   "pi_last_name": "Vandenberghe",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lieven Vandenberghe",
   "pi_email_addr": "vandenbe@ee.ucla.edu",
   "nsf_id": "000488711",
   "pi_start_date": "2011-08-29",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "",
  "perf_city_name": "Los Angeles",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900951594",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "092E",
   "pgm_ref_txt": "Control systems & applications"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 128075.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 124171.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 126515.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The project has aimed at developing new methods for dynamical system identification and time series modeling, based on convex optimization formulations and large scale optimization algorithms.&nbsp; It was motivated by recent advances in convex techniques for optimization problems that involve sparsity and low-rank constraints, and in first-order methods for non-smooth convex optimization problems. These advances can be combined to create promising new tools for system identification and time series modeling.<br /><br />As a first contribution, we proposed a system identification method for problems with partially missing inputs and outputs.&nbsp; The method is based on a subspace formulation and uses the trace norm heuristic for structured low-rank matrix approximation, with the missing input and output values treated as the optimization variables in the problem.&nbsp; We also developed fast first-order algorithms, based on the Douglas-Rachford algorithm, for solving the resulting convex&nbsp; optimization problems, which are challenging for general-purpose solvers.<br /><br />As a second contribution, we proposed semidefinite optimization methods for signal processing problems in which vectors or matrices need to be decomposed into sums of a few elements from an infinite set (`dictionary') parameterized by a matrix pencil.&nbsp; This extends popular 1-norm regularization methods but avoids the need for discretization.&nbsp; Our results exploit the connections between semidefinite optimization duality and the Kalman-Yakubovich-Popov lemma, a fundamental result in system theory.<br /><br />In the area of graphical models, we investigated the proximal Newton method for restricted covariance selection.&nbsp; In this problem the topology (sparsity pattern) of the graphical model is partially specified and a regularized maximum likelihood function is maximized to identify the rest of the topology.&nbsp; We analyzed the convergence properties of the proximal Newton method applied to self-concordant functions in detail.&nbsp; In particular, we included the possibility of inexact search directions.<br /><br />Finally, we studied different decomposition methods for large-scale convex optimization.&nbsp; This includes primal-dual decomposition methods that extend classical primal and dual decomposition techniques, and decomposition methods for sparse semidefinite optimization and sparse matrix nearness problems.<br /><br />Several of the results obtained in the project&nbsp; have been integrated into a graduate course on large-scale optimization at UCLA, and in lectures by the PI for short courses and summer schools.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/14/2016<br>\n\t\t\t\t\tModified by: Lieven&nbsp;Vandenberghe</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe project has aimed at developing new methods for dynamical system identification and time series modeling, based on convex optimization formulations and large scale optimization algorithms.  It was motivated by recent advances in convex techniques for optimization problems that involve sparsity and low-rank constraints, and in first-order methods for non-smooth convex optimization problems. These advances can be combined to create promising new tools for system identification and time series modeling.\n\nAs a first contribution, we proposed a system identification method for problems with partially missing inputs and outputs.  The method is based on a subspace formulation and uses the trace norm heuristic for structured low-rank matrix approximation, with the missing input and output values treated as the optimization variables in the problem.  We also developed fast first-order algorithms, based on the Douglas-Rachford algorithm, for solving the resulting convex  optimization problems, which are challenging for general-purpose solvers.\n\nAs a second contribution, we proposed semidefinite optimization methods for signal processing problems in which vectors or matrices need to be decomposed into sums of a few elements from an infinite set (`dictionary') parameterized by a matrix pencil.  This extends popular 1-norm regularization methods but avoids the need for discretization.  Our results exploit the connections between semidefinite optimization duality and the Kalman-Yakubovich-Popov lemma, a fundamental result in system theory.\n\nIn the area of graphical models, we investigated the proximal Newton method for restricted covariance selection.  In this problem the topology (sparsity pattern) of the graphical model is partially specified and a regularized maximum likelihood function is maximized to identify the rest of the topology.  We analyzed the convergence properties of the proximal Newton method applied to self-concordant functions in detail.  In particular, we included the possibility of inexact search directions.\n\nFinally, we studied different decomposition methods for large-scale convex optimization.  This includes primal-dual decomposition methods that extend classical primal and dual decomposition techniques, and decomposition methods for sparse semidefinite optimization and sparse matrix nearness problems.\n\nSeveral of the results obtained in the project  have been integrated into a graduate course on large-scale optimization at UCLA, and in lectures by the PI for short courses and summer schools.\n\n\t\t\t\t\tLast Modified: 11/14/2016\n\n\t\t\t\t\tSubmitted by: Lieven Vandenberghe"
 }
}