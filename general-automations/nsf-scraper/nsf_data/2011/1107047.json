{
 "awd_id": "1107047",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Penalization Methods for Screening, Variable Selection and Dimension Reduction in High-Dimensional Regression via Multiple Index Models",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2011-06-15",
 "awd_exp_date": "2014-05-31",
 "tot_intn_awd_amt": 50000.0,
 "awd_amount": 50000.0,
 "awd_min_amd_letter_date": "2011-06-06",
 "awd_max_amd_letter_date": "2011-06-06",
 "awd_abstract_narration": "The project aims to develop effective penalization methods for screening, dimension reduction, and variable selection in high dimensional regression. The investigators focus mainly on multiple index models, because this type of models combines the strengths of linear and nonparametric regression while avoiding their drawbacks. A novel penalization approach is employed for model fitting, which regularizes both the parametric and nonparametric components of a multiple index model.  A pilot study shows that this approach is more advantageous than other existing ones. When facing ultra-high dimensionality, the investigators use a forward variable screening procedure to reduce the dimension to a manageable size before applying the proposed penalization. The investigators plan to study the theoretical properties of this approach and develop fast and efficient computing algorithms for its implementation. The proposed approach is further extended to applications involving categorical responses or random effects.\r\n\r\nAdvances in science and technology have led to an explosive growth of massive data across a variety of areas such as bioinformatics, climate research, internet, etc. Traditional statistical methods for clustering, regression and classification become ineffective when dealing with a large number of variables. Lately, a tremendous amount of research effort has been dedicated to the development of statistical methods such as dimension reduction and variable selection for analyzing this type of massive data. The investigators join the effort by proposing a novel penalization approach and developing efficient computing algorithms. The results from this project not only advance statistical research but also help other scientists and researchers better understand and analyze their massive data and hence enhance their scientific discovery.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yu Michael",
   "pi_last_name": "Zhu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yu Michael Zhu",
   "pi_email_addr": "yuzhu@stat.purdue.edu",
   "nsf_id": "000282548",
   "pi_start_date": "2011-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "2550 NORTHWESTERN AVE # 1100",
  "perf_city_name": "WEST LAFAYETTE",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479061332",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This research project focuses on the development of penalty-based methods for dimension reduction and variable selection in high dimensional regression analysis. In particular,&nbsp;it focuses on developing&nbsp;methods under the assumption of single&nbsp;and multiple index models. Index models form a special family of semiparametric models, because they all consist of a parametric component (i.e. indices) and a lower dimensional nonparametric component (i.e. link functions). They can be considered compromises between linear regression models and fully nonparametric models. Index models are used&nbsp;to model the relationship between a response variable&nbsp;and a&nbsp;vector of explanatory variables, and&nbsp;are often used to facilitate dimension reduction and variable selection. When the number of&nbsp;explanatory variables is large,&nbsp;fitting index models can become challenging due to the curse of dimensionality, and therefore, penalty-based regularization methods need to be used to&nbsp;make&nbsp; fitting index models efficient and stable in high dimensions.</p>\n<p>In this project,&nbsp;absolute value penalty&nbsp;(i.e. lasso-type penalty) functions are used&nbsp; for index model regularization. For multiple index models, instead of penalizing only&nbsp;indices,&nbsp;a novel penalty function that penalizes both&nbsp;gradients and indices have be proposed, and algorithms under this&nbsp;penalty function have been developed. It has be shown that the proposed method can lead to more efficient results&nbsp;statistically as well as computationally. For&nbsp;single index models, in order to further improve computational efficiency, regression splines are&nbsp;used to estimate the&nbsp;nonparametric link function while&nbsp;penalizing the&nbsp;index&nbsp;using the&nbsp;lasso penalty function or a general&nbsp;lasso-type penalty function. Different constraints for identifiability have also been explored. The proposed approach leads to more efficient&nbsp;algorithms, and achieves better variable selection results in high dimensional spaces than other existing methods. The conditions under which the proposed methods are consistent&nbsp;for variable selection under single index models have also been obtained.</p>\n<p>The proposed methods have been implemented in R packages, which are made available to the public.&nbsp;They can be used by data analysts for high dimensional regression analysis, especially when the linear model cannot be assumed for the relationship between the response variable and the explanatory variables.&nbsp;This research project not only helps to advance the&nbsp;statistical theory and methodology&nbsp;for high dimensional semiparametric regression analysis but also helps to develop effective tools that benefit researchers in a variety of areas.</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/08/2014<br>\n\t\t\t\t\tModified by: Yu Michael&nbsp;Zhu</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis research project focuses on the development of penalty-based methods for dimension reduction and variable selection in high dimensional regression analysis. In particular, it focuses on developing methods under the assumption of single and multiple index models. Index models form a special family of semiparametric models, because they all consist of a parametric component (i.e. indices) and a lower dimensional nonparametric component (i.e. link functions). They can be considered compromises between linear regression models and fully nonparametric models. Index models are used to model the relationship between a response variable and a vector of explanatory variables, and are often used to facilitate dimension reduction and variable selection. When the number of explanatory variables is large, fitting index models can become challenging due to the curse of dimensionality, and therefore, penalty-based regularization methods need to be used to make  fitting index models efficient and stable in high dimensions.\n\nIn this project, absolute value penalty (i.e. lasso-type penalty) functions are used  for index model regularization. For multiple index models, instead of penalizing only indices, a novel penalty function that penalizes both gradients and indices have be proposed, and algorithms under this penalty function have been developed. It has be shown that the proposed method can lead to more efficient results statistically as well as computationally. For single index models, in order to further improve computational efficiency, regression splines are used to estimate the nonparametric link function while penalizing the index using the lasso penalty function or a general lasso-type penalty function. Different constraints for identifiability have also been explored. The proposed approach leads to more efficient algorithms, and achieves better variable selection results in high dimensional spaces than other existing methods. The conditions under which the proposed methods are consistent for variable selection under single index models have also been obtained.\n\nThe proposed methods have been implemented in R packages, which are made available to the public. They can be used by data analysts for high dimensional regression analysis, especially when the linear model cannot be assumed for the relationship between the response variable and the explanatory variables. This research project not only helps to advance the statistical theory and methodology for high dimensional semiparametric regression analysis but also helps to develop effective tools that benefit researchers in a variety of areas.\n\n \n\n \n\n \n\n \n\n\t\t\t\t\tLast Modified: 11/08/2014\n\n\t\t\t\t\tSubmitted by: Yu Michael Zhu"
 }
}