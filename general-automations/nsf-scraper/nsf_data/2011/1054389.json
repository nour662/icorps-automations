{
 "awd_id": "1054389",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER: An Axiomatic Basis for Statistical Privacy",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032928832",
 "po_email": "dcosley@nsf.gov",
 "po_sign_block_name": "Dan Cosley",
 "awd_eff_date": "2011-02-01",
 "awd_exp_date": "2017-12-31",
 "tot_intn_awd_amt": 429501.0,
 "awd_amount": 437501.0,
 "awd_min_amd_letter_date": "2011-01-21",
 "awd_max_amd_letter_date": "2015-04-14",
 "awd_abstract_narration": "Statistical privacy is the art of releasing the datasets that provide useful information about population trends without revealing private information about any individual. Recent high-profile attacks on datasets released by AOL and Netflix demonstrate the need for rigorous application-specific privacy definitions to guide the anonymization of data. The goal of this project is to develop modular components, called privacy axioms, that can be chained together to create customized privacy definitions and anonymized data for statistical privacy applications. Such modularity can enable data curators without extensive expertise in statistical privacy to release anonymized data while providing privacy guarantees that are more interpretable and reliable.\r\n\r\nIntellectual merit: this project is designed to provide a unifying framework for statistical privacy that can bring about a deeper understanding of privacy issues and provide guidance for the safe anonymization and release of sensitive data. In addition to theoretical developments, this research plan also targets specific existing applications at Penn State and the U.S. Census Bureau.\r\n\r\nBroader impact: the systematic approach to privacy pursued by this project can enable access to and analysis of anonymized data in domains where access to data is otherwise heavily restricted. \r\nThis project aims to build upon the investigator's prior experience with outreach programs such as the Summer Research Opportunities Program (SROP) by involving undergraduates in the proposed research. To prepare students for future work that requires analysis of anonymized data, this research is also being integrated into machine learning courses at Penn State.\r\n\r\nFor further information see the project web site at the URL:\r\nhttp://www.cse.psu.edu/~dkifer/axiomatizingprivacy.html",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Daniel",
   "pi_last_name": "Kifer",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Daniel Kifer",
   "pi_email_addr": "duk17@psu.edu",
   "nsf_id": "000519235",
   "pi_start_date": "2011-01-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Pennsylvania State Univ University Park",
  "inst_street_address": "201 OLD MAIN",
  "inst_street_address_2": "",
  "inst_city_name": "UNIVERSITY PARK",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "8148651372",
  "inst_zip_code": "168021503",
  "inst_country_name": "United States",
  "cong_dist_code": "15",
  "st_cong_dist_code": "PA15",
  "org_lgl_bus_name": "THE PENNSYLVANIA STATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "NPM2J7MSCF61"
 },
 "perf_inst": {
  "perf_inst_name": "Pennsylvania State Univ University Park",
  "perf_str_addr": "201 OLD MAIN",
  "perf_city_name": "UNIVERSITY PARK",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "168021503",
  "perf_ctry_code": "US",
  "perf_cong_dist": "15",
  "perf_st_cong_dist": "PA15",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "171400",
   "pgm_ele_name": "Special Projects - CNS"
  },
  {
   "pgm_ele_code": "779500",
   "pgm_ele_name": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ele_code": "806000",
   "pgm_ele_name": "Secure &Trustworthy Cyberspace"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7795",
   "pgm_ref_txt": "TRUSTWORTHY COMPUTING"
  },
  {
   "pgm_ref_code": "9178",
   "pgm_ref_txt": "UNDERGRADUATE EDUCATION"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 80297.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 82975.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 85772.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 96700.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 91757.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Data that are collected from individuals can provide useful information about the population. At the same time, the data records should be kept confidential to protect the privacy of those individuals. In order to publish useful information while protecting privacy, a data owner needs to choose a privacy definition and an algorithm that satisfies the definition. The algorithm operates on the real data and produces an output (tabulations or microdata) that can be released to the public.</p>\n<p>The research funded by this award treated a privacy definition as a mathematical object. This approach allowed us to determine the provable privacy guarantees that can be achieved by an algorithm that satisfies the privacy definition. We also studied the complementary problem of how to measure the utility of an algorithm that operates on sensitive data. The research resulted in:</p>\n<ol>\n<li>A new class of privacy definitions called Pufferfish that allows formal privacy to be studied in a Bayesian framework. Using the pufferfish framework we were able to rigorously study the inferential guarantees of differential privacy.</li>\n<li>We provided theoretical tools to study the protections of privacy definitions based on the concept of a row cone, which can be interpreted in terms of probabilistic inference.</li>\n<li>We provided a characterization of utility measures that are suitable for measuring the quality of an algorithm (the information content of the data that it outputs while providing privacy protections).</li>\n<li>We provided a characterization of \"optimal\" algorithms for differential privacy in terms of information content.</li>\n<li>We provided new algorithms for estimating the degree distribution of a social network under privacy constraints.</li>\n<li>We provided new attacks on legacy disclosure control methods such as data swapping.</li>\n<li>We developed new algorithms for privacy-preserving \"count-of-counts\" histograms, which can be used to provide statistics about household size distribution in the U.S.</li>\n<li>We also developed new algorithms for large-scale outlier detection that were able to label each data point with an estimate of the degree to which it is an outlier.</li>\n</ol>\n<p>The results of the project were disseminated through academia and industry, including talks at the Census Bureau.&nbsp; They were also used to analyze the effect of various policy decisions on the inferential properties of disclosure control methods that were under consideration for the 2020 Decennial Census.</p>\n<p>The techniques developed in the research were also incorporated into machine learning courses and provided research opportunities for undergraduate students (some of whom went on to graduate study).</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 12/21/2018<br>\n\t\t\t\t\tModified by: Daniel&nbsp;Kifer</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nData that are collected from individuals can provide useful information about the population. At the same time, the data records should be kept confidential to protect the privacy of those individuals. In order to publish useful information while protecting privacy, a data owner needs to choose a privacy definition and an algorithm that satisfies the definition. The algorithm operates on the real data and produces an output (tabulations or microdata) that can be released to the public.\n\nThe research funded by this award treated a privacy definition as a mathematical object. This approach allowed us to determine the provable privacy guarantees that can be achieved by an algorithm that satisfies the privacy definition. We also studied the complementary problem of how to measure the utility of an algorithm that operates on sensitive data. The research resulted in:\n\nA new class of privacy definitions called Pufferfish that allows formal privacy to be studied in a Bayesian framework. Using the pufferfish framework we were able to rigorously study the inferential guarantees of differential privacy.\nWe provided theoretical tools to study the protections of privacy definitions based on the concept of a row cone, which can be interpreted in terms of probabilistic inference.\nWe provided a characterization of utility measures that are suitable for measuring the quality of an algorithm (the information content of the data that it outputs while providing privacy protections).\nWe provided a characterization of \"optimal\" algorithms for differential privacy in terms of information content.\nWe provided new algorithms for estimating the degree distribution of a social network under privacy constraints.\nWe provided new attacks on legacy disclosure control methods such as data swapping.\nWe developed new algorithms for privacy-preserving \"count-of-counts\" histograms, which can be used to provide statistics about household size distribution in the U.S.\nWe also developed new algorithms for large-scale outlier detection that were able to label each data point with an estimate of the degree to which it is an outlier.\n\n\nThe results of the project were disseminated through academia and industry, including talks at the Census Bureau.  They were also used to analyze the effect of various policy decisions on the inferential properties of disclosure control methods that were under consideration for the 2020 Decennial Census.\n\nThe techniques developed in the research were also incorporated into machine learning courses and provided research opportunities for undergraduate students (some of whom went on to graduate study).\n\n\t\t\t\t\tLast Modified: 12/21/2018\n\n\t\t\t\t\tSubmitted by: Daniel Kifer"
 }
}