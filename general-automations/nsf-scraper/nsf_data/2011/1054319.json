{
 "awd_id": "1054319",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CAREER:  Flexible Learning for Natural Language Processing",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2011-02-01",
 "awd_exp_date": "2016-01-31",
 "tot_intn_awd_amt": 549812.0,
 "awd_amount": 565812.0,
 "awd_min_amd_letter_date": "2011-01-14",
 "awd_max_amd_letter_date": "2015-03-02",
 "awd_abstract_narration": "Statistical learning is now central to natural language processing\r\n(NLP).  Bridging the gap between learning and linguistic\r\nrepresentation requires going beyond learning parameters.  This CAREER\r\nproject addresses three challenging, unresolved questions:\r\n\r\n1. Given recent advances in learning the parameters of linguistic\r\nmodels and in approximate inference, how can the process of feature\r\ndesign be automated?\r\n\r\n2. Given that NLP tasks are often defined without recourse to real\r\napplications and that a specific annotated dataset is unlikely to\r\nfulfill the needs of multiple NLP projects, can learning frameworks be\r\nextended to perform automatic task refinement, simplifying a\r\nlinguistic analysis task to obtain more consistent, more precise, or\r\nfaster performance?\r\n\r\n3. Can computational models of language take into account the non-text\r\ncontext in which our linguistic data are embedded?  Building on recent\r\nsuccess in social text analysis and text-driven forecasting, this\r\nCAREER project seeks to exploit context to refine models of linguistic\r\nstructure while enabling advances in this application area.\r\n\r\nThis basic research supports advances in a wide range of language\r\nengineering applications and discrete data analysis.  In addition to\r\ncore research advances, this CAREER project contributes a new\r\npublicly-available parser that models the most consistently learnable\r\nelements of syntactic struture.  Educational activities include a new\r\nproject-based on text-driven forecasting within the PI's undergraduate\r\nNLP course and a new undergraduate course in machine learning. It\r\nsupports involvement by the PI in outreach activities to high school\r\nstudents and to a wider range of students at CMU by exposing aspects\r\nof his research in non-CS classrooms.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Noah",
   "pi_last_name": "Smith",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Noah A Smith",
   "pi_email_addr": "noah@allenai.org",
   "nsf_id": "000228357",
   "pi_start_date": "2011-01-14",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "5000 FORBES AVE",
  "perf_city_name": "PITTSBURGH",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133815",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1045",
   "pgm_ref_txt": "CAREER-Faculty Erly Career Dev"
  },
  {
   "pgm_ref_code": "1187",
   "pgm_ref_txt": "PECASE- eligible"
  },
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7945",
   "pgm_ref_txt": "DES AUTO FOR MICRO & NANO SYST"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0115",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001516DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 105536.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 110458.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 123614.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 129019.0
  },
  {
   "fund_oblg_fiscal_yr": 2015,
   "fund_oblg_amt": 97185.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This CAREER&nbsp;project addressed three challenging, unresolved technical questions.</p>\n<p><strong>Given recent advances in learning the parameters of linguistic&nbsp;models and in approximate inference, how can the process of feature&nbsp;design be automated?</strong></p>\n<p>A wide range of new learning algorithms for text data were introduced, building on ideas in multiple kernel learning, structured sparsity, new loss functions for structured prediction, and representation learning with recurrent neural networks. &nbsp;Together, these advances have increased automation in the development of systems for translation, parsing, and other natural language processing modules.</p>\n<p><strong>Given that NLP tasks are often defined without recourse to real&nbsp;applications and that a specific annotated dataset is unlikely to&nbsp;fulfill the needs of multiple NLP projects, can learning frameworks be&nbsp;extended to perform automatic task refinement, simplifying a&nbsp;linguistic analysis task to obtain more consistent, more precise, or&nbsp;faster performance?</strong></p>\n<p>A wide range of new inference algorithms that flexibly adapt to new datasets and prediction tasks involving linguistic structure were introduced. &nbsp;These include alternating directions dual decomposition, large-scale sociolinguistic discovery methods, new approaches to efficiently analyzing and disambiguating multiword expressions, new NLP tools targeting social media text, and the graph fragment language approach to rapid annotation of dependency and multiword structure by humans.</p>\n<p><strong>Can computational models of language take into account the non-text&nbsp;context in which our linguistic data are embedded?</strong></p>\n<p>A wide range of new data analysis methods that associate text with non-linguistic context were introduced. &nbsp;These include inferring a network of linguistic influence among American cities, detection of content censorship in microblogs, models of geographically situated language, prediction of sports outcomes from social media text, and models for online argumentation and sarcasm, among others.</p>\n<p>Beyond the technical goals of the project, contributions were made to more than ten international tutorials and short courses; seven graduate students and nine undergraduate researchers were trained in research at the junction of computer science, linguistics, and the social sciences. &nbsp;Software and datasets, as well as a public demo of algorithms developed, have been released for use by other researchers.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/23/2016<br>\n\t\t\t\t\tModified by: Noah&nbsp;A&nbsp;Smith</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThis CAREER project addressed three challenging, unresolved technical questions.\n\nGiven recent advances in learning the parameters of linguistic models and in approximate inference, how can the process of feature design be automated?\n\nA wide range of new learning algorithms for text data were introduced, building on ideas in multiple kernel learning, structured sparsity, new loss functions for structured prediction, and representation learning with recurrent neural networks.  Together, these advances have increased automation in the development of systems for translation, parsing, and other natural language processing modules.\n\nGiven that NLP tasks are often defined without recourse to real applications and that a specific annotated dataset is unlikely to fulfill the needs of multiple NLP projects, can learning frameworks be extended to perform automatic task refinement, simplifying a linguistic analysis task to obtain more consistent, more precise, or faster performance?\n\nA wide range of new inference algorithms that flexibly adapt to new datasets and prediction tasks involving linguistic structure were introduced.  These include alternating directions dual decomposition, large-scale sociolinguistic discovery methods, new approaches to efficiently analyzing and disambiguating multiword expressions, new NLP tools targeting social media text, and the graph fragment language approach to rapid annotation of dependency and multiword structure by humans.\n\nCan computational models of language take into account the non-text context in which our linguistic data are embedded?\n\nA wide range of new data analysis methods that associate text with non-linguistic context were introduced.  These include inferring a network of linguistic influence among American cities, detection of content censorship in microblogs, models of geographically situated language, prediction of sports outcomes from social media text, and models for online argumentation and sarcasm, among others.\n\nBeyond the technical goals of the project, contributions were made to more than ten international tutorials and short courses; seven graduate students and nine undergraduate researchers were trained in research at the junction of computer science, linguistics, and the social sciences.  Software and datasets, as well as a public demo of algorithms developed, have been released for use by other researchers.\n\n\t\t\t\t\tLast Modified: 04/23/2016\n\n\t\t\t\t\tSubmitted by: Noah A Smith"
 }
}