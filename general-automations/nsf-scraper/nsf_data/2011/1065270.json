{
 "awd_id": "1065270",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RI: Medium: Collaborative Research: Learning Representations of Language for Domain Adaptation",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Tatiana Korelsky",
 "awd_eff_date": "2011-04-01",
 "awd_exp_date": "2016-03-31",
 "tot_intn_awd_amt": 150000.0,
 "awd_amount": 200000.0,
 "awd_min_amd_letter_date": "2011-03-02",
 "awd_max_amd_letter_date": "2014-02-20",
 "awd_abstract_narration": "Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts.  A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and for the inability to generalize to previously unseen words. \r\n\r\nThis project is the first to systematically investigate representation-learning as a technique for improving performance on domain adaptation.  It explores latent-variable language models ? including Factorial Hidden Markov Models, dependency parsing models, and deep architectures ? as techniques for extracting novel features from text.  The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.  The project also explores novel procedures for training a language model, which incorporate Web-scale ngram statistics as substitutes for standard statistics used in unsupervised training.\r\n\r\nLanguage users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology.  By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality.  For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.  By involving the diverse student bodies at Temple University and Philadelphia-area high schools, the project helps to broaden participation in computer science research by underrepresented groups.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Douglas",
   "pi_last_name": "Downey",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Douglas C Downey",
   "pi_email_addr": "dougd@allenai.org",
   "nsf_id": "000534948",
   "pi_start_date": "2011-03-02",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Northwestern University",
  "inst_street_address": "633 CLARK ST",
  "inst_street_address_2": "",
  "inst_city_name": "EVANSTON",
  "inst_state_code": "IL",
  "inst_state_name": "Illinois",
  "inst_phone_num": "3125037955",
  "inst_zip_code": "602080001",
  "inst_country_name": "United States",
  "cong_dist_code": "09",
  "st_cong_dist_code": "IL09",
  "org_lgl_bus_name": "NORTHWESTERN UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "EXZVPWZBLUE8"
 },
 "perf_inst": {
  "perf_inst_name": "Northwestern University",
  "perf_str_addr": "633 CLARK ST",
  "perf_city_name": "EVANSTON",
  "perf_st_code": "IL",
  "perf_st_name": "Illinois",
  "perf_zip_code": "602080001",
  "perf_ctry_code": "US",
  "perf_cong_dist": "09",
  "perf_st_cong_dist": "IL09",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7495",
   "pgm_ref_txt": "ROBUST INTELLIGENCE"
  },
  {
   "pgm_ref_code": "7924",
   "pgm_ref_txt": "MEDIUM PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0113",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001314DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0114",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001415DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 100000.0
  },
  {
   "fund_oblg_fiscal_yr": 2013,
   "fund_oblg_amt": 50000.0
  },
  {
   "fund_oblg_fiscal_yr": 2014,
   "fund_oblg_amt": 50000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Supervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts. A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and the inability to generalize to previously unseen words.</p>\n<p>&nbsp;</p>\n<p>This project systematically investigated representation-learning as a technique for improving performance when adapting NLP systems to new domains. It explored latent-variable models, including factorial hidden Markov models, partial lattice Markov random fields, distributed hidden Markov models, probabilistic language adaptation models, and deep neural networks as techniques for extracting novel features from text. The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier.&nbsp;</p>\n<p>&nbsp;</p>\n<p>In our publications, we demonstrated that the new representation-learning techniques developed in this project improved the state-of-the-art in fundamental NLP tasks, such as part-of-speech tagging, dependency parsing and information extraction, when training data for the target domain is limited or absent.&nbsp; In addition to scientific publications summarizing the results, we also released a number of code bases for our new representation-learning approaches.</p>\n<p>&nbsp;</p>\n<p>Language users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology. By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality. For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts.&nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/17/2016<br>\n\t\t\t\t\tModified by: Douglas&nbsp;C&nbsp;Downey</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSupervised Natural Language Processing (NLP) systems perform poorly on domains and vocabulary that differ from training texts. A growing body of empirical and theoretical work points to the features used by traditional NLP systems as the culprit for domain-dependence and the inability to generalize to previously unseen words.\n\n \n\nThis project systematically investigated representation-learning as a technique for improving performance when adapting NLP systems to new domains. It explored latent-variable models, including factorial hidden Markov models, partial lattice Markov random fields, distributed hidden Markov models, probabilistic language adaptation models, and deep neural networks as techniques for extracting novel features from text. The resulting representations yield similar features for distributionally-similar words, thereby allowing generalization to words not seen during training of a classifier. \n\n \n\nIn our publications, we demonstrated that the new representation-learning techniques developed in this project improved the state-of-the-art in fundamental NLP tasks, such as part-of-speech tagging, dependency parsing and information extraction, when training data for the target domain is limited or absent.  In addition to scientific publications summarizing the results, we also released a number of code bases for our new representation-learning approaches.\n\n \n\nLanguage users are extraordinarily inventive, and new domains of discourse appear constantly, such as in specialized areas of science and technology. By building on top of the representations produced by this project, NLP systems can improve in accuracy on new domains and on Web text, bringing applications like the Semantic Web closer to reality. For resource-poor languages and domains, the project can help reduce the cost of annotating texts by reducing the need for broad coverage in the training texts. \n\n \n\n\t\t\t\t\tLast Modified: 06/17/2016\n\n\t\t\t\t\tSubmitted by: Douglas C Downey"
 }
}