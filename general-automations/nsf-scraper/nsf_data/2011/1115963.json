{
 "awd_id": "1115963",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Interior-point algorithms for conic optimization with sparse matrix cone constraints",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": "7032924488",
 "po_email": "jwang@nsf.gov",
 "po_sign_block_name": "Junping Wang",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2015-08-31",
 "tot_intn_awd_amt": 303100.0,
 "awd_amount": 303100.0,
 "awd_min_amd_letter_date": "2011-06-06",
 "awd_max_amd_letter_date": "2011-06-06",
 "awd_abstract_narration": "Conic optimization is an extension of linear programming in which the \r\ncomponentwise vector inequalities are replaced by inequalities with \r\nrespect to nonpolyhedral convex cones.  The conic optimization model is  \r\nwidely used in the recent literature on convex optimization and provides\r\nan elegant framework for extending interior-point algorithms from linear \r\nprogramming to convex optimization.  It is also the basis of popular \r\nmodeling systems for convex optimization.  \r\nThe research on algorithms for conic optimization has mainly focused on  \r\nthree types of inequalities, associated with the nonnegative orthant, \r\nthe second-order cone, and the positive semidefinite cone.  \r\nThis restriction is motivated by symmetry properties that can be exploited \r\nto formulate symmetric primal-dual interior-point algorithms.\r\nHowever, large gaps in linear algebra complexity exist between the \r\nthree types of conic constraints, and this can lead to inefficiencies when \r\nconvex optimization problems are converted to the standard conic format.  \r\nThis study considers approaches to improve the efficiency of conic optimization \r\nsolvers by considering a larger class of conic constraints, \r\ndefined by chordal sparse matrix cones, i.e., cones of positive \r\nsemidefinite matrices with a given chordal sparsity pattern, \r\nand the associated dual cones of chordal sparse matrices that \r\nhave a positive semidefinite completion.  These cones include as special \r\ncases the three standard cones, but also several interesting non-self-dual \r\ncones.  Moreover non-chordal sparsity patterns can often be efficiently \r\nembedded in chordal patterns and, as a consequence, sparse semidefinite \r\nprograms can be solved as non-symmetric cone programs involving \r\nlower-dimensional cones than the positive semidefinite cone used in  \r\nsemidefinite programming methods.  The choice for chordal matrix cones is  \r\nfurther motivated by the existence of fast algorithms for evaluating the \r\nassociated barrier functions and their derivatives.\r\nThe investigator and his collaborators study nonsymmetric \r\ninterior-point algorithms for sparse matrix cones, building on techniques \r\ndeveloped for large-scale sparse matrix computations, in particular, \r\nmultifrontal and supernodal factorization algorithms and parallel sparse \r\nmatrix algorithms.\r\n\r\nA wide variety of practical problems in engineering and science can be  \r\nformulated as nonlinear convex optimization problems, and solved using \r\nalgorithms developed over the last few decades.  \r\nThe success of these techniques has created a demand for robust and \r\nefficient algorithms for very large convex optimization problems, \r\nespecially for applications in machine learning, computer vision, \r\nelectronic design automation, sensor networks, and combinatorial \r\noptimization.  The problem sizes that arise in these fields often \r\nexceed the capabilities of general-purpose solvers.  \r\nThe work of the prinicipal investigator with his collaborators considers approaches to improve the scalability of interior-point\r\nalgorithms, an important class of convex optimization algorithms.\r\nFreely available high-quality software implementations of the techniques developed in the\r\nproject are a product of the research.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lieven",
   "pi_last_name": "Vandenberghe",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lieven Vandenberghe",
   "pi_email_addr": "vandenbe@ee.ucla.edu",
   "nsf_id": "000488711",
   "pi_start_date": "2011-06-06",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Los Angeles",
  "inst_street_address": "10889 WILSHIRE BLVD STE 700",
  "inst_street_address_2": "",
  "inst_city_name": "LOS ANGELES",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "3107940102",
  "inst_zip_code": "900244200",
  "inst_country_name": "United States",
  "cong_dist_code": "36",
  "st_cong_dist_code": "CA36",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, LOS ANGELES",
  "org_prnt_uei_num": "",
  "org_uei_num": "RN64EPNH8JC6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Los Angeles",
  "perf_str_addr": "10889 WILSHIRE BLVD STE 700",
  "perf_city_name": "LOS ANGELES",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "900244200",
  "perf_ctry_code": "US",
  "perf_cong_dist": "36",
  "perf_st_cong_dist": "CA36",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "127100",
   "pgm_ele_name": "COMPUTATIONAL MATHEMATICS"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "9263",
   "pgm_ref_txt": "COMPUTATIONAL SCIENCE & ENGING"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 303100.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Semidefinite optimization is an important class of convex optimization and has been studied for applications in control, signal processing, statistics, combinatorial optimization, and other disciplines.&nbsp;&nbsp; It is also used extensively in modeling systems for convex optimization, which are based on converting general convex optimization problems to equivalent semidefinite optimization problems (SDPs).&nbsp;&nbsp; Many of these applications of semidefinite optimization involve large problems that are difficult to solve by existing general-purpose implementations of interior-point algorithms.&nbsp; This project has contributed to algorithms for solving large sparse semidefinite optimization, by combining advances in convex optimization algorithms with techniques from sparse matrix theory and graph theory.<br /><br />As a first contribution, we developed fast algorithms for evaluating logarithmic barrier functions for cones of positive semidefinite matrices matrices with a given sparsity pattern, and the derivatives of the barrier functions.&nbsp; Efficient algorithms for these barrier computations are important as building blocks of interior-point methods for sparse semidefinite optimization.&nbsp; The techniques used in the algorithms extend multifrontal and supernodal algorithms for sparse Cholesky factorization.<br /><br />Second, we developed decomposition methods that make it possible to solve large sparse SDPs iteratively, by solving a sequence of quadratic semidefinite optimization problems that are less expensive to solve by interior-point methods than the full problem.&nbsp; For many problems, the quadratic subproblems are separable and reduce to independent smaller problems that can be solved in parallel.&nbsp; Related decomposition methods were also&nbsp; developed for certain classes of sparse matrix nearness problems.&nbsp; Here the problem is to fit to a given matrix a matrix with a specified sparsity pattern and with the additional property of being positive semidefinite, having a positive semidefinite completion, or having a Euclidean distance matrix completion. Problems of this type arise in statistics and signal processing, for example, in the problem of fitting a covariance matrix to&nbsp; estimates of a subset of its entries.&nbsp; In the decomposition approach these problems are reduced to solving a sequence of smaller dense eigenvalue problems.<br /><br />In a third component of the project we developed primal-dual strategies for decomposing general convex optimization problems, by applying splitting algorithms to the primal-dual optimality conditions.&nbsp; This can be viewed as an extension of dual decomposition techniques (for optimization problems that are separable after removal of coupling constraints) and primal decomposition (for problems that are separable except for a few coupling variables) to more general types&nbsp; of problem structure.&nbsp; As specific applications we considered problems of image deblurring via regularized convex optimization.</p>\n<p>The project has resulted in several publications, supplemented with software available from the PI's website.&nbsp; Some of the results have been incorporated into a graduate course on large-scale optimization at UCLA, and in lectures by the PI in short courses and summer schools.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/23/2015<br>\n\t\t\t\t\tModified by: Lieven&nbsp;Vandenberghe</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nSemidefinite optimization is an important class of convex optimization and has been studied for applications in control, signal processing, statistics, combinatorial optimization, and other disciplines.   It is also used extensively in modeling systems for convex optimization, which are based on converting general convex optimization problems to equivalent semidefinite optimization problems (SDPs).   Many of these applications of semidefinite optimization involve large problems that are difficult to solve by existing general-purpose implementations of interior-point algorithms.  This project has contributed to algorithms for solving large sparse semidefinite optimization, by combining advances in convex optimization algorithms with techniques from sparse matrix theory and graph theory.\n\nAs a first contribution, we developed fast algorithms for evaluating logarithmic barrier functions for cones of positive semidefinite matrices matrices with a given sparsity pattern, and the derivatives of the barrier functions.  Efficient algorithms for these barrier computations are important as building blocks of interior-point methods for sparse semidefinite optimization.  The techniques used in the algorithms extend multifrontal and supernodal algorithms for sparse Cholesky factorization.\n\nSecond, we developed decomposition methods that make it possible to solve large sparse SDPs iteratively, by solving a sequence of quadratic semidefinite optimization problems that are less expensive to solve by interior-point methods than the full problem.  For many problems, the quadratic subproblems are separable and reduce to independent smaller problems that can be solved in parallel.  Related decomposition methods were also  developed for certain classes of sparse matrix nearness problems.  Here the problem is to fit to a given matrix a matrix with a specified sparsity pattern and with the additional property of being positive semidefinite, having a positive semidefinite completion, or having a Euclidean distance matrix completion. Problems of this type arise in statistics and signal processing, for example, in the problem of fitting a covariance matrix to  estimates of a subset of its entries.  In the decomposition approach these problems are reduced to solving a sequence of smaller dense eigenvalue problems.\n\nIn a third component of the project we developed primal-dual strategies for decomposing general convex optimization problems, by applying splitting algorithms to the primal-dual optimality conditions.  This can be viewed as an extension of dual decomposition techniques (for optimization problems that are separable after removal of coupling constraints) and primal decomposition (for problems that are separable except for a few coupling variables) to more general types  of problem structure.  As specific applications we considered problems of image deblurring via regularized convex optimization.\n\nThe project has resulted in several publications, supplemented with software available from the PI's website.  Some of the results have been incorporated into a graduate course on large-scale optimization at UCLA, and in lectures by the PI in short courses and summer schools.\n\n\t\t\t\t\tLast Modified: 11/23/2015\n\n\t\t\t\t\tSubmitted by: Lieven Vandenberghe"
 }
}