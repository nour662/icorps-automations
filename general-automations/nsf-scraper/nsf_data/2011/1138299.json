{
 "awd_id": "1138299",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC:  Coordinating Communication: Visual, Social & Biological Factors in Grounding for Humans and Agents",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "William Bainbridge",
 "awd_eff_date": "2010-10-25",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 241611.0,
 "awd_amount": 272611.0,
 "awd_min_amd_letter_date": "2011-07-18",
 "awd_max_amd_letter_date": "2011-07-18",
 "awd_abstract_narration": "With the advent of increasing numbers of increasingly smart machines, there is a growing need to develop technologies that are not only smart, but sensitive to the people and the other machines around them, and sensitive to the context in which they are used.  Such an understanding will permit the development of technologies that can coordinate their interactions with humans in a more natural, seamless and fluid fashion. To meet these goals, this research program focuses on three critical yet under-studied contexts of interaction, each of which represents a different constraint upon interpersonal communication: (1) the physical context of shared visual access, (2) the social context of rapport, and (3) the biological context of aging. While some research has been conducted on each of these contextual factors, none has addressed their interaction, nor gathered them into one broader conception of the role of context in interpersonal coordination. This research applies a theory-driven design approach that includes experimental studies, theory development, computational modeling, system implementation and evaluation. In particular, the research program proposes: a) A rigorous study of human-to-human communication using elicitation experiments to develop a more detailed understanding of interpersonal communication across a range of contexts; b) A formalization of the findings into computationally explicit forms that provide predictions of behavior and capture the observed behavioral patterns; c) Integration of the models into a dialogue manager that is implemented within a larger computational architecture; and, d) Evaluation of the implemented system by having untrained humans interact with the system in such a way as to evaluate its effectiveness and reveal gaps in the underlying models as well as in our theoretical understanding.\r\n\r\nThe outcome of this research will advance our theoretical understanding of the role various contextual factors play during interpersonal communication. The results will be useful to a variety of scientific communities including those that study basic human communication (e.g., psychologists, linguists and communication researchers) and those that study interactive computational systems (e.g., computer scientists, computational linguists, and interaction designers). The research will also provide practical design guidelines and a general computational model that describes how machines can make intelligent choices on the basis of these contextual factors during everyday interactions. At a practical level, the general computational model can be applied by technologists developing many different technologies, such as embodied agents, large-scale displays, ubiquitous computing, in-car navigation, and assistive technologies for the elderly and those with cognitive impairments.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Justine",
   "pi_last_name": "Cassell",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Justine M Cassell",
   "pi_email_addr": "justine@cs.cmu.edu",
   "nsf_id": "000176817",
   "pi_start_date": "2011-07-18",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Carnegie-Mellon University",
  "inst_street_address": "5000 FORBES AVE",
  "inst_street_address_2": "",
  "inst_city_name": "PITTSBURGH",
  "inst_state_code": "PA",
  "inst_state_name": "Pennsylvania",
  "inst_phone_num": "4122688746",
  "inst_zip_code": "152133815",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "PA12",
  "org_lgl_bus_name": "CARNEGIE MELLON UNIVERSITY",
  "org_prnt_uei_num": "U3NKNFLNQ613",
  "org_uei_num": "U3NKNFLNQ613"
 },
 "perf_inst": {
  "perf_inst_name": "Carnegie-Mellon University",
  "perf_str_addr": "",
  "perf_city_name": "",
  "perf_st_code": "PA",
  "perf_st_name": "Pennsylvania",
  "perf_zip_code": "152133890",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "PA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  },
  {
   "pgm_ref_code": "9215",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING SYSTEMS"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  },
  {
   "pgm_ref_code": "HPCC",
   "pgm_ref_txt": "HIGH PERFORMANCE COMPUTING & COMM"
  }
 ],
 "app_fund": [
  {
   "app_code": "0107",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "490100",
   "fund_code": "app-0107",
   "fund_name": "",
   "fund_symb_id": ""
  },
  {
   "app_code": "0108",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01000809DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0110",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001011DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2007,
   "fund_oblg_amt": 241611.0
  },
  {
   "fund_oblg_fiscal_yr": 2008,
   "fund_oblg_amt": 15000.0
  },
  {
   "fund_oblg_fiscal_yr": 2010,
   "fund_oblg_amt": 16000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p><span>In this grant, we focused on three essential, under-studied interaction contexts, which represent different constraints on interpersonal coordination in communication. The first is the<span>&nbsp;</span><strong>physical context of shared visual</strong><span><strong>&nbsp;</strong></span><strong>access</strong>, where interlocutors must coordinate behavior as visual contexts update; second is <strong>social context of rapport</strong><span><strong>&nbsp;</strong></span>between individuals for whom each successive meeting may affect their coordinated behavior and communication; third, <strong>biological context of aging</strong><span><strong>&nbsp;</strong></span>as changes in visual acuity, cognitive demands, and social preferences change information grounding and conversation coordination. These contexts occur multiple times daily, whether giving directions face-to-face or by phone, asking a friend or an aide to purchase a prescription, and giving directions to a student or to our grandmother.</span></p>\n<p><span>We investigated<span>&nbsp;</span><strong>shared visual access</strong><span>&nbsp;</span>and its effects on grounding across several domains to best understand what fosters the most efficient collaboration. &nbsp;Grounding refers to the amount of shared understanding that is necessary at each point in order for a conversation to continue successfully. Air traffic controllers engage in high grounding due to high-stakes pressures, whereas friends in casual conversation often engage in low grounding.</span></p>\n<p><span>We ran three studies requiring participants to refer well enough for their interlocutor to correctly pick out the referred-to item using: (1) complex, ambiguous Lego&trade; structures; (2) collaborative puzzle solving with obscured or re-oriented views;&nbsp; (3) collaborating pairs with large, distant displays or small, close displays.&nbsp; We found that different visual environmental manipulations can speed up grounding for collaborating pairs and, therefore, cause faster rates of task completion (Table 1). &nbsp;</span></p>\n<p><span>These results were integrated into novel software, <em>Momentum</em>, which we developed to increase task completion for conversations without shared visual context (eg. online) by facilitating discussions on written prompts (Fig. 1). Overall, <em>Momentum</em><span>&nbsp;</span>groups spent nearly twice as long generating new ideas (rather than trying to explain their ideas to everyone) than baseline groups. <em>Momentum</em><span>&nbsp;</span>groups also developed more good ideas, as rated by external judges.&nbsp; These results suggest that this software is effective in the absence of shared visual grounding and is helpful in fostering efficient and productive group collaboration.</span></p>\n<p><span>To investigate the effect of <strong>social context</strong><span><strong>&nbsp;</strong></span>on survey respondents, we video-recorded professional survey interviewers administering a sensitive survey. &nbsp;Afterwards, interviewees were given another survey to judge the level of rapport they felt with their interviewer. We found that interviewers judged as &ldquo;high rapport&rdquo; by their interviewees (HRIs) smiled more during utterances and nodded significantly more than interviewers judged as low-rapport interviewers (LRIs).&nbsp; However, HRIs tended to gaze at respondents significantly<span>&nbsp;</span><em>less</em><span><em>&nbsp;</em></span>during conversation than LRIs.&nbsp; A closer look reveals that for sensitive questions, interviewers smiled slightly significantly less than for non-sensitive questions.&nbsp; These results indicate that understanding how nonverbal behavior affects rapport for particular conversational topics may promote the best levels of rapport between interviewer and interviewee for specific applications.</span></p>\n<p><span>We studied the role of<span>&nbsp;</span><...",
  "por_txt_cntn": "\nIn this grant, we focused on three essential, under-studied interaction contexts, which represent different constraints on interpersonal coordination in communication. The first is the physical context of shared visual access, where interlocutors must coordinate behavior as visual contexts update; second is social context of rapport between individuals for whom each successive meeting may affect their coordinated behavior and communication; third, biological context of aging as changes in visual acuity, cognitive demands, and social preferences change information grounding and conversation coordination. These contexts occur multiple times daily, whether giving directions face-to-face or by phone, asking a friend or an aide to purchase a prescription, and giving directions to a student or to our grandmother.\n\nWe investigated shared visual access and its effects on grounding across several domains to best understand what fosters the most efficient collaboration.  Grounding refers to the amount of shared understanding that is necessary at each point in order for a conversation to continue successfully. Air traffic controllers engage in high grounding due to high-stakes pressures, whereas friends in casual conversation often engage in low grounding.\n\nWe ran three studies requiring participants to refer well enough for their interlocutor to correctly pick out the referred-to item using: (1) complex, ambiguous Lego&trade; structures; (2) collaborative puzzle solving with obscured or re-oriented views;  (3) collaborating pairs with large, distant displays or small, close displays.  We found that different visual environmental manipulations can speed up grounding for collaborating pairs and, therefore, cause faster rates of task completion (Table 1).  \n\nThese results were integrated into novel software, Momentum, which we developed to increase task completion for conversations without shared visual context (eg. online) by facilitating discussions on written prompts (Fig. 1). Overall, Momentum groups spent nearly twice as long generating new ideas (rather than trying to explain their ideas to everyone) than baseline groups. Momentum groups also developed more good ideas, as rated by external judges.  These results suggest that this software is effective in the absence of shared visual grounding and is helpful in fostering efficient and productive group collaboration.\n\nTo investigate the effect of social context on survey respondents, we video-recorded professional survey interviewers administering a sensitive survey.  Afterwards, interviewees were given another survey to judge the level of rapport they felt with their interviewer. We found that interviewers judged as \"high rapport\" by their interviewees (HRIs) smiled more during utterances and nodded significantly more than interviewers judged as low-rapport interviewers (LRIs).  However, HRIs tended to gaze at respondents significantly less during conversation than LRIs.  A closer look reveals that for sensitive questions, interviewers smiled slightly significantly less than for non-sensitive questions.  These results indicate that understanding how nonverbal behavior affects rapport for particular conversational topics may promote the best levels of rapport between interviewer and interviewee for specific applications.\n\nWe studied the role of social context on learning by investigating how middle school peer tutors spoke to one another during algebra tutoring sessions. We discovered that, for friends, face threatening behaviors (such as insulting the partner) are correlated with higher learning gains while, for strangers, face threatening behaviors were negatively correlated with learning gains, possibly due to different behavioral interpretations.  Additionally, we paired a child with a virtual agent (Fig. 2) for two tutoring sessions across two days. As with child-child tutoring we found that negative social language (often insulting language) and second person references (\"you go..."
 }
}