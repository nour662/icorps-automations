{
 "awd_id": "1115242",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CGV: Small: Collaborative Research: Sparse Reconstruction and Frequency Analysis for Computer Graphics Rendering and Imaging",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 250000.0,
 "awd_amount": 250000.0,
 "awd_min_amd_letter_date": "2011-06-27",
 "awd_max_amd_letter_date": "2011-06-27",
 "awd_abstract_narration": "A broad range of problems in computer graphics rendering, appearance acquisition, and imaging, involve sampling, reconstruction, and integration of high-dimensional (4D-8D) signals.  Real-time rendering of glossy materials and intricate lighting effects like caustics, for example, can require pre-computing the response of the scene to different light and viewing directions, which is often a 6D dataset.  Similarly, image-based appearance acquisition of facial details, car paint, or glazed wood requires us to take images from different light and view directions.  Even offline rendering of visual effects like motion blur from a fast-moving car, or depth of field, involves high-dimensional sampling across time and lens aperture.  The same problems are also common in computational imaging applications such as light field cameras.  While the PIs and others have made significant progress in subsequent analysis and compact representation for some of these problems, the initial full dataset must almost always still be acquired or computed by brute force which is prohibitively expensive, taking hours to days of computation and acquisition time, as well as being a challenge for memory usage and storage.\r\n\r\nThe PIs' goal in this project is to make fundamental contributions that enable dramatically sparser sampling and reconstruction of these signals, before the full dataset is acquired or simulated.  The key idea is to exploit the structure of the data that often lies in lower-frequency, sparse, or low-dimensional spaces.  Their recent collaboration on a Fourier analysis of motion blur has shown that the frequency spectrum of dynamic scenes is sheared into a narrow wedge in the space-time domain.  This enables novel sheared (not axis-aligned) filters and a sparse sampling.  The PIs will build upon these preliminary results to develop a unified framework for frequency analysis and sparse data reconstruction of visual appearance in computer graphics.  To these ends, they will first lay the theoretical foundations, including a novel frequency analysis of Monte Carlo integration and 5D space-time analysis of light fields.  They will then develop efficient practical algorithms for a variety of problem domains, including sparse reconstruction of light transport matrices for relighting, sheared sampling and denoising for offline shadow rendering, time-coherent compressive sampling for appearance acquisition, and new approaches to computational photography and imaging.\r\n\r\nBroader Impacts:  From a theoretical perspective, this project will develop a fundamental signal-processing analysis of light transport and appearance and imaging datasets, which will provide the foundation for further work not just in computer graphics but in signal-processing, computer vision, and image analysis as well.  Project outcomes will apply to diverse sets of problems and will lead to transformative advances across the spectrum of rendering and imaging applications.  The PIs will leverage existing collaborations with industry to transition the new technologies to practical production use.  Outreach to K-12 students and the public will be enabled by a new science popularization blog that will leverage the public's excitement for advances in digital photography to introduce novel technical concepts, as well as by events such as the Computer Science Education Day for high school students at UC-Berkeley.  The new algorithms and datasets resulting from this work will be made available to the research community; moreover, imaging algorithms will be released in open-source format to work with consumer digital and cell-phone cameras.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Ravi",
   "pi_last_name": "Ramamoorthi",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Ravi Ramamoorthi",
   "pi_email_addr": "ravir@cs.ucsd.edu",
   "nsf_id": "000486826",
   "pi_start_date": "2011-06-27",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "1608 4TH ST STE 201",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947101749",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "745300",
   "pgm_ele_name": "GRAPHICS & VISUALIZATION"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7453",
   "pgm_ref_txt": "GRAPHICS & VISUALIZATION"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 250000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This NSF project is in the area of computer graphics rendering or image synthesis, creating computer-generated realistic images,indistinguishable from real photographs, a goal referred to as photorealism. &nbsp;Creating computer-generated realistic images is a long-time goal of the field, and has numerous applications from entertainment (the computer-generated special effects and video games that are now ubiquitous) to product visualization (online realistic depictions of articles for shopping) to virtual and augmented reality, to immersive experiences and training for firemen, pilots and doctors.</p>\n<p>In the past three decades, computer graphics has made great progress towards creating realistic images, but the methods involved are still slow, often taking hours to days. &nbsp;The main outcome of this project was a sequence of theoretical and practical advances designed to improve the efficiency of realistic image synthesis programs in computer graphics (technically, these are algorithms for methods based on techniques known as ray or path tracing). &nbsp;</p>\n<p>Our methods are based on adaptive sampling and filtering. Traditional computer graphics image synthesis algorithms work by distributing a number of samples (corresponding to actual paths of light) at a point or pixel in the image. &nbsp;By randomly choosing these paths and averaging over a number of samples, one converges to the true value corresponding to an actual physical simulation of the illumination inthe scene. &nbsp;We develop new theoretical analyses based on methods of&nbsp;multi-dimensional Fourier or frequency analysis that predict the bandwidth of various effects like shadows, depth of field and motion blur. &nbsp;These analyses allow us to sample adaptively, placing samples where there is likely to be the most contribution (actually fastest variations in image intensity). &nbsp;Moreover, by combining information from samples at multiple points or pixels in the image, we can filter information to produce smooth results from a very sparse set of samples.</p>\n<p>Methods based on sampling and filtering have resulted in dramatic reductions in the amount of time needed for creating realistic computer-generated images, by factors of 10-100. &nbsp;Indeed, results from this grant are widely cited in the literature and follow-ups have been the subject of entire sessions at the premier ACM SIGGRAPH computer graphics conference. &nbsp;One of the key advances over the current grant was that in some cases, sampling and filtering could be made fast enough to run at real-time frame rates, suitable for interactive visualization or games. &nbsp;This is a new frontier of real-time photorealistic rendering, which has long been a holy grail in computer graphics.</p>\n<p>The work has resulted in many key publications at the leading ACM SIGGRAPH and ACM SIGGRAPH Asia conferences (published in the ACMTransactions on Graphics), including papers at SIGGRAPH Asia 2011, SIGGRAPH Asia 2012, SIGGRAPH 2013, SIGGRAPH 2014, TOG 2015. &nbsp;Together, this is a suite of methods that makes extraordinary advances in the efficiency of algorithms for soft shadows (penumbras), indirect illumination (from multiple bounces of light), and combinations of effects such as out-of-focus objects, shadows, and indirect illumination. &nbsp;Our latest paper is a major advance, bringing all of these effects into the interactive or real-time domain. &nbsp;We have alsoapplied the methods towards the fast-growing area of mixed reality (2015 EuroGraphics Symposium on Rendering), which has potential for considerable impact given the planned headsets from Oculus, Google etc.</p>\n<p>Many of these recent developments are described in a EuroGraphics State of the Art Report 2015 by a number of authors including me. &nbsp;That work details the revolution in adaptive sampling and filtering, describing bodies of work that have now made a profound impact ...",
  "por_txt_cntn": "\nThis NSF project is in the area of computer graphics rendering or image synthesis, creating computer-generated realistic images,indistinguishable from real photographs, a goal referred to as photorealism.  Creating computer-generated realistic images is a long-time goal of the field, and has numerous applications from entertainment (the computer-generated special effects and video games that are now ubiquitous) to product visualization (online realistic depictions of articles for shopping) to virtual and augmented reality, to immersive experiences and training for firemen, pilots and doctors.\n\nIn the past three decades, computer graphics has made great progress towards creating realistic images, but the methods involved are still slow, often taking hours to days.  The main outcome of this project was a sequence of theoretical and practical advances designed to improve the efficiency of realistic image synthesis programs in computer graphics (technically, these are algorithms for methods based on techniques known as ray or path tracing).  \n\nOur methods are based on adaptive sampling and filtering. Traditional computer graphics image synthesis algorithms work by distributing a number of samples (corresponding to actual paths of light) at a point or pixel in the image.  By randomly choosing these paths and averaging over a number of samples, one converges to the true value corresponding to an actual physical simulation of the illumination inthe scene.  We develop new theoretical analyses based on methods of multi-dimensional Fourier or frequency analysis that predict the bandwidth of various effects like shadows, depth of field and motion blur.  These analyses allow us to sample adaptively, placing samples where there is likely to be the most contribution (actually fastest variations in image intensity).  Moreover, by combining information from samples at multiple points or pixels in the image, we can filter information to produce smooth results from a very sparse set of samples.\n\nMethods based on sampling and filtering have resulted in dramatic reductions in the amount of time needed for creating realistic computer-generated images, by factors of 10-100.  Indeed, results from this grant are widely cited in the literature and follow-ups have been the subject of entire sessions at the premier ACM SIGGRAPH computer graphics conference.  One of the key advances over the current grant was that in some cases, sampling and filtering could be made fast enough to run at real-time frame rates, suitable for interactive visualization or games.  This is a new frontier of real-time photorealistic rendering, which has long been a holy grail in computer graphics.\n\nThe work has resulted in many key publications at the leading ACM SIGGRAPH and ACM SIGGRAPH Asia conferences (published in the ACMTransactions on Graphics), including papers at SIGGRAPH Asia 2011, SIGGRAPH Asia 2012, SIGGRAPH 2013, SIGGRAPH 2014, TOG 2015.  Together, this is a suite of methods that makes extraordinary advances in the efficiency of algorithms for soft shadows (penumbras), indirect illumination (from multiple bounces of light), and combinations of effects such as out-of-focus objects, shadows, and indirect illumination.  Our latest paper is a major advance, bringing all of these effects into the interactive or real-time domain.  We have alsoapplied the methods towards the fast-growing area of mixed reality (2015 EuroGraphics Symposium on Rendering), which has potential for considerable impact given the planned headsets from Oculus, Google etc.\n\nMany of these recent developments are described in a EuroGraphics State of the Art Report 2015 by a number of authors including me.  That work details the revolution in adaptive sampling and filtering, describing bodies of work that have now made a profound impact inindustry.  My original contributions (papers in 2009 on frequency analysis for motion blur and adaptive wavelet rendering) are cited as initial works that launched t..."
 }
}