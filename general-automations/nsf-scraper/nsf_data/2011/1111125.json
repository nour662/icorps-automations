{
 "awd_id": "1111125",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "HCC: Large: Collaborative Research: Human-Robot Dialog for Collaborative Navigation Tasks",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Ephraim Glinert",
 "awd_eff_date": "2011-08-15",
 "awd_exp_date": "2015-07-31",
 "tot_intn_awd_amt": 408903.0,
 "awd_amount": 416903.0,
 "awd_min_amd_letter_date": "2011-08-11",
 "awd_max_amd_letter_date": "2012-04-11",
 "awd_abstract_narration": "This research involves collaboration among investigators at three institutions.  The PIs anticipate a future in which humans and intelligent robots will collaborate on shared tasks.  To achieve this vision, a robot must have sufficiently rich knowledge of the task domain and that knowledge must be usable in ways that support effective communication between a human and the robot.  Navigational space is one of the few task domains where the structure of the knowledge is sufficiently well understood for a physically-embodied robot agent to be a useful collaborator, meeting genuine human needs.  In this project, the PIs will develop and evaluate an intelligent robot capable of being genuinely useful to a human, and capable of natural dialog with a human about their shared task.\r\n\r\nThe Hybrid Spatial Semantic Hierarchy (HSSH) is a human-inspired multi-ontology representation for knowledge of navigational space.  The spatial representations in the HSSH provide for efficient incremental learning, graceful degradation under resource limitations, and natural interfaces for different kinds of human-robot interactions.  Speech is a natural though demanding way to use natural language to communicate with a robot.  To maintain real-time performance, natural language understanding must be organized to minimize the amount of backtracking from early conclusions in light of later information.  This project will answer three scientific questions.\r\n\r\n(1) Can the HSSH framework, extended with real-time computer vision, express the kinds of knowledge of natural human environments that are relevant to navigation tasks? \r\n(2) Can the HSSH representation support effective natural language communication in the spatial navigation domain? \r\n3) Can we develop effective human-robot interaction that meets the needs of a person and improves the performance of the system?\r\n\r\nTo these ends, the PIs will perform this research with two different kinds of navigational robots, each learning from its travel experiences and building an increasingly sophisticated cognitive map: an intelligent robotic wheelchair which carries its human driver to desired destinations, and a telepresence robot that transmits its perceptions to a remote human driver as it navigates within an environment so the driver can achieve virtual presence and communicate with others remotely.  To inform the design process, the PIs will conduct focus groups with potential users.  They will also evaluate their implemented systems throughout the process, creating an iterative design-test cycle.\r\n\r\nBroader Impacts:  To be successful, an intelligent robot must not only be able to perceive the world, represent what it learns, make useful inferences and plans, and act effectively.  It must also be able to communicate effectively with other agents, and particularly with people.  This confluence among grounded knowledge representation, situated natural language understanding, and human-robot interaction is intellectually fundamental, and is the focus of this research.  Since the domain of spatial knowledge is foundational for virtually all aspects of human knowledge, project outcomes will have broad applicability.  This work will create technologies for mobility assistance for people with disabilities in perception (blindness or low vision), cognition (developmental delay or dementia), or general frailty (old age).  It will also support telepresence applications such as telecommuting, telemedicine and search and rescue.  The project includes outreach to K-12 and community college students, K-12 teachers, and the public in a number of venues.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Holly",
   "pi_last_name": "Yanco",
   "pi_mid_init": "A",
   "pi_sufx_name": "",
   "pi_full_name": "Holly A Yanco",
   "pi_email_addr": "holly@cs.uml.edu",
   "nsf_id": "000278965",
   "pi_start_date": "2011-08-11",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Massachusetts Lowell",
  "inst_street_address": "220 PAWTUCKET ST STE 400",
  "inst_street_address_2": "",
  "inst_city_name": "LOWELL",
  "inst_state_code": "MA",
  "inst_state_name": "Massachusetts",
  "inst_phone_num": "9789344170",
  "inst_zip_code": "018543573",
  "inst_country_name": "United States",
  "cong_dist_code": "03",
  "st_cong_dist_code": "MA03",
  "org_lgl_bus_name": "UNIVERSITY OF MASSACHUSETTS LOWELL",
  "org_prnt_uei_num": "",
  "org_uei_num": "LTNVSTJ3R6D5"
 },
 "perf_inst": {
  "perf_inst_name": "University of Massachusetts Lowell",
  "perf_str_addr": "220 PAWTUCKET ST STE 400",
  "perf_city_name": "LOWELL",
  "perf_st_code": "MA",
  "perf_st_name": "Massachusetts",
  "perf_zip_code": "018543573",
  "perf_ctry_code": "US",
  "perf_cong_dist": "03",
  "perf_st_cong_dist": "MA03",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "736400",
   "pgm_ele_name": "Info Integration & Informatics"
  },
  {
   "pgm_ele_code": "736700",
   "pgm_ele_name": "HCC-Human-Centered Computing"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7364",
   "pgm_ref_txt": "INFO INTEGRATION & INFORMATICS"
  },
  {
   "pgm_ref_code": "7367",
   "pgm_ref_txt": "Cyber-Human Systems"
  },
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 408903.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 8000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Through this award, we developed a semi-autonomous telepresence robot system for use by people with motor and/or cognitive disabilities. Telepresence robots allow people to travel to distant places without having to get into a car; they can be thought of a video chat application on wheels. The less expensive telepresence robot systems on the market must be fully teleoperated, meaning that the person using it must be able to control the movement of the robot in the remote environment. Such teleoperation requires someone to focus on the driving of the robot. For people who have a cognitive or motor disability, teleoperation can be very difficult. By developing a semi-autonomous telepresence robot system and a touch-based user interface, we have enabled people to use a telepresence robot who might not have been able to use one before.&nbsp;</p>\n<p>First, we designed and built the robot system. We started with a VGo Communications telepresence robot platform. As sold, this platform must be teleoperated. We added additional computer processing and sensors to the platform, to allow it to move autonomously to a location specified on the user interface. We also added two cameras to the robot, which allowed the remote user to have a wider field of view from the base of the robot up to the walls of the space that the robot was moving through. Four robots were built for this award. The first was an initial prototype. Three of the second design were built; one was provided to the University of Michigan and another to Tufts University, both of whom were collaborating institutions on this award. The third robot stayed with us at the University of Massachusetts Lowell for our research.</p>\n<p>Next, we developed the user interface for the robot system. We worked closely with therapists at the Crotched Mountain Rehabilitation Center in Greenfield, NH to design and program a user interface that would be easy to use for their clients who have motor and/or cognitive disabilities.&nbsp;</p>\n<p>Finally, we conducted human subjects studies to validate the robot system. The studies showed that the system was not only easily used by people with motor and/or cognitive disabilities, but also showed that children were also able to use the robot system.</p>\n<p>A robot system like this one could allow people to be able to go to many remote locations without leaving home. Such systems could be placed in museums, in schools, in theaters, and other such locations for people to connect into from their homes, to allow people who can't leave their houses to be able to travel throughout the world.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 06/15/2016<br>\n\t\t\t\t\tModified by: Holly&nbsp;A&nbsp;Yanco</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2016/1111125/1111125_10120834_1466002866491_margo_tom_2-nofaces--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2016/1111125/1111125_10120834_1466002866491_margo_tom_2-nofaces--rgov-800width.jpg\" title=\"Robots developed through this award\"><img src=\"/por/images/Reports/POR/2016/1111125/1111125_10120834_1466002866491_margo_tom_2-nofaces--rgov-66x44.jpg\" alt=\"Robots developed through this award\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">This photo shows three of the four robots that were developed under this award. The one with the tie is the first generation of the system. The ones with the Hawaiian shirts are two of three of the second generation robots...",
  "por_txt_cntn": "\nThrough this award, we developed a semi-autonomous telepresence robot system for use by people with motor and/or cognitive disabilities. Telepresence robots allow people to travel to distant places without having to get into a car; they can be thought of a video chat application on wheels. The less expensive telepresence robot systems on the market must be fully teleoperated, meaning that the person using it must be able to control the movement of the robot in the remote environment. Such teleoperation requires someone to focus on the driving of the robot. For people who have a cognitive or motor disability, teleoperation can be very difficult. By developing a semi-autonomous telepresence robot system and a touch-based user interface, we have enabled people to use a telepresence robot who might not have been able to use one before. \n\nFirst, we designed and built the robot system. We started with a VGo Communications telepresence robot platform. As sold, this platform must be teleoperated. We added additional computer processing and sensors to the platform, to allow it to move autonomously to a location specified on the user interface. We also added two cameras to the robot, which allowed the remote user to have a wider field of view from the base of the robot up to the walls of the space that the robot was moving through. Four robots were built for this award. The first was an initial prototype. Three of the second design were built; one was provided to the University of Michigan and another to Tufts University, both of whom were collaborating institutions on this award. The third robot stayed with us at the University of Massachusetts Lowell for our research.\n\nNext, we developed the user interface for the robot system. We worked closely with therapists at the Crotched Mountain Rehabilitation Center in Greenfield, NH to design and program a user interface that would be easy to use for their clients who have motor and/or cognitive disabilities. \n\nFinally, we conducted human subjects studies to validate the robot system. The studies showed that the system was not only easily used by people with motor and/or cognitive disabilities, but also showed that children were also able to use the robot system.\n\nA robot system like this one could allow people to be able to go to many remote locations without leaving home. Such systems could be placed in museums, in schools, in theaters, and other such locations for people to connect into from their homes, to allow people who can't leave their houses to be able to travel throughout the world.\n\n\t\t\t\t\tLast Modified: 06/15/2016\n\n\t\t\t\t\tSubmitted by: Holly A Yanco"
 }
}