{
 "awd_id": "1137229",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "EFRI-M3C: Partnered Rehabilitative Movement: Cooperative Human-robot Interactions for Motor Assistance, Learning, and Communication",
 "cfda_num": "47.041",
 "org_code": "07040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Radhakisan Baheti",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2017-08-31",
 "tot_intn_awd_amt": 2000000.0,
 "awd_amount": 2000000.0,
 "awd_min_amd_letter_date": "2011-09-01",
 "awd_max_amd_letter_date": "2011-09-01",
 "awd_abstract_narration": "Our vision is to develop caregiver robots that interact fluidly and flexibly with humans during functional\r\nmotor activities, while providing motor assistance, enhancement, and communication to facilitate motor\r\nlearning. However, we currently lack theories to understand how rehabilitation and movement therapists\r\nprovide timely and appropriate physical feedback and assistance to improve mobility in individuals with\r\nmotor impairments. To develop devices that could accompany an individual as both assistant and\r\nmovement therapist, our goal is to study human motor coordination during cooperative physical\r\ninteractions with a humanoid assistive robot. We will use rehabilitative partner dance as a paradigm to\r\nexamine a sensory-motor theory of cooperative physical interactions relevant to walking and other\r\nfunctional motor activities. We will use a \"partnered box step\", a constrained and defined pattern of\r\nweight shifts and directional changes, as a paradigm for a cooperative physical interaction with welldefined\r\nmotor goals.\r\nObjectives: To 1) experimentally verify a hierarchical theory of human sensory-motor control and\r\nlearning and 2) develop predictive models of whole-body human movement for cooperative physical\r\ninteractions with machines. Over four years, we will test our models by demonstrating the successful\r\nparticipation of the robot in a box step as leader or follower and adapt its movements to the motor skill\r\nlevel of a human partner.\r\nIntellectual Merit: Our work will provide transformative experimental, theoretical, and practical\r\ninterdisciplinary frameworks that will forge new paths toward autonomous cooperative robots with\r\nphysical intelligence to enhance, assist, and improve motor skills in humans with varying motor\r\ncapabilities. These advances will aid prosthetic and robotic design and may advance our understanding of\r\nthe brain.\r\nBroader Impacts: The expected project outcomes would have long-term impact on the quality of life of\r\nmillions of Americans by improving fitness, motor skills, and social engagement. Applications include\r\nhealthcare devices or sports robots that entertain and improve fitness. We will provide seminars on\r\nmobility-related issues and rehabilitative dance instruction to older adult living communities and\r\npopulations with motor impairments. The broad appeal and social nature of this work will likely garner\r\nmedia publicity that will increase public interest in science and technology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "EFMA",
 "org_div_long_name": "Office of Emerging Frontiers in Research and Innovation (EFRI)",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lena",
   "pi_last_name": "Ting",
   "pi_mid_init": "H",
   "pi_sufx_name": "",
   "pi_full_name": "Lena H Ting",
   "pi_email_addr": "lting@emory.edu",
   "nsf_id": "000095985",
   "pi_start_date": "2011-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Karen",
   "pi_last_name": "Liu",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Karen Liu",
   "pi_email_addr": "karenliu@cs.stanford.edu",
   "nsf_id": "000430108",
   "pi_start_date": "2011-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Charles",
   "pi_last_name": "Kemp",
   "pi_mid_init": "C",
   "pi_sufx_name": "",
   "pi_full_name": "Charles C Kemp",
   "pi_email_addr": "charlie.kemp@bme.gatech.edu",
   "nsf_id": "000367160",
   "pi_start_date": "2011-09-01",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Madeleine",
   "pi_last_name": "Hackney",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Madeleine Hackney",
   "pi_email_addr": "mehackn@emory.edu",
   "nsf_id": "000579506",
   "pi_start_date": "2011-09-01",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Emory University",
  "inst_street_address": "201 DOWMAN DR NE",
  "inst_street_address_2": "",
  "inst_city_name": "ATLANTA",
  "inst_state_code": "GA",
  "inst_state_name": "Georgia",
  "inst_phone_num": "4047272503",
  "inst_zip_code": "303221061",
  "inst_country_name": "United States",
  "cong_dist_code": "05",
  "st_cong_dist_code": "GA05",
  "org_lgl_bus_name": "EMORY UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "S352L5PJLMP8"
 },
 "perf_inst": {
  "perf_inst_name": "Georgia Institute of Technology",
  "perf_str_addr": "313 Ferst Dr NW",
  "perf_city_name": "Atlanta",
  "perf_st_code": "GA",
  "perf_st_name": "Georgia",
  "perf_zip_code": "303180535",
  "perf_ctry_code": "US",
  "perf_cong_dist": "05",
  "perf_st_cong_dist": "GA05",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "763300",
   "pgm_ele_name": "EFRI Research Projects"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7633",
   "pgm_ref_txt": "EFRI RESEARCH PROJECTS"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 2000000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Our highly collaborative interdisciplinary team advanced both fundamental science and engineering toward the yet-unrealized vision of seamless physical interactions between humans and machines to improve our work, health, and recreation. &nbsp;The research team had expertise spanning mobile robotics, human-robot interaction, human biomechanics and neuroscience, physical rehabilitation, and computer animation. Our focus was on autonomous (non-wearable) robots that can touch and interact physically with humans.</p>\n<p>Our work provides fundamental new knowledge about how humans can interact physically at the hands to coordinate their walking with a robot or another person to improve their gait. Specifically, we showed that a humanoid robot move in coordination with a person simply by the forces at the hand. We found that relatively small forces at the hands could communicate rich information about intended movement direction and speed, skill level, and movement errors between two people in the absence of visual or auditory cues. Further, we showed that older adults are accepting of robot technologies for exercise-based robotic rehabilitation.</p>\n<p>We advanced technologies and algorithms to improve the ability of robots to physically interact with humans and explore unknown objects in the environment. We developed sensors and algorithms to allow a robot to explore a cluttered environment by pushing on objects, inferring their properties, and altering their movement to adapt to soft and hard objects. Further, a novel robotic sensing device allowed humans and objects to be distinguished through touch using force and thermal sensing.</p>\n<p>To improve the ability to design more intuitive physical interactions between humans and robots, we developed computer simulation algorithm of physical interactions between humans and both stationary and moving objects.</p>\n<p>Finally, as a simple example of an intuitive assistive physical interaction between a human and machine during walking, we developed a prototype of energy-recycling assistive stairs that store and return energy to a human user. The stairs store energy during when walking down the steps, cushioning the descent, and release energy when walking up the steps, assisting the ascent. &nbsp;</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 11/22/2017<br>\n\t\t\t\t\tModified by: Lena&nbsp;H&nbsp;Ting</p>\n</div>\n<div class=\"porSideCol\">\n<div class=\"each-gallery\">\n<div class=\"galContent\" id=\"gallery0\">\n<div class=\"photoCount\" id=\"photoCount0\">\n\t\t\t\t\t\t\t\t\tImages (<span id=\"selectedPhoto0\">1</span> of <span class=\"totalNumber\"></span>)\t\t\n\t\t\t\t\t\t\t\t</div>\n<div class=\"galControls\" id=\"controls0\"></div>\n<div class=\"galSlideshow\" id=\"slideshow0\"></div>\n<div class=\"galEmbox\" id=\"embox\">\n<div class=\"image-title\"></div>\n</div>\n</div>\n<div class=\"galNavigation\" id=\"navigation0\">\n<ul class=\"thumbs\" id=\"thumbs0\">\n<li>\n<a href=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386681840_DSC_0202--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386681840_DSC_0202--rgov-800width.jpg\" title=\"Energy-Recycling Assistive Stairs\"><img src=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386681840_DSC_0202--rgov-66x44.jpg\" alt=\"Energy-Recycling Assistive Stairs\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Energy-recycling assistive stairs reduce joint torque in both ascending and descending stairs.From: Song, Y.S., Ha, S., Shu, H., Ting, L.H., Liu, C.K. (2017) Stair Negotiation Made Easier Using Novel Interactive Energy-Recycling Assistive Stairs, PLoS ONE, Jul 12;12(7):e0179637</div>\n<div class=\"imageCredit\">Y.S. Song</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Lena&nbsp;H&nbsp;Ting</div>\n<div class=\"imageTitle\">Energy-Recycling Assistive Stairs</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386342484_human-humanstepping--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386342484_human-humanstepping--rgov-800width.jpg\" title=\"Human-Human Partnered Stepping\"><img src=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386342484_human-humanstepping--rgov-66x44.jpg\" alt=\"Human-Human Partnered Stepping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Small force interactions at the hand communicate rich information to coordinate stepping between two humans. From Sawers, A., Bhattacharjee, T., McKay, J.L., Hackney, M.E., Kemp C.C., Ting, L.H. (2017) Small Forces Can Communicate Movement Goals and Distinguish Expert and Novice Human-Human Phy</div>\n<div class=\"imageCredit\">A. Sawers</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Lena&nbsp;H&nbsp;Ting</div>\n<div class=\"imageTitle\">Human-Human Partnered Stepping</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386551592_OAAcceptance--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386551592_OAAcceptance--rgov-800width.jpg\" title=\"Older Adult Human-Robot Partnered Stepping\"><img src=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386551592_OAAcceptance--rgov-66x44.jpg\" alt=\"Older Adult Human-Robot Partnered Stepping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">Older adults are accepting of a  robot for exercised-based therapies to improve mobility. From: Chen, T.L., Beer, J.M., Bhattacharjee, T., Ting, L.H., Hackney, Rogers, W.A, Kemp, C.C. (2017) Older Adults? Acceptance of a Robot for Partner Dance-Based Exer1.Exercise. PLoS ONE, \u00a012 (10), e0182736</div>\n<div class=\"imageCredit\">T. Chen</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Lena&nbsp;H&nbsp;Ting</div>\n<div class=\"imageTitle\">Older Adult Human-Robot Partnered Stepping</div>\n</div>\n</li>\n<li>\n<a href=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386105798_Figure1_exp_setup--rgov-214x142.jpg\" original=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386105798_Figure1_exp_setup--rgov-800width.jpg\" title=\"Human-Robot Partnered Stepping\"><img src=\"/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386105798_Figure1_exp_setup--rgov-66x44.jpg\" alt=\"Human-Robot Partnered Stepping\"></a>\n<div class=\"imageCaptionContainer\">\n<div class=\"imageCaption\">A robot followers moves with a human leader based solely on force interactions at the hands.From Chen, T.L., Bhattacharjee, T., McKay, J.L., Hackney, M.E., Borinski, Ting, L.H., Kemp, C.C., (2015) Dance with me: Human-robot partnered stepping based on haptic communication. PLoS ONE, 10(5):e01251</div>\n<div class=\"imageCredit\">T. Chen</div>\n<div class=\"imagePermisssions\">Creative Commons</div>\n<div class=\"imageSubmitted\">Lena&nbsp;H&nbsp;Ting</div>\n<div class=\"imageTitle\">Human-Robot Partnered Stepping</div>\n</div>\n</li>\n</ul>\n</div>\n</div>\n</div>\n</div>",
  "por_txt_cntn": "\nOur highly collaborative interdisciplinary team advanced both fundamental science and engineering toward the yet-unrealized vision of seamless physical interactions between humans and machines to improve our work, health, and recreation.  The research team had expertise spanning mobile robotics, human-robot interaction, human biomechanics and neuroscience, physical rehabilitation, and computer animation. Our focus was on autonomous (non-wearable) robots that can touch and interact physically with humans.\n\nOur work provides fundamental new knowledge about how humans can interact physically at the hands to coordinate their walking with a robot or another person to improve their gait. Specifically, we showed that a humanoid robot move in coordination with a person simply by the forces at the hand. We found that relatively small forces at the hands could communicate rich information about intended movement direction and speed, skill level, and movement errors between two people in the absence of visual or auditory cues. Further, we showed that older adults are accepting of robot technologies for exercise-based robotic rehabilitation.\n\nWe advanced technologies and algorithms to improve the ability of robots to physically interact with humans and explore unknown objects in the environment. We developed sensors and algorithms to allow a robot to explore a cluttered environment by pushing on objects, inferring their properties, and altering their movement to adapt to soft and hard objects. Further, a novel robotic sensing device allowed humans and objects to be distinguished through touch using force and thermal sensing.\n\nTo improve the ability to design more intuitive physical interactions between humans and robots, we developed computer simulation algorithm of physical interactions between humans and both stationary and moving objects.\n\nFinally, as a simple example of an intuitive assistive physical interaction between a human and machine during walking, we developed a prototype of energy-recycling assistive stairs that store and return energy to a human user. The stairs store energy during when walking down the steps, cushioning the descent, and release energy when walking up the steps, assisting the ascent.  \n\n \n\n\t\t\t\t\tLast Modified: 11/22/2017\n\n\t\t\t\t\tSubmitted by: Lena H Ting"
 }
}