{
 "awd_id": "1111328",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RI: Large: Collaborative Research: 3D Structure and Motion in Dynamic Natural Scenes",
 "cfda_num": "47.070",
 "org_code": "05020000",
 "po_phone": "7032925149",
 "po_email": "kwhang@nsf.gov",
 "po_sign_block_name": "Kenneth Whang",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2016-08-31",
 "tot_intn_awd_amt": 302048.0,
 "awd_amount": 302048.0,
 "awd_min_amd_letter_date": "2011-08-26",
 "awd_max_amd_letter_date": "2011-08-26",
 "awd_abstract_narration": "How does a vision system recover the 3-dimensional structure of the world -- such as the layout of the environment, surface shape, or object motion -- from the dynamic 2-dimensional images received by the sensors in a camera, or the retinas in our eyes?  This problem is fundamental to both computer and biological vision.  Computer vision has developed a variety of algorithms for estimating specific aspects of a scene such as the 3-dimensional positions of points whose correspondence over time can be established, but obtaining complete and robust scene representations for complex natural scenes and viewing conditions remains a challenge.  Biological vision systems have evolved impressive capabilities that suggest they have detailed and robust representations of the 3-dimensional world, but the neural representations that subserve this are poorly understood and neurophysiological studies thus far have provided little insight into the computational process.  This project will pursue an interdisciplinary approach by attempting the understand the universal principles that lie at the heart of 3-dimensional scene analysis.\r\n\r\nSpecifically, the project will  1) develop a novel class of computational models that recover and represent 3-dimensional scene information, 2) collect high quality video and range data of dynamic natural scenes under a variety of controlled motion conditions, and 3) test the perceptual implications of these models in psychophysical experiments.  The computational models will utilize non-linear decomposition - i.e., the ability to explain complex, time-varying images in terms of the non-linear interaction of multiple factors, such as the interaction between observer motion, the 3-dimensional scene layout, and surface patterns.  Importantly, the components of these models will be adapted to the statistics of natural motion patterns that arise from observer motion through natural scenes and movement around points of fixation.\r\n\r\nThe project is a collaboration between three laboratories that have played a leading role in developing theoretical models of natural image statistics, visual neural representations, and perceptual processes.  The investigators seek to combine their efforts to develop new models, data sets, and characterizations of 3-dimensional natural scene structure that go beyond previous studies of natural image statistics, and that can be tested in neurophysiological and psychophysical experiments.  This project has the potential to bring about fundamental advances in neuroscience, visual perception, and computer vision by developing new classes of models that robustly infer representations of the 3-dimensional natural environment.  It will create a set of high quality databases that will be made available to help other investigators study these issues.  It will also open up new possibilities for generating realistic stimuli that can guide novel investigations of neural representation and processing.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "IIS",
 "org_div_long_name": "Division of Information & Intelligent Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Wilson",
   "pi_last_name": "Geisler",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Wilson S Geisler",
   "pi_email_addr": "w.geisler@utexas.edu",
   "nsf_id": "000472439",
   "pi_start_date": "2011-08-26",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Texas at Austin",
  "inst_street_address": "110 INNER CAMPUS DR",
  "inst_street_address_2": "",
  "inst_city_name": "AUSTIN",
  "inst_state_code": "TX",
  "inst_state_name": "Texas",
  "inst_phone_num": "5124716424",
  "inst_zip_code": "787121139",
  "inst_country_name": "United States",
  "cong_dist_code": "25",
  "st_cong_dist_code": "TX25",
  "org_lgl_bus_name": "UNIVERSITY OF TEXAS AT AUSTIN",
  "org_prnt_uei_num": "",
  "org_uei_num": "V6AFQPN18437"
 },
 "perf_inst": {
  "perf_inst_name": "University of Texas at Austin",
  "perf_str_addr": "110 INNER CAMPUS DR",
  "perf_city_name": "AUSTIN",
  "perf_st_code": "TX",
  "perf_st_name": "Texas",
  "perf_zip_code": "787121139",
  "perf_ctry_code": "US",
  "perf_cong_dist": "25",
  "perf_st_cong_dist": "TX25",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "749500",
   "pgm_ele_name": "Robust Intelligence"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7925",
   "pgm_ref_txt": "LARGE PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 302048.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Humans and many other animals are able to estimate the three-dimensional (3D) structure of the environment around them from the two-dimensional (2D) images captured by the eyes.&nbsp; Much of what we know about how the human brain accomplishes this remarkable feat has been obtained in experiments with simple laboratory stimuli.&nbsp; However, the ultimate goal is to understand how the brain estimates 3D structure from real-world natural stimuli.&nbsp; There has been relatively little work with natural stimuli because their complexity makes it is difficult to develop formal theories and measure human performance.&nbsp; The aim of this collaborative project was to develop theories of 3D perception and to measure 3D perception in natural scenes, including conditions with dynamic stimuli.&nbsp; The research at the University of Texas at Austin was directed at the understanding perception of 3D structure from four different cues (dimensions of information): defocus-blur, binocular disparity, monocular texture and luminance gradients, and motion speed.</p>\n<p>&nbsp;</p>\n<p>For defocus-blur, binocular disparity, and motion we carried out a new kind of theoretical analysis developed in our lab (&ldquo;accuracy maximization analysis&rdquo;) that determines the receptive fields (filters) that are optimal for estimating depth or speed information from each cue.&nbsp; These optimal receptive fields led to new hypotheses for neural computation that were then tested. &nbsp;For the case of binocular disparity we found a strong correlation between the optimal receptive fields for natural stimuli and the disparity selective receptive fields of neurons found in primary visual cortex.&nbsp; For the case of motion speed we found a strong correlation between human performance and optimal performance of an ideal observer on both natural and simple laboratory stimuli. For the case of defocus, we found the optimal filters for estimating the level of defocus in local regions of the image. By using chromatic and spatial information these filters make it possible to estimate depth at each image location.&nbsp; The optimal filters exploit optical aberrations and even work in cameras with high-quality optics (a paper reporting this was awarded best paper at an engineering conference).&nbsp; We also measured human ability to discriminate defocus in natural images and developed a principled model to account for the results.&nbsp; Our algorithms for estimating defocus based on analysis of natural image statistics are about to be tested for use in low-vision aids.</p>\n<p>&nbsp;</p>\n<p>Another major study was directed at measuring the statistics of binocular-disparity, texture, and luminance cues relevant for estimating local 3D surface orientation.&nbsp; A high-precision laser range scanner and a high-quality digital camera mounted on a robotic gantry were used to collect binocular images together with an estimate of the distance (range) at each image pixel location.&nbsp; From this data (which has been made publically available) we were able to determine how best to combine the local gradients along these three cue dimensions to estimate local surface tilt in natural scenes.&nbsp; In subsequent work (not covered under this project) one of the post-docs supported under this project showed that human tilt estimates in natural scenes closely match the parameter-free predictions from the natural image statistics measured in this study.</p>\n<p>&nbsp;</p>\n<p>Studies completed under this project have substantially advanced our understanding of how the human visual system estimates 3D structure under natural conditions.&nbsp; The measured scenes statistics and computational theories developed may also lead to substantial practical applications.</p>\n<p>&nbsp;</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 04/25/2018<br>\n\t\t\t\t\tModified by: Wilson&nbsp;S&nbsp;Geisler</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHumans and many other animals are able to estimate the three-dimensional (3D) structure of the environment around them from the two-dimensional (2D) images captured by the eyes.  Much of what we know about how the human brain accomplishes this remarkable feat has been obtained in experiments with simple laboratory stimuli.  However, the ultimate goal is to understand how the brain estimates 3D structure from real-world natural stimuli.  There has been relatively little work with natural stimuli because their complexity makes it is difficult to develop formal theories and measure human performance.  The aim of this collaborative project was to develop theories of 3D perception and to measure 3D perception in natural scenes, including conditions with dynamic stimuli.  The research at the University of Texas at Austin was directed at the understanding perception of 3D structure from four different cues (dimensions of information): defocus-blur, binocular disparity, monocular texture and luminance gradients, and motion speed.\n\n \n\nFor defocus-blur, binocular disparity, and motion we carried out a new kind of theoretical analysis developed in our lab (\"accuracy maximization analysis\") that determines the receptive fields (filters) that are optimal for estimating depth or speed information from each cue.  These optimal receptive fields led to new hypotheses for neural computation that were then tested.  For the case of binocular disparity we found a strong correlation between the optimal receptive fields for natural stimuli and the disparity selective receptive fields of neurons found in primary visual cortex.  For the case of motion speed we found a strong correlation between human performance and optimal performance of an ideal observer on both natural and simple laboratory stimuli. For the case of defocus, we found the optimal filters for estimating the level of defocus in local regions of the image. By using chromatic and spatial information these filters make it possible to estimate depth at each image location.  The optimal filters exploit optical aberrations and even work in cameras with high-quality optics (a paper reporting this was awarded best paper at an engineering conference).  We also measured human ability to discriminate defocus in natural images and developed a principled model to account for the results.  Our algorithms for estimating defocus based on analysis of natural image statistics are about to be tested for use in low-vision aids.\n\n \n\nAnother major study was directed at measuring the statistics of binocular-disparity, texture, and luminance cues relevant for estimating local 3D surface orientation.  A high-precision laser range scanner and a high-quality digital camera mounted on a robotic gantry were used to collect binocular images together with an estimate of the distance (range) at each image pixel location.  From this data (which has been made publically available) we were able to determine how best to combine the local gradients along these three cue dimensions to estimate local surface tilt in natural scenes.  In subsequent work (not covered under this project) one of the post-docs supported under this project showed that human tilt estimates in natural scenes closely match the parameter-free predictions from the natural image statistics measured in this study.\n\n \n\nStudies completed under this project have substantially advanced our understanding of how the human visual system estimates 3D structure under natural conditions.  The measured scenes statistics and computational theories developed may also lead to substantial practical applications.\n\n \n\n\t\t\t\t\tLast Modified: 04/25/2018\n\n\t\t\t\t\tSubmitted by: Wilson S Geisler"
 }
}