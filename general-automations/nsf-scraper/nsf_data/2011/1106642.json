{
 "awd_id": "1106642",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative Research: Objective Bayesian Model Selection and Estimation in High Dimensional Statistical Models",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Gabor Szekely",
 "awd_eff_date": "2011-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 99191.0,
 "awd_amount": 99191.0,
 "awd_min_amd_letter_date": "2011-09-20",
 "awd_max_amd_letter_date": "2011-09-20",
 "awd_abstract_narration": "It is widely accepted that in many high dimensional situations, model selection has to be performed either before parameter estimation or simultaneously, in order to reduce the number of parameters under consideration. Indeed, model selection is one of the major challenges facing statisticians working with high dimensional data. Tools such as regularization and sparsity are some of the common notions employed to obtain parsimonious models to explain observed data. In recent years, the field of statistics has witnessed an explosion of frequentist and Bayesian methods for high dimensional problems. Despite these and other advances, Bayesian model selection in an \"objective\" sense in high dimensional problems remains an important problem that has yet to be solved satisfactorily. The need for objectivity translates into a need for specifying noninformative improper priors, which in turn renders the traditional Bayes factors unusable. The project proposes to derive objective Bayesian estimation and model selection procedures in a large class of high dimensional graphical models. The methodology that is proposed in this project therefore aims to contribute to much needed theory in the area of objective Bayesian model selection for high dimensional graphical models. In the process the methodology studies the benefits and shortcomings of objective Bayesian methods in this context. The theory that is developed feeds into developing algorithms and computational techniques for model selection/estimation in high dimensional settings.\r\n\r\nThe availability of throughput or high dimensional data has touched almost every field of science. The need to formulate correct models that explain observed high dimensional data permeates through many scientific fields. Indeed, such data where the number of variables is often much higher than the number of samples, referred to as the \"large p small n\" problem, is now more pervasive than it has ever been. Discovering statistical signals in high dimensional data, proposing correct models that can explain such data, and parameter estimation in these high dimensional settings are some of the major challenges that modern day statisticians have to contend with. Moreover, such challenges also feature in high stakes debates such as climate change, effectiveness of certain drugs in clinical trials, and relevance of various biomarkers in cancer studies. This project proposes to develop statistical methodology which is specifically targeted towards identifying models which explain high dimensional data in an objective manner. In particular the project is designed to develop better objective Bayesian model selection and parameter estimation methods in high dimensional problems, and has widespread applications. The PI and co-PI collaborate with scientists in applied fields, especially with faculty/researchers in their Medical Schools, Schools of Engineering and Environmental Sciences. Training of graduate students and mentoring is an integral part of this collaborative research. Scientific output from the project is intended for publication in high impact peer-reviewed journals.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Balakanapathy",
   "pi_last_name": "Rajaratnam",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Balakanapathy Rajaratnam",
   "pi_email_addr": "brajaratnam01@gmail.com",
   "nsf_id": "000081127",
   "pi_start_date": "2011-09-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Stanford University",
  "inst_street_address": "450 JANE STANFORD WAY",
  "inst_street_address_2": "",
  "inst_city_name": "STANFORD",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "6507232300",
  "inst_zip_code": "943052004",
  "inst_country_name": "United States",
  "cong_dist_code": "16",
  "st_cong_dist_code": "CA16",
  "org_lgl_bus_name": "THE LELAND STANFORD JUNIOR UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "HJD6G4D6TJY5"
 },
 "perf_inst": {
  "perf_inst_name": "Stanford University",
  "perf_str_addr": "450 JANE STANFORD WAY",
  "perf_city_name": "STANFORD",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "943052004",
  "perf_ctry_code": "US",
  "perf_cong_dist": "16",
  "perf_st_cong_dist": "CA16",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  }
 ],
 "pgm_ref": null,
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 99191.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>High-dimensional data, or big data, where the number of variables is  much larger than the sample size, is now available from a variety of  scientific applications such as genetics, finance and climate sciences.  Understanding the complex network of relationships in these datasets is  an important goal, and estimation of the covariance matrix of the  variables is a fundamental step towards that goal. The aim of this  project was to develop methods for high-dimesional covariance estimation  with strong methodological and computational properties. We have been  successful in this goal.<br /> <br /> Our first contribution has been the development of a novel method for  sparse estimation of the inverse covariance matrix, based on penalized  pseudo-likelihood. This method builds on the previous work in this area,  and provides a state of the art algorithm with strong methodological  properties such as a provable convergence guarantee and consistency, and  is computationally efficient.<br /> <br /> Our second contribution has been the development of a novel class of  prior distributions for high-diemnsional Bayesian estimation of sparse  inverse covariance matrices. Through these priors, efficient estimation  is possible for a much larger class of sparsity patterns in the inverse  covariance matrix than in the previous literature.<br /> <br /> Our third contribution entails understanding when graphs can be used to represent associations between variables in infinite dimensional or infinite variate probability distributions. This investigation is useful for Big Data applications where the number of features is ultra high dimensional.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/11/2016<br>\n\t\t\t\t\tModified by: Balakanapathy&nbsp;Rajaratnam</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nHigh-dimensional data, or big data, where the number of variables is  much larger than the sample size, is now available from a variety of  scientific applications such as genetics, finance and climate sciences.  Understanding the complex network of relationships in these datasets is  an important goal, and estimation of the covariance matrix of the  variables is a fundamental step towards that goal. The aim of this  project was to develop methods for high-dimesional covariance estimation  with strong methodological and computational properties. We have been  successful in this goal.\n \n Our first contribution has been the development of a novel method for  sparse estimation of the inverse covariance matrix, based on penalized  pseudo-likelihood. This method builds on the previous work in this area,  and provides a state of the art algorithm with strong methodological  properties such as a provable convergence guarantee and consistency, and  is computationally efficient.\n \n Our second contribution has been the development of a novel class of  prior distributions for high-diemnsional Bayesian estimation of sparse  inverse covariance matrices. Through these priors, efficient estimation  is possible for a much larger class of sparsity patterns in the inverse  covariance matrix than in the previous literature.\n \n Our third contribution entails understanding when graphs can be used to represent associations between variables in infinite dimensional or infinite variate probability distributions. This investigation is useful for Big Data applications where the number of features is ultra high dimensional.\n\n\t\t\t\t\tLast Modified: 09/11/2016\n\n\t\t\t\t\tSubmitted by: Balakanapathy Rajaratnam"
 }
}