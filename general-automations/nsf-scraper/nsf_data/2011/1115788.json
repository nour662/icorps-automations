{
 "awd_id": "1115788",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "MCS: AF: Small: Algorithms for Large Scale Prediction Problems",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Rahul Shah",
 "awd_eff_date": "2011-07-15",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 350000.0,
 "awd_amount": 350000.0,
 "awd_min_amd_letter_date": "2011-07-13",
 "awd_max_amd_letter_date": "2011-07-13",
 "awd_abstract_narration": "In large scale prediction problems that arise in many application areas, data is plentiful, and it is computational resources that constrain the performance of prediction methods.  The broad goal of this research project is the design and analysis of methods for large scale prediction problems that make effective use of limited computational resources. The main aims are: to improve our understanding of the tradeoff between the accuracy of a prediction method and its computational requirements; to develop model selection methods that adaptively choose the model complexity to give the best predictive accuracy for the available computational resources; to improve our understanding of the difficulty of solving large scale prediction problems using distributed computational resources; to develop analysis techniques and methods for asynchronous online prediction, which exploit the flexibility to respond to queries out of order; and hence to develop effective methods for large scale prediction problems.\r\n\r\nAs data acquisition and storage has become cheaper, enormous data sets have become available in many areas, including web information retrieval, the biological, medical, and physical sciences,  manufacturing, finance and retail.  Consequently, for many statistical prediction problems, the amount of data available is so huge that we can treat it as unlimited. For instance, in using image and caption data to train a prediction rule that can automatically choose appropriate labels for images, the web provides an effectively unlimited supply of training data.  Similar situations arise in using click stream data to predict the choices of visitors to a popular web site, or in using customers' ratings of movies to make useful recommendations.  For these large scale prediction problems, the bottleneck to performance is not the amount of data, rather it is the computational resources that are available.  Many modern prediction methods have been designed and analyzed from the perspective that data is precious: they aim for optimal predictive accuracy for a given sample size. But for large scale problems, this is the wrong perspective; computation is the precious resource that must be used wisely. This shift in perspective introduces some novel tradeoffs. One of the most important tradeoffs arises in choosing the complexity of a prediction rule. Should we use our computational resources trying to optimize over a very complex family of prediction rules, which would not allow us to gather much data?  Or should we save computation by using simpler prediction rules, and instead spend this computation on gathering more data?  This research project is aimed at improving our understanding of these tradeoffs, and hence developing strategies for large scale prediction problems that best exploit the available computational resources.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Peter",
   "pi_last_name": "Bartlett",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Peter Bartlett",
   "pi_email_addr": "bartlett@stat.berkeley.edu",
   "nsf_id": "000489454",
   "pi_start_date": "2011-07-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-Berkeley",
  "inst_street_address": "1608 4TH ST STE 201",
  "inst_street_address_2": "",
  "inst_city_name": "BERKELEY",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5106433891",
  "inst_zip_code": "947101749",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": "REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE",
  "org_prnt_uei_num": "",
  "org_uei_num": "GS3YEVSS12N6"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-Berkeley",
  "perf_str_addr": "1608 4TH ST STE 201",
  "perf_city_name": "BERKELEY",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947101749",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "126900",
   "pgm_ele_name": "STATISTICS"
  },
  {
   "pgm_ele_code": "779600",
   "pgm_ele_name": "Algorithmic Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7796",
   "pgm_ref_txt": "ALGORITHMIC FOUNDATIONS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7926",
   "pgm_ref_txt": "ALGORITHMS"
  },
  {
   "pgm_ref_code": "7933",
   "pgm_ref_txt": "NUM, SYMBOL, & ALGEBRA COMPUT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 350000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>In large scale prediction problems that arise in many application areas (including web information retrieval, biological science, medical science, manufacturing, finance, retail, and physical sciences), data is plentiful and it is computational resources that constrain performance.<br />This research project focused on the design and analysis of methods for large scale prediction problems that make effective use of limited computational resources. Some of the highlights include: (1) the development of prediction methods that automatically choose the complexity of a prediction rule (called `model selection methods') to make the best use of the available computational resources; (2) new, efficient methods for optimization, including methods that make optimal use of distributed computational resources to solve certain optimization problems (called non-smooth convex problems) that are important for large-scale machine learning applications; (3) the discovery of the best possible strategies for several important prediction problems, including a novel game-theoretic formulation of the classical problem of linear regression, which has been intensively studied since the nineteenth century; and (4) the development of sequential decision methods (for problems such as evaluation of the effectiveness of medical treatment regimes, where there is an exploration/exploitation trade-off) with performance guarantees that show they do not degrade with the size of the problem.<br />The project integrated research and education through: (a) the involvement of five graduate students and six postdocs in the research and in the dissemination of research results, giving them experience in research, in collaboration, and in written and verbal technical communication; (b) the organization of two workshops on the themes of the research, held at the Neural Information Processing Systems Conferences, targeting graduate student attendees and researchers from a variety of disciplines; and (c) the inclusion of the research results in large graduate courses at Berkeley, and in the web-based course materials. &nbsp;Results of the research project have also been disseminated broadly through journals and conferences targeted to a variety of disciplines, including learning theory, theoretical computer science, information theory, and statistics.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 10/02/2015<br>\n\t\t\t\t\tModified by: Peter&nbsp;Bartlett</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nIn large scale prediction problems that arise in many application areas (including web information retrieval, biological science, medical science, manufacturing, finance, retail, and physical sciences), data is plentiful and it is computational resources that constrain performance.\nThis research project focused on the design and analysis of methods for large scale prediction problems that make effective use of limited computational resources. Some of the highlights include: (1) the development of prediction methods that automatically choose the complexity of a prediction rule (called `model selection methods') to make the best use of the available computational resources; (2) new, efficient methods for optimization, including methods that make optimal use of distributed computational resources to solve certain optimization problems (called non-smooth convex problems) that are important for large-scale machine learning applications; (3) the discovery of the best possible strategies for several important prediction problems, including a novel game-theoretic formulation of the classical problem of linear regression, which has been intensively studied since the nineteenth century; and (4) the development of sequential decision methods (for problems such as evaluation of the effectiveness of medical treatment regimes, where there is an exploration/exploitation trade-off) with performance guarantees that show they do not degrade with the size of the problem.\nThe project integrated research and education through: (a) the involvement of five graduate students and six postdocs in the research and in the dissemination of research results, giving them experience in research, in collaboration, and in written and verbal technical communication; (b) the organization of two workshops on the themes of the research, held at the Neural Information Processing Systems Conferences, targeting graduate student attendees and researchers from a variety of disciplines; and (c) the inclusion of the research results in large graduate courses at Berkeley, and in the web-based course materials.  Results of the research project have also been disseminated broadly through journals and conferences targeted to a variety of disciplines, including learning theory, theoretical computer science, information theory, and statistics.\n\n\t\t\t\t\tLast Modified: 10/02/2015\n\n\t\t\t\t\tSubmitted by: Peter Bartlett"
 }
}