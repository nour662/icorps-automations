{
 "awd_id": "1122519",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Topology of Neural Coding in Recurrent Networks:  Theory and Data Analysis",
 "cfda_num": "47.049",
 "org_code": "03040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Mary Ann Horn",
 "awd_eff_date": "2011-10-01",
 "awd_exp_date": "2015-09-30",
 "tot_intn_awd_amt": 316862.0,
 "awd_amount": 316862.0,
 "awd_min_amd_letter_date": "2011-09-21",
 "awd_max_amd_letter_date": "2011-09-21",
 "awd_abstract_narration": "This project develops a mathematical theory that relates the coding properties of neuronal populations to the structure of the local networks to which they belong.  An important ingredient is the analysis of topological invariants, such as homology groups, of the stimulus spaces represented by networks of neurons, and how they constrain the connectivity of the underlying networks.  This necessitates an approach that blends algebraic-topological methods with more traditional dynamical systems models.   The research will produce testable predictions about the structure of networks that support stimulus representation, and will deepen our understanding of the relationship between network structure and function.  The theory will be both tested and guided by the analysis of multi-unit electrophysiological recordings in behaving animals.\r\n\r\nThe brain is a vast collection of interconnected neural circuits. In many brain areas, important neural computations are accomplished by local networks of neurons. However, the structure of local recurrent circuits in the brain is still poorly understood, even in the most studied brain areas such as the hippocampus.  In contrast, neuroscience experiments have been much more successful in uncovering coding properties of individual neurons and, more recently, in characterizing patterns of population activity in local neuronal circuits.  This research develops a mathematical theory that exploits our knowledge of the representational properties of neuronal populations in order to better understand the structure of the underlying networks.  The findings yield new insight into the role of neural circuits in learning and memory, and of how the brain organizes knowledge.  Progress in the basic understanding of neural circuits is essential for improving our understanding of learning disabilities and diseases (such as epilepsy and schizophrenia) that are believed to be related to the malfunction of neural circuits.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "MPS",
 "org_dir_long_name": "Directorate for Mathematical and Physical Sciences",
 "div_abbr": "DMS",
 "org_div_long_name": "Division Of Mathematical Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vladimir",
   "pi_last_name": "Itskov",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Vladimir Itskov",
   "pi_email_addr": "vladimir.itskov@psu.edu",
   "nsf_id": "000496685",
   "pi_start_date": "2011-09-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of Nebraska-Lincoln",
  "inst_street_address": "2200 VINE ST # 830861",
  "inst_street_address_2": "",
  "inst_city_name": "LINCOLN",
  "inst_state_code": "NE",
  "inst_state_name": "Nebraska",
  "inst_phone_num": "4024723171",
  "inst_zip_code": "685032427",
  "inst_country_name": "United States",
  "cong_dist_code": "01",
  "st_cong_dist_code": "NE01",
  "org_lgl_bus_name": "BOARD OF REGENTS OF THE UNIVERSITY OF NEBRASKA",
  "org_prnt_uei_num": "",
  "org_uei_num": "HTQ6K6NJFHA6"
 },
 "perf_inst": {
  "perf_inst_name": "University of Nebraska-Lincoln",
  "perf_str_addr": "2200 VINE ST # 830861",
  "perf_city_name": "LINCOLN",
  "perf_st_code": "NE",
  "perf_st_name": "Nebraska",
  "perf_zip_code": "685032427",
  "perf_ctry_code": "US",
  "perf_cong_dist": "01",
  "perf_st_cong_dist": "NE01",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "733400",
   "pgm_ele_name": "MATHEMATICAL BIOLOGY"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7569",
   "pgm_ref_txt": "CYBERINFRASTRUCTURE/SCIENCE"
  },
  {
   "pgm_ref_code": "9150",
   "pgm_ref_txt": "EXP PROG TO STIM COMP RES"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 316862.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The major outcomes of the funded research can be split into two rough categories:</p>\n<p><strong>(1) &nbsp;Topological analysis structure of neural activity, and</strong></p>\n<p><strong>(2)&nbsp;Neural network theory.</strong></p>\n<p>Detecting structure in neural activity is critical for understanding the function of neural circuits. The coding properties of neurons are typically investigated by correlating their responses to external stimuli. It is not clear, however, if the structure of neural activity can be inferred intrinsically, without a priori knowledge of the relevant stimuli. We introduced a novel method, called clique topology, that detects intrinsic structure in neural activity that is invariant under nonlinear monotone transformations. Using pairwise correlations of neurons in the hippocampus, we demonstrated that our method is capable of detecting geometric structure from neural activity alone, without appealing to external stimuli or receptive fields.&nbsp; Remarkably, we found similar results during non-spatial behaviors such as wheel running and rapid eye movement (REM) sleep. This suggests that the geometric structure of correlations is shaped by the underlying hippocampal circuits and is not merely a consequence of position coding. Some of the &nbsp;main findings of this line of research were published in the Proceeding of the National Academy of Sciences:</p>\n<p><a href=\"http://www.pnas.org/content/112/44/13455\">http://www.pnas.org/content/112/44/13455</a></p>\n<p>In a separate, yet related, line of work we developed a fresh approach to the problem of encoding binary patterns as fixed point attractors in simple neural networks, while minimizing the introduction of so-called \"spurious states\" (additional attractors that one did not wish to encode).&nbsp; This problem dates back at least 30 years, to the famous Hopfield model (1982). This work established the link between the stable sets of a synaptic matrix and the metric&nbsp;geometry of point arrangements; the unexpected&nbsp;link being the role of Cayley-Menger determinants in stability of a deformation of a rank=1&nbsp; matrix. This in turn enabled the usage of&nbsp;&nbsp; classical convex geometry results (such as Helly's theorem) to understand how a common class of neural codes can naturally arise on a recurrent network.&nbsp; The connection we uncovered between network dynamics and distance geometry may also generate independent mathematical interest.</p>\n<p>While these results established an exact relationship of a (symmetric)&nbsp;synaptic matrix to the combinatorial properties of a neural code, it did not completely solve&nbsp; the ``Network Encoding Problem,'' i.e., the problem of designing a recurrent network&nbsp; for any prescribed combinatorial neural code. This is because this problem is intrinsically challenging -- in its simplest form, it amounts to&nbsp;finding a symmetric matrix&nbsp;whose pattern of stable principal submatrices matches a prescribed simplicial complex.</p>\n<p>&nbsp;</p>\n<p>Nevertheless, the original motivation of relating the topological properties of the stimuli to the structural properties of the synaptic matrix proved to be quite tractable and we found that both purely feedforward and strictly recurrent networks are capable of encoding any prescribed homotopy type.&nbsp;Moreover, while the Network Encoding problem for strongly recurrent networks is unsolved, we proved that a feedforward network can encode any prescribed simplicial complex (but &nbsp;not every prescribed combinatorial neural code). The proof is accomplished via an explicit construction that made use of the inverse of the nerve endofunctor on simplicial complexes. This result is rather unexpected, since &nbsp;classical theorems in neural networks postulate that in order to approximate an arbitrary &nbsp;continuous function, one needs <em>two</em> &nbsp;feedforward layers to do so (as opposed t...",
  "por_txt_cntn": "\nThe major outcomes of the funded research can be split into two rough categories:\n\n(1)  Topological analysis structure of neural activity, and\n\n(2) Neural network theory.\n\nDetecting structure in neural activity is critical for understanding the function of neural circuits. The coding properties of neurons are typically investigated by correlating their responses to external stimuli. It is not clear, however, if the structure of neural activity can be inferred intrinsically, without a priori knowledge of the relevant stimuli. We introduced a novel method, called clique topology, that detects intrinsic structure in neural activity that is invariant under nonlinear monotone transformations. Using pairwise correlations of neurons in the hippocampus, we demonstrated that our method is capable of detecting geometric structure from neural activity alone, without appealing to external stimuli or receptive fields.  Remarkably, we found similar results during non-spatial behaviors such as wheel running and rapid eye movement (REM) sleep. This suggests that the geometric structure of correlations is shaped by the underlying hippocampal circuits and is not merely a consequence of position coding. Some of the  main findings of this line of research were published in the Proceeding of the National Academy of Sciences:\n\nhttp://www.pnas.org/content/112/44/13455\n\nIn a separate, yet related, line of work we developed a fresh approach to the problem of encoding binary patterns as fixed point attractors in simple neural networks, while minimizing the introduction of so-called \"spurious states\" (additional attractors that one did not wish to encode).  This problem dates back at least 30 years, to the famous Hopfield model (1982). This work established the link between the stable sets of a synaptic matrix and the metric geometry of point arrangements; the unexpected link being the role of Cayley-Menger determinants in stability of a deformation of a rank=1  matrix. This in turn enabled the usage of   classical convex geometry results (such as Helly's theorem) to understand how a common class of neural codes can naturally arise on a recurrent network.  The connection we uncovered between network dynamics and distance geometry may also generate independent mathematical interest.\n\nWhile these results established an exact relationship of a (symmetric) synaptic matrix to the combinatorial properties of a neural code, it did not completely solve  the ``Network Encoding Problem,'' i.e., the problem of designing a recurrent network  for any prescribed combinatorial neural code. This is because this problem is intrinsically challenging -- in its simplest form, it amounts to finding a symmetric matrix whose pattern of stable principal submatrices matches a prescribed simplicial complex.\n\n \n\nNevertheless, the original motivation of relating the topological properties of the stimuli to the structural properties of the synaptic matrix proved to be quite tractable and we found that both purely feedforward and strictly recurrent networks are capable of encoding any prescribed homotopy type. Moreover, while the Network Encoding problem for strongly recurrent networks is unsolved, we proved that a feedforward network can encode any prescribed simplicial complex (but  not every prescribed combinatorial neural code). The proof is accomplished via an explicit construction that made use of the inverse of the nerve endofunctor on simplicial complexes. This result is rather unexpected, since  classical theorems in neural networks postulate that in order to approximate an arbitrary  continuous function, one needs two  feedforward layers to do so (as opposed to one layer in our case that allows to encode any prescribed simplicial complex). \n\nAll the major findings related to this NSF grant can be found in peer reviewed publications aggregated at  \n\nhttp://www.personal.psu.edu/vui1/publications.html\n\n \n\n\t\t\t\t\tLast Modified: 03/02/2016\n\n\t\t\t\t\tSubmitted by: Vladimir Itskov"
 }
}