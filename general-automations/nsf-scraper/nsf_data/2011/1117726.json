{
 "awd_id": "1117726",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "CSR: Small: Statistical Memory Monitoring in Hardware for Security and Performance",
 "cfda_num": "47.070",
 "org_code": "05050000",
 "po_phone": "7032925197",
 "po_email": "mmcclure@nsf.gov",
 "po_sign_block_name": "Marilyn McClure",
 "awd_eff_date": "2011-07-15",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 420000.0,
 "awd_amount": 420000.0,
 "awd_min_amd_letter_date": "2011-07-13",
 "awd_max_amd_letter_date": "2011-07-13",
 "awd_abstract_narration": "This project applies statistical sampling techniques to detect deviation from normal or desired behavior for a variety of applications, ranging from security problems such as phase-change memory (PCM) wear leveling and intrusion detection to performance problems such as multicore communication, data migration, and locality optimizations. Existing performance monitoring and measurement techniques are insuf&#64257;cient for our applications for a variety of reasons. Many depend upon the OS which (1) may be compromised and hence be unsuitable for security-related monitoring, and (2) tracks data at page granularity whereas memory hierarchy performance often needs monitoring at the block granularity.  To avoid these limitations, this project designs a monitoring architecture for statistically sampling memory access patterns. Brute-force monitoring would require large, frequently-searched hardware structures that increase complexity and power, whereas sampling enables much smaller structures that are searched at low rates, incurring far less overhead. In general, sampling loses accuracy or requires a large number of samples (and large hardware structures) if the monitored behavior exhibits high standard deviation. Our key insight is that we can bound the standard deviation of the behavior within the region of interest to the application, thereby allowing accurate and low-overhead sampling. Our key intellectual merit is to show that statistical sampling and performance monitoring can unify the aims of disparate applications and enable monitoring with high accuracy and low overhead. The broader impacts include paving the way for robust statistics-based performance monitoring in future computer systems and for research and education efforts that combine statistics and computer architecture.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CNS",
 "org_div_long_name": "Division Of Computer and Network Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Vijay",
   "pi_last_name": "Pai",
   "pi_mid_init": "S",
   "pi_sufx_name": "",
   "pi_full_name": "Vijay S Pai",
   "pi_email_addr": "vpai@purdue.edu",
   "nsf_id": "000184842",
   "pi_start_date": "2011-07-13",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Terani",
   "pi_last_name": "Vijaykumar",
   "pi_mid_init": "N",
   "pi_sufx_name": "",
   "pi_full_name": "Terani N Vijaykumar",
   "pi_email_addr": "vijay@ecn.purdue.edu",
   "nsf_id": "000337724",
   "pi_start_date": "2011-07-13",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Purdue University",
  "inst_street_address": "2550 NORTHWESTERN AVE # 1100",
  "inst_street_address_2": "",
  "inst_city_name": "WEST LAFAYETTE",
  "inst_state_code": "IN",
  "inst_state_name": "Indiana",
  "inst_phone_num": "7654941055",
  "inst_zip_code": "479061332",
  "inst_country_name": "United States",
  "cong_dist_code": "04",
  "st_cong_dist_code": "IN04",
  "org_lgl_bus_name": "PURDUE UNIVERSITY",
  "org_prnt_uei_num": "YRXVL4JYCEF5",
  "org_uei_num": "YRXVL4JYCEF5"
 },
 "perf_inst": {
  "perf_inst_name": "Purdue University",
  "perf_str_addr": "2550 NORTHWESTERN AVE # 1100",
  "perf_city_name": "WEST LAFAYETTE",
  "perf_st_code": "IN",
  "perf_st_name": "Indiana",
  "perf_zip_code": "479061332",
  "perf_ctry_code": "US",
  "perf_cong_dist": "04",
  "perf_st_cong_dist": "IN04",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "735400",
   "pgm_ele_name": "CSR-Computer Systems Research"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7354",
   "pgm_ref_txt": "COMPUTER SYSTEMS"
  },
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 420000.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>This project targets applying sound statistical techniques, as opposed to ad hoc heuristics, to computer system performance and security. To this end, we &nbsp;have proposed several novel schemes: (1)&nbsp;<span>For Phase Change Memory wear leveling, &nbsp;which can be a security issue due to malicious wear inducement, we have proposed a statistically-sound wear leveling scheme. Our approach, called statistical wear leveling (SWL), which randomizes address-to-frame mapping on the basis of the estimated overwrite rates instead of write rates. SWL achieves both lower common-case write overhead and lower hardware overhead, and similar,&nbsp; high common-case lifetime as compared to the previous schemes while achieving reasonable worst-case lifetime. &nbsp;<span>Previous work, called Security Refresh, incurs either high write overhead of 50% or more in the common case (single level), or high hardware overhead of a set of logic for every 2 MB (two level) while achieving about 25 months of worst-case lifetime. In contrast, SWL achieves low write overhead of less than 0.15%, and similar, high lifetime of thousands of years in the common case; SWL trades off higher write overhead of 400% and lower yet reasonable lifetime of more than 6 months in the worst case to achieve low hardware overhead of a set of logic and a 13-entry sampling buffer for every 2 GB. &nbsp;</span></span>(2) We have applied statistical techniques (such as segmented regression) to model the performance of disks and created a tool called DiskMod. DiskMod integrates with the SimICS architectural simulation platform with little cost or configuration complexity and is able to provide performance estimates comparable to, and often better than, more detailed physical-level models that have been proposed previously. (3) Our new strategies for shared-cache partitioning in multithreaded applications yields substantial benefits in performance improvements for applications that otherwise use cache inefficiently. The main reason for this inefficiency is that all of their threads are simultaneously in \"flat regions\" of the reuse distance curve, where any small increase or reduction in way allocation will have no significant impact on cache misses. Even though these applications are balanced among threads, a better strategy is to temporally imbalance the partitioning among threads so that one thread pushes beyond the next reuse distance knee while the others are not affected much. Fairness is maintained by preferring threads in round-robin order. (4)&nbsp;We have also used queuing theory for multicore power saving.Lowering core clock frequency reduces power allowing more cores to be active (i.e., higher throughput) for a given total power budget. However, single-thread performance worsens due to lower frequency. To address this issue, we employ queuing theory to show that the higher throughput reduces queuing delays and offsets the slower clock speed to maintain single-thread performance. (5) &nbsp;We have&nbsp;applied statistical sampling techniques to reduce communication and data movement by approximating in &nbsp;MapReduce without missing rare keys. Many MapReduce problems lend themselves to approximation (e.g., averaging, ranking, other statistics). Previous work on approximation in MapReduce can miss keys, which can cause unbounded error. We employ a novel distributed, stratified, online sampling to avoid rare keys. &nbsp;Previous database work has done stratified offline sampling, but such offline step &nbsp;means that the entire tuple space must be read to construct the samples prior to performing the computation. In MapReduce problems, which are often concerned with large-scale analytics of a data set rather than repeated queries over the same data, the cost of constructing the samples offline cannot be amortized, so these approaches are not appropriate. We have devised a novel telescoping algorithm for suc...",
  "por_txt_cntn": "\nThis project targets applying sound statistical techniques, as opposed to ad hoc heuristics, to computer system performance and security. To this end, we  have proposed several novel schemes: (1) For Phase Change Memory wear leveling,  which can be a security issue due to malicious wear inducement, we have proposed a statistically-sound wear leveling scheme. Our approach, called statistical wear leveling (SWL), which randomizes address-to-frame mapping on the basis of the estimated overwrite rates instead of write rates. SWL achieves both lower common-case write overhead and lower hardware overhead, and similar,  high common-case lifetime as compared to the previous schemes while achieving reasonable worst-case lifetime.  Previous work, called Security Refresh, incurs either high write overhead of 50% or more in the common case (single level), or high hardware overhead of a set of logic for every 2 MB (two level) while achieving about 25 months of worst-case lifetime. In contrast, SWL achieves low write overhead of less than 0.15%, and similar, high lifetime of thousands of years in the common case; SWL trades off higher write overhead of 400% and lower yet reasonable lifetime of more than 6 months in the worst case to achieve low hardware overhead of a set of logic and a 13-entry sampling buffer for every 2 GB.  (2) We have applied statistical techniques (such as segmented regression) to model the performance of disks and created a tool called DiskMod. DiskMod integrates with the SimICS architectural simulation platform with little cost or configuration complexity and is able to provide performance estimates comparable to, and often better than, more detailed physical-level models that have been proposed previously. (3) Our new strategies for shared-cache partitioning in multithreaded applications yields substantial benefits in performance improvements for applications that otherwise use cache inefficiently. The main reason for this inefficiency is that all of their threads are simultaneously in \"flat regions\" of the reuse distance curve, where any small increase or reduction in way allocation will have no significant impact on cache misses. Even though these applications are balanced among threads, a better strategy is to temporally imbalance the partitioning among threads so that one thread pushes beyond the next reuse distance knee while the others are not affected much. Fairness is maintained by preferring threads in round-robin order. (4) We have also used queuing theory for multicore power saving.Lowering core clock frequency reduces power allowing more cores to be active (i.e., higher throughput) for a given total power budget. However, single-thread performance worsens due to lower frequency. To address this issue, we employ queuing theory to show that the higher throughput reduces queuing delays and offsets the slower clock speed to maintain single-thread performance. (5)  We have applied statistical sampling techniques to reduce communication and data movement by approximating in  MapReduce without missing rare keys. Many MapReduce problems lend themselves to approximation (e.g., averaging, ranking, other statistics). Previous work on approximation in MapReduce can miss keys, which can cause unbounded error. We employ a novel distributed, stratified, online sampling to avoid rare keys.  Previous database work has done stratified offline sampling, but such offline step  means that the entire tuple space must be read to construct the samples prior to performing the computation. In MapReduce problems, which are often concerned with large-scale analytics of a data set rather than repeated queries over the same data, the cost of constructing the samples offline cannot be amortized, so these approaches are not appropriate. We have devised a novel telescoping algorithm for such sampling. Our approach achieves 50% better performance than the baseline MapReduce (without approximation) without missing rare keys and with erro..."
 }
}