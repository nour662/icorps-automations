{
 "awd_id": "1128436",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "Collaborative: Gesture Recognition Challenge",
 "cfda_num": "47.041",
 "org_code": "07010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Paul Werbos",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2012-08-31",
 "tot_intn_awd_amt": 49800.0,
 "awd_amount": 49800.0,
 "awd_min_amd_letter_date": "2011-07-21",
 "awd_max_amd_letter_date": "2011-07-21",
 "awd_abstract_narration": "The objective of this research is both to advance the field of video data processing (more particularly gesture recognition) and to illustrate the power of deep learning architectures and transfer learning. The approach is to organize a challenge culminating in a life evaluation at the site of a conference.\r\nIntellectual merit: Much of the recent research in Adaptive and Intelligent Systems (AIS) has sacrificed the grand goal of designing systems ever approaching human intelligence for solving data mining tasks of practical interest with more immediate reward. This project gives an opportunity to deep learning architectures inspired by neural networks to demonstrate their ability to address more complex problems requiring to transfer knowledge from task to task (transfer learning), leveraging the availability of video data not directly related to the target task of gesture recognition. The participants will also be involved in a data exchange to grow an unprecedented large and diverse database of gestures. \r\nBroader Impact: Challenges have proved to be a great stimulus of research. For a long lasting impact, the challenge platform and the data and software repositories will remain open beyond the term of the NSF funded project. The educational components of the project include engaging students in the contest, providing material directly usable in teaching curricula, and demonstrating gesture recognition to high school students to expose them to computer vision research and sign language communication. Our connections with the deaf community will allow us to gear the product of this research to advance assistive technology.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "ENG",
 "org_dir_long_name": "Directorate for Engineering",
 "div_abbr": "ECCS",
 "org_div_long_name": "Division of Electrical, Communications and Cyber Systems",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Isabelle",
   "pi_last_name": "Guyon",
   "pi_mid_init": "M",
   "pi_sufx_name": "",
   "pi_full_name": "Isabelle M Guyon",
   "pi_email_addr": "guyon@clopinet.com",
   "nsf_id": "000203746",
   "pi_start_date": "2011-07-21",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Clopinet",
  "inst_street_address": "955 Creston Road",
  "inst_street_address_2": "",
  "inst_city_name": "Berkeley",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5105246211",
  "inst_zip_code": "947081501",
  "inst_country_name": "United States",
  "cong_dist_code": "12",
  "st_cong_dist_code": "CA12",
  "org_lgl_bus_name": null,
  "org_prnt_uei_num": null,
  "org_uei_num": null
 },
 "perf_inst": {
  "perf_inst_name": "Clopinet",
  "perf_str_addr": "955 Creston Road",
  "perf_city_name": "Berkeley",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "947081501",
  "perf_ctry_code": "US",
  "perf_cong_dist": "12",
  "perf_st_cong_dist": "CA12",
  "perf_ctry_name": "United States",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "760700",
   "pgm_ele_name": "EPCN-Energy-Power-Ctrl-Netwrks"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1653",
   "pgm_ref_txt": "Adaptive & intelligent systems"
  },
  {
   "pgm_ref_code": "9102",
   "pgm_ref_txt": "WOMEN, MINORITY, DISABLED, NEC"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 49800.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Gesture recognition is an important sub-problem in many computer vision applications, including image/video indexing, robot navigation, video surveillance, computer interfaces, and gaming. With simple gestures such as hand waving, gesture recognition could enable controlling the&nbsp;lights or thermostat in your home or changing TV channels. The same technology may&nbsp;even make it possible to automatically detect more complex human&nbsp;behaviors, to allow surveillance systems to sound an alarm when&nbsp;someone is acting suspiciously, for example, or to send help whenever&nbsp;a bedridden patient shows signs of distress.&nbsp;</p>\n<p>Gesture recognition also provides excellent benchmarks for Adaptive and Intelligent Systems (AIS) and computer vision algorithms. The recognition of continuous, natural gestures is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues. Technical difficulties include tracking reliably hand, head and body parts, and achieving 3D invariance. The&nbsp;competition we organized in the context of this NSF sponsored project helped improve the accuracy of gesture recognition&nbsp;using Microsoft Kinect(TM) motion sensor technology, a&nbsp;low cost 3D depth-sensing camera.</p>\n<p><strong>Intellectual merit: </strong>Much of the recent research in Adaptive and Intelligent Systems (AIS) has sacrificed the grand goal of designing systems ever approaching human intelligence for solving data mining tasks of practical interest with more immediate reward.&nbsp;Humans can recognize new gestures after&nbsp;seeing just one example (one-shot-learning). With computers though,&nbsp;recognizing even well-defined gestures, such as sign language, is much&nbsp;more challenging and has traditionally required thousands of training&nbsp;examples to teach the software.&nbsp;One of our goals was to evaluate whether transfer learning algorithms, which can exploit miscellaneous data resources, can improve the performance of systems designed to work on new similar tasks (e.g. recognize a new vocabulary of gestures).&nbsp;To see what the machines are capable of, we launched in 2012 a&nbsp;competition sponsored by ChaLearn with prizes donated by Microsoft. The challenge&nbsp;helped narrow down the gap between&nbsp;machine and human performance. In two rounds each lasting four months, the challenge attracted a total of 85 teams&nbsp;making 935 entries. They lowered&nbsp;the error rate, starting from a baseline method making more than 50%&nbsp;error to 7% error.&nbsp;The winner of the challenge, Alfonso Nieto Castanon, used a method he&nbsp;invented, which is inspired by the human vision system. He and the&nbsp;second and third place winners in either round were awarded $5000, $3000 and $2000&nbsp;respectively and got an opportunity to present their results&nbsp;at the CVPR 2012 and ICPR 2012 conferences. We also organized demonstration competitions of gesture recognition&nbsp;systems using Kinect(TM) in conjunction with those&nbsp;events, with similar prizes donated by Microsoft. Novel data representations were proposed to tackle with success in real time the problem of hand and finger posture recognition.</p>\n<p><strong>Broader impact: </strong>Challenges have proved to be a great stimulus of research in machine learning, pattern recognition, and robotics. Our main activities were the collection of a large dataset made publicly available, and the organization of two rounds of a quantitative challenge and qualitative demonstration competition for two major IEEE conferences. Our activities also included the dissemination of methods by editing a special topic of the Journal of Machine Learning research (JMLR), which will also be published as a book. We are in the process of completing the librar...",
  "por_txt_cntn": "\nGesture recognition is an important sub-problem in many computer vision applications, including image/video indexing, robot navigation, video surveillance, computer interfaces, and gaming. With simple gestures such as hand waving, gesture recognition could enable controlling the lights or thermostat in your home or changing TV channels. The same technology may even make it possible to automatically detect more complex human behaviors, to allow surveillance systems to sound an alarm when someone is acting suspiciously, for example, or to send help whenever a bedridden patient shows signs of distress. \n\nGesture recognition also provides excellent benchmarks for Adaptive and Intelligent Systems (AIS) and computer vision algorithms. The recognition of continuous, natural gestures is very challenging due to the multimodal nature of the visual cues (e.g., movements of fingers and lips, facial expressions, body pose), as well as technical limitations such as spatial and temporal resolution and unreliable depth cues. Technical difficulties include tracking reliably hand, head and body parts, and achieving 3D invariance. The competition we organized in the context of this NSF sponsored project helped improve the accuracy of gesture recognition using Microsoft Kinect(TM) motion sensor technology, a low cost 3D depth-sensing camera.\n\nIntellectual merit: Much of the recent research in Adaptive and Intelligent Systems (AIS) has sacrificed the grand goal of designing systems ever approaching human intelligence for solving data mining tasks of practical interest with more immediate reward. Humans can recognize new gestures after seeing just one example (one-shot-learning). With computers though, recognizing even well-defined gestures, such as sign language, is much more challenging and has traditionally required thousands of training examples to teach the software. One of our goals was to evaluate whether transfer learning algorithms, which can exploit miscellaneous data resources, can improve the performance of systems designed to work on new similar tasks (e.g. recognize a new vocabulary of gestures). To see what the machines are capable of, we launched in 2012 a competition sponsored by ChaLearn with prizes donated by Microsoft. The challenge helped narrow down the gap between machine and human performance. In two rounds each lasting four months, the challenge attracted a total of 85 teams making 935 entries. They lowered the error rate, starting from a baseline method making more than 50% error to 7% error. The winner of the challenge, Alfonso Nieto Castanon, used a method he invented, which is inspired by the human vision system. He and the second and third place winners in either round were awarded $5000, $3000 and $2000 respectively and got an opportunity to present their results at the CVPR 2012 and ICPR 2012 conferences. We also organized demonstration competitions of gesture recognition systems using Kinect(TM) in conjunction with those events, with similar prizes donated by Microsoft. Novel data representations were proposed to tackle with success in real time the problem of hand and finger posture recognition.\n\nBroader impact: Challenges have proved to be a great stimulus of research in machine learning, pattern recognition, and robotics. Our main activities were the collection of a large dataset made publicly available, and the organization of two rounds of a quantitative challenge and qualitative demonstration competition for two major IEEE conferences. Our activities also included the dissemination of methods by editing a special topic of the Journal of Machine Learning research (JMLR), which will also be published as a book. We are in the process of completing the library of tools addressing the problems of the challenge with algorithms used by the winners. For a long lasting impact, the challenge platform, the data and software repositories will remain available beyond the term of the NSF funded project. Awards in the form ..."
 }
}