{
 "awd_id": "1116820",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "CIF: Small: Polar Codes --- From Theory to Practice",
 "cfda_num": "47.070",
 "org_code": "05010000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Richard Brown",
 "awd_eff_date": "2011-09-01",
 "awd_exp_date": "2014-08-31",
 "tot_intn_awd_amt": 494628.0,
 "awd_amount": 494628.0,
 "awd_min_amd_letter_date": "2011-08-08",
 "awd_max_amd_letter_date": "2012-09-10",
 "awd_abstract_narration": "Digital communication pervades our daily lives while digital storage devices have become the principal means of preserving our information. During the \"information age,\" in which we now live, the need for reliable transmission and storage of digital data is of paramount importance. What makes such reliable transmission and storage possible are error- correcting codes, first conceived by Claude Shannon over 50 years ago. The recent invention of polar codes is, without doubt, the most original and profound development in the theory of error-correcting codes in the past decade. Polar codes provably achieve the capacity of any memoryless symmetric channel, with low encoding and decoding complexity, thereby providing the first deterministic and constructive solution to the problem posed by Shannon in 1948. Nevertheless, the impact of polar codes in practice has been, so far, negligible. The objective of this project is to advance the theory of polar codes on one hand, and to bring polar codes much closer to practice on the other hand. If successful, the outcome of this research is likely to become an enabling technology for numerous communications applications, both commercial and for national security.\r\n\r\nIn order to make polar codes practical, major obstacles must be resolved. The first key problem in the field is how to efficiently construct polar codes. This project aims to develop a linear-time construction algorithm, with explicit guarantees on the quality of its output. The investigators also study algebraic and combinatorial structure of polar codes, with the goal of developing a good analytical handle on the rate of channel polarization. Currently available empirical results indicate that the rate of channel polarization is too slow for many applications. Thus the investigators intend to drastically improve the performance of polar codes, at short to moderate code lengths, by introducing certain key modifications in the successive-cancellation decoding algorithm. One especially promising idea in this regard is list decoding. A concerted effort is devoted to the analysis of list decoding algorithms for polar codes. Furthermore, a full implementation of such decoding algorithms in high-speed and low-power VLSI is pursued. This part of the research involves algorithmic transformations for the key steps of the decoder, effective high-throughput design techniques, careful VLSI complexity/area analysis, and new computation scheduling ideas. Finally, applications of polar codes and channel polarization beyond point-to-point communications are considered. Such applications include multiple-access channels, relay channels, Slepian-Wolf coding, and information-theoretic security.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "CSE",
 "org_dir_long_name": "Directorate for Computer and Information Science and Engineering",
 "div_abbr": "CCF",
 "org_div_long_name": "Division of Computing and Communication Foundations",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Alexander",
   "pi_last_name": "Vardy",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Alexander Vardy",
   "pi_email_addr": "vardy@ece.ucsd.edu",
   "nsf_id": "000490552",
   "pi_start_date": "2011-08-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "University of California-San Diego",
  "inst_street_address": "9500 GILMAN DR",
  "inst_street_address_2": "",
  "inst_city_name": "LA JOLLA",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "8585344896",
  "inst_zip_code": "920930021",
  "inst_country_name": "United States",
  "cong_dist_code": "50",
  "st_cong_dist_code": "CA50",
  "org_lgl_bus_name": "UNIVERSITY OF CALIFORNIA, SAN DIEGO",
  "org_prnt_uei_num": "",
  "org_uei_num": "UYTTZT6G9DT1"
 },
 "perf_inst": {
  "perf_inst_name": "University of California-San Diego",
  "perf_str_addr": "9500 GILMAN DR",
  "perf_city_name": "LA JOLLA",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "920930021",
  "perf_ctry_code": "US",
  "perf_cong_dist": "50",
  "perf_st_cong_dist": "CA50",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "779700",
   "pgm_ele_name": "Comm & Information Foundations"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "7923",
   "pgm_ref_txt": "SMALL PROJECT"
  },
  {
   "pgm_ref_code": "7935",
   "pgm_ref_txt": "COMM & INFORMATION THEORY"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 158123.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 336505.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The digital information age, in which we now live, commenced with the work of Claude Shannon in the 1940s. Shannon showed that every communication channel and every data storage system --- no matter how complicated --- can be characterized by a single constant, called the <em>capacity</em>. Moreover, he proved that it is in principle possible to communicate reliably at information rates up to this capacity using <em>error-correcting codes</em>.</p>\n<p>Since then, the search for error-correcting codes that achieve Shannon's capacity has been the main open problem in the fields of information theory and coding theory. This search took over 60 years. Polar codes, invented by Erdal Arikan in 2009, provably achieve capacity for a wide range of communication channels with low encoding and decoding complexity, thereby providing the <em>first deterministic solution to this problem</em>. The major goal of this project was to bridge the gap between this remarkable theoretical advance and the existing practice of error-correcting codes --- as implemented, for example, in wireless communications, in fiber-optic networks, in computer disk drives, as well as a plethora of other networks and devices that make the information age possible.</p>\n<p>In order to achieve this goal, in this project, we have succeeded to resolve the two main open problems in the theory of polar codes, which largely prevented their utilization in practice. At the time of writing, polar codes are actively considered for standardization as the the error-correction method of choice for 5G wireless communications. They are also considered for use in fiber-optic cables, in flash memories, and in other applications. These developments are, in large part, a direct consequence of the results obtained in our NSF-funded research.</p>\n<p>Polar codes certainly could not be used in practice without a systematic and efficient method to construct them. When polar codes were introduced in 2009, the problem of actually constructing such codes in polynomial time was left open, and was widely considered the most important open problem in this field. Our research has led to a provably effective <strong><em>construction algorithm for polar codes</em></strong>&nbsp;that runs in linear time. The algorithm works extremely well in practice; it has become the <em>de facto</em>&nbsp;standard used by most researchers in the field to construct polar codes.&nbsp;</p>\n<p>Another crucial problem in the theory of polar codes was that of improving their performance at short code lengths. Although asymptotically polar codes achieve capacity, it has been observed that at short and moderate lengths, they do not perform as well as othert state-of-the-art error-correcting codes, such turbo codes or low-density parity-check (LDPC) codes. Consequently, prior to our results, the prevailing opinion was that for polar codes to be any good, the code length needs to be extraordinarily high, thereby making such codes impractical. In this project, we showed emphatically that this conventional wisdom is wrong. We first developed a much more powerful decoding algorithm for polar codes, and then combined it with a subtle modification of the code structure to outperform LDPC codes already at a length of 1,024 bits. The resulting <strong><em>list-decoding scheme for polar codes</em></strong>&nbsp;is decidedly practical; it has been patented by the University of California, with the US government having certain rights in the invention. Moreover, the polar encoders and list-decoders developed in this project currently stand as <span style=\"text-decoration: underline;\">the</span>&nbsp;best-known coding scheme for binary-input Gaussian channels at short lengths. They outperform all other polynomial-time coding schemes constructed since the time of Shannon.</p>\n<p>Our results in the area of high-throughput VLSI architectures have also had a significant impact on the engineering practice of polar codes. Specifically, we have developed a suite of <strong><em>efficient hardware implementations for polar decoders.</em></strong> We showed that such decoders can be implemented in the logarithmic domain, thereby eliminating costly multiplication and division operations. Building upon these results, we were able to increase the throughput of polar decoding hardware by an order of magnitude relative to the prior state-of-the-art. In the final year of the project, we have demonstrated an FPGA implementation of a gigabit-per-second successive-cancellation polar decoder.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/15/2016<br>\n\t\t\t\t\tModified by: Alexander&nbsp;Vardy</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe digital information age, in which we now live, commenced with the work of Claude Shannon in the 1940s. Shannon showed that every communication channel and every data storage system --- no matter how complicated --- can be characterized by a single constant, called the capacity. Moreover, he proved that it is in principle possible to communicate reliably at information rates up to this capacity using error-correcting codes.\n\nSince then, the search for error-correcting codes that achieve Shannon's capacity has been the main open problem in the fields of information theory and coding theory. This search took over 60 years. Polar codes, invented by Erdal Arikan in 2009, provably achieve capacity for a wide range of communication channels with low encoding and decoding complexity, thereby providing the first deterministic solution to this problem. The major goal of this project was to bridge the gap between this remarkable theoretical advance and the existing practice of error-correcting codes --- as implemented, for example, in wireless communications, in fiber-optic networks, in computer disk drives, as well as a plethora of other networks and devices that make the information age possible.\n\nIn order to achieve this goal, in this project, we have succeeded to resolve the two main open problems in the theory of polar codes, which largely prevented their utilization in practice. At the time of writing, polar codes are actively considered for standardization as the the error-correction method of choice for 5G wireless communications. They are also considered for use in fiber-optic cables, in flash memories, and in other applications. These developments are, in large part, a direct consequence of the results obtained in our NSF-funded research.\n\nPolar codes certainly could not be used in practice without a systematic and efficient method to construct them. When polar codes were introduced in 2009, the problem of actually constructing such codes in polynomial time was left open, and was widely considered the most important open problem in this field. Our research has led to a provably effective construction algorithm for polar codes that runs in linear time. The algorithm works extremely well in practice; it has become the de facto standard used by most researchers in the field to construct polar codes. \n\nAnother crucial problem in the theory of polar codes was that of improving their performance at short code lengths. Although asymptotically polar codes achieve capacity, it has been observed that at short and moderate lengths, they do not perform as well as othert state-of-the-art error-correcting codes, such turbo codes or low-density parity-check (LDPC) codes. Consequently, prior to our results, the prevailing opinion was that for polar codes to be any good, the code length needs to be extraordinarily high, thereby making such codes impractical. In this project, we showed emphatically that this conventional wisdom is wrong. We first developed a much more powerful decoding algorithm for polar codes, and then combined it with a subtle modification of the code structure to outperform LDPC codes already at a length of 1,024 bits. The resulting list-decoding scheme for polar codes is decidedly practical; it has been patented by the University of California, with the US government having certain rights in the invention. Moreover, the polar encoders and list-decoders developed in this project currently stand as the best-known coding scheme for binary-input Gaussian channels at short lengths. They outperform all other polynomial-time coding schemes constructed since the time of Shannon.\n\nOur results in the area of high-throughput VLSI architectures have also had a significant impact on the engineering practice of polar codes. Specifically, we have developed a suite of efficient hardware implementations for polar decoders. We showed that such decoders can be implemented in the logarithmic domain, thereby eliminating costly multiplication and division operations. Building upon these results, we were able to increase the throughput of polar decoding hardware by an order of magnitude relative to the prior state-of-the-art. In the final year of the project, we have demonstrated an FPGA implementation of a gigabit-per-second successive-cancellation polar decoder.\n\n\t\t\t\t\tLast Modified: 09/15/2016\n\n\t\t\t\t\tSubmitted by: Alexander Vardy"
 }
}