{
 "awd_id": "1052765",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Continuing Grant",
 "awd_titl_txt": "RUI: Effects of hand gesture on auditory and vocabulary learning of a second language",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": "7032927238",
 "po_email": "btuller@nsf.gov",
 "po_sign_block_name": "Betty Tuller",
 "awd_eff_date": "2011-03-15",
 "awd_exp_date": "2015-02-28",
 "tot_intn_awd_amt": 316238.0,
 "awd_amount": 316238.0,
 "awd_min_amd_letter_date": "2011-03-08",
 "awd_max_amd_letter_date": "2012-07-30",
 "awd_abstract_narration": "Learning to hear novel speech sounds is one of the major challenges of mastering a second language (L2). Previous work using intensive auditory training has helped learners perceive challenging and unfamiliar speech sounds but no method to date has consistently brought learners to native levels. This research project by Yukari Hirata and Spencer Kelly at Colgate University attempts to improve upon previous methods by coupling auditory instruction with visual and motor information conveyed though hand gestures. Specifically, Hirata and Kelly will explore how observing and producing different types of gestures helps native English speakers learn novel speech sounds in Japanese. Hirata and Kelly will investigate this question using behavioral and brain measures and will explore whether mastering these novel speech sounds also leads to better L2 vocabulary learning.\r\n\r\nGiven the increasing contact among people all over the world, developing effective and efficient strategies for L2 teaching is more important than ever. The project also allows for integration of research into the educational experience of undergraduate students at a primarily undergraduate institution, exposing them to the excitement of scientific inquiry. Finally, strengthening on-campus laboratory facilities (that can be used by several disciplines) for the behavioral and neural investigation of foreign language learning will enrich and diversify undergraduate science education.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Yukari",
   "pi_last_name": "Hirata",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Yukari Hirata",
   "pi_email_addr": "yhirata@mail.colgate.edu",
   "nsf_id": "000392715",
   "pi_start_date": "2011-03-08",
   "pi_end_date": null
  },
  {
   "pi_role": "Co-Principal Investigator",
   "pi_first_name": "Spencer",
   "pi_last_name": "Kelly",
   "pi_mid_init": "D",
   "pi_sufx_name": "",
   "pi_full_name": "Spencer D Kelly",
   "pi_email_addr": "skelly@mail.colgate.edu",
   "nsf_id": "000302016",
   "pi_start_date": "2011-03-08",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "Colgate University",
  "inst_street_address": "13 OAK DR",
  "inst_street_address_2": "",
  "inst_city_name": "HAMILTON",
  "inst_state_code": "NY",
  "inst_state_name": "New York",
  "inst_phone_num": "3152287457",
  "inst_zip_code": "133461386",
  "inst_country_name": "United States",
  "cong_dist_code": "22",
  "st_cong_dist_code": "NY22",
  "org_lgl_bus_name": "COLGATE UNIVERSITY",
  "org_prnt_uei_num": "",
  "org_uei_num": "D4P7H8NWZER7"
 },
 "perf_inst": {
  "perf_inst_name": "Colgate University",
  "perf_str_addr": "13 OAK DR",
  "perf_city_name": "HAMILTON",
  "perf_st_code": "NY",
  "perf_st_name": "New York",
  "perf_zip_code": "133461386",
  "perf_ctry_code": "US",
  "perf_cong_dist": "22",
  "perf_st_cong_dist": "NY22",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "725200",
   "pgm_ele_name": "Perception, Action & Cognition"
  },
  {
   "pgm_ele_code": "773100",
   "pgm_ele_name": "Other Global Learning & Trng"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "5937",
   "pgm_ref_txt": "SWEDEN"
  },
  {
   "pgm_ref_code": "5979",
   "pgm_ref_txt": "Europe and Eurasia"
  },
  {
   "pgm_ref_code": "9229",
   "pgm_ref_txt": "RES IN UNDERGRAD INST-RESEARCH"
  },
  {
   "pgm_ref_code": "9251",
   "pgm_ref_txt": "REU SUPP-Res Exp for Ugrd Supp"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  },
  {
   "app_code": "0112",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001213DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 117084.0
  },
  {
   "fund_oblg_fiscal_yr": 2012,
   "fund_oblg_amt": 199154.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>Building on theories of embodied cognition and gesture-speech integration, the present research attempted to improve upon previous methods of second language (L2) instruction by coupling auditory materials with multimodal information conveyed though hand gestures. Specifically, our research examined whether observing and producing different types of hand gesture affect L2 auditory perception, and whether strengthening auditory and phonological representations of an L2 strengthen semantic representations. Length of vowels is phonemic in Japanese, and native English speakers are known to have difficulty auditorily distinguishing these phonemic contrasts. In our study, eighty-eight native English speakers took part in one of four types of video instruction on Japanese vocabulary items with vowel length contrasts. In the videos, the verbal presentation was accompanied by two types of hand movements conveying different phonological rhythm information (i.e., familiar versus novel). In addition, participants either passively observed the gestures in the videos or actively imitated them while learning the L2 word meanings. We measured three learning outcomes: 1) the ability to auditorily distinguish the novel phoneme contrasts, 2) vocabulary learning, and 3) neural correlates of long-term memory consolidation as measured by event-related potentials (ERPs).</p>\n<p>All four groups showed significant improvement in their auditory tests after training, but the amount of improvement did not differ across the four groups. However, the 'Syllable-Observe' condition was the only condition that yielded balanced auditory improvement across four different stimulus types. An unexpected result also emerged: the present 'Syllable-Observe' condition is, by analogy, no more effective than a training condition in which people heard only audio in Hirata &amp; Kelly (2010). Thus, we conclude that the role of gesture&mdash;familiar or unfamiliar and observed or imitated&mdash;is limited in L2 phonological representation and learning. This is interesting in light of the many helpful roles of hand gesture in language learning reported by previous studies (mostly for semantic and pragmatic aspects of language). Our NSF study was the first to thoroughly examine this limitation of hand gesture in L2 phonological learning.</p>\n<p>The vocabulary test scores also showed no difference among the four conditions, indicating no sensitivity to different types of hand gestures or how they were used. An interesting finding emerged, however: the vocabulary scores correlated not only with the auditory post-test but also with the auditory pre-test scores. This suggests that auditory abilities at the end of training are not only closely associated with how well a learner remembers meaning of words, but perhaps more strikingly, auditory abilities at the beginning of training can even predict how well one will memorize vocabulary. &nbsp;</p>\n<p>Regarding the ERP data, our original plan was to examine whether there would be any subtle differences among the four training conditions at a neural level that would be missed by our behavioral measures. We found that the ERP data were consistent with all of the behavioral results as described above, and thus our neural measures did not distinguish the four conditions in any other way than what the behavioral data suggested. &nbsp;</p>\n<p>As a collaboration between a phonetician and a cognitive neuroscientist, the intellectual merit of the present project was that it crossed traditional disciplinary boundaries and tackled issues that were not possible to address within a single field of study. This interdisciplinary collaboration has allowed the PIs to explore important theoretical issues regarding the role of the body in learning, the relationship between gesture and speech in communication, and the connection between phonological learning and vocabulary learning in an L2...",
  "por_txt_cntn": "\nBuilding on theories of embodied cognition and gesture-speech integration, the present research attempted to improve upon previous methods of second language (L2) instruction by coupling auditory materials with multimodal information conveyed though hand gestures. Specifically, our research examined whether observing and producing different types of hand gesture affect L2 auditory perception, and whether strengthening auditory and phonological representations of an L2 strengthen semantic representations. Length of vowels is phonemic in Japanese, and native English speakers are known to have difficulty auditorily distinguishing these phonemic contrasts. In our study, eighty-eight native English speakers took part in one of four types of video instruction on Japanese vocabulary items with vowel length contrasts. In the videos, the verbal presentation was accompanied by two types of hand movements conveying different phonological rhythm information (i.e., familiar versus novel). In addition, participants either passively observed the gestures in the videos or actively imitated them while learning the L2 word meanings. We measured three learning outcomes: 1) the ability to auditorily distinguish the novel phoneme contrasts, 2) vocabulary learning, and 3) neural correlates of long-term memory consolidation as measured by event-related potentials (ERPs).\n\nAll four groups showed significant improvement in their auditory tests after training, but the amount of improvement did not differ across the four groups. However, the 'Syllable-Observe' condition was the only condition that yielded balanced auditory improvement across four different stimulus types. An unexpected result also emerged: the present 'Syllable-Observe' condition is, by analogy, no more effective than a training condition in which people heard only audio in Hirata &amp; Kelly (2010). Thus, we conclude that the role of gesture&mdash;familiar or unfamiliar and observed or imitated&mdash;is limited in L2 phonological representation and learning. This is interesting in light of the many helpful roles of hand gesture in language learning reported by previous studies (mostly for semantic and pragmatic aspects of language). Our NSF study was the first to thoroughly examine this limitation of hand gesture in L2 phonological learning.\n\nThe vocabulary test scores also showed no difference among the four conditions, indicating no sensitivity to different types of hand gestures or how they were used. An interesting finding emerged, however: the vocabulary scores correlated not only with the auditory post-test but also with the auditory pre-test scores. This suggests that auditory abilities at the end of training are not only closely associated with how well a learner remembers meaning of words, but perhaps more strikingly, auditory abilities at the beginning of training can even predict how well one will memorize vocabulary.  \n\nRegarding the ERP data, our original plan was to examine whether there would be any subtle differences among the four training conditions at a neural level that would be missed by our behavioral measures. We found that the ERP data were consistent with all of the behavioral results as described above, and thus our neural measures did not distinguish the four conditions in any other way than what the behavioral data suggested.  \n\nAs a collaboration between a phonetician and a cognitive neuroscientist, the intellectual merit of the present project was that it crossed traditional disciplinary boundaries and tackled issues that were not possible to address within a single field of study. This interdisciplinary collaboration has allowed the PIs to explore important theoretical issues regarding the role of the body in learning, the relationship between gesture and speech in communication, and the connection between phonological learning and vocabulary learning in an L2.\n\nThe broader impacts of this research are many: First, the project thoroughly integrated resea..."
 }
}