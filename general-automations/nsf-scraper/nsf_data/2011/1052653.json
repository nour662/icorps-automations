{
 "awd_id": "1052653",
 "agcy_id": "NSF",
 "tran_type": "Grant",
 "awd_istr_txt": "Standard Grant",
 "awd_titl_txt": "RUI: Effects of visual phonetic similarity on audiovisual spoken word recognition",
 "cfda_num": "47.075",
 "org_code": "04040000",
 "po_phone": null,
 "po_email": "",
 "po_sign_block_name": "Joan Maling",
 "awd_eff_date": "2011-07-15",
 "awd_exp_date": "2015-06-30",
 "tot_intn_awd_amt": 107922.0,
 "awd_amount": 107922.0,
 "awd_min_amd_letter_date": "2011-07-20",
 "awd_max_amd_letter_date": "2014-06-03",
 "awd_abstract_narration": "Spoken word recognition is defined as the process by which acoustic patterns are matched to meanings in the \"mental lexicon\" -- the memory repository of the approximately 85000 words known by an adult language speaker. Previous research has demonstrated that the number of lexical entries acoustically similar to a given word influences the ease with which that word is recognized. However, speech recognition is not solely an acoustic phenomenon; watching someone speak also provides additional information about the content of an utterance. Although there is presently a solid understanding of the role of acoustic similarity in spoken word recognition, less is known about visual similarity, and very little is known about the interaction of acoustic and visual similarity when both sources of information are available.  This may be because visual similarity is particularly difficult to measure. One problem is that speech units that are acoustically different can be visually identical. Words that are visually identical are said to comprise a Lexical Equivalence Class (LEC). Previous research has shown that the number of words residing in an LEC affects the ease with which those words are lipread. Similarly, the size of an LEC affects the extent to which visual information enhances the recognition of auditory speech. However, the makeup of an LEC is to some extent dependent on the speaker. The proposed research will accomplish two goals: first, a publicly accessible computational tool will be built to facilitate computational and experimental investigations of visual lexical similarity. Second, several behavioral experiments will further elucidate the role that visual similarity plays in spoken word recognition. This project will advance our understanding of the basic mechanisms involved in spoken word recognition. Such knowledge will be useful for clinicians working with deaf or hearing-impaired populations, and engineers working on problems in automatic speech recognition.",
 "awd_arra_amount": 0.0,
 "dir_abbr": "SBE",
 "org_dir_long_name": "Directorate for Social, Behavioral and Economic Sciences",
 "div_abbr": "BCS",
 "org_div_long_name": "Division of Behavioral and Cognitive Sciences",
 "awd_agcy_code": "4900",
 "fund_agcy_code": "4900",
 "pi": [
  {
   "pi_role": "Principal Investigator",
   "pi_first_name": "Lorin",
   "pi_last_name": "Lachs",
   "pi_mid_init": "",
   "pi_sufx_name": "",
   "pi_full_name": "Lorin Lachs",
   "pi_email_addr": "llachs@csufresno.edu",
   "nsf_id": "000556605",
   "pi_start_date": "2011-07-20",
   "pi_end_date": null
  }
 ],
 "inst": {
  "inst_name": "California State University-Fresno Foundation",
  "inst_street_address": "4910 N CHESTNUT AVE",
  "inst_street_address_2": "",
  "inst_city_name": "FRESNO",
  "inst_state_code": "CA",
  "inst_state_name": "California",
  "inst_phone_num": "5592780840",
  "inst_zip_code": "937261852",
  "inst_country_name": "United States",
  "cong_dist_code": "20",
  "st_cong_dist_code": "CA20",
  "org_lgl_bus_name": "CALIFORNIA STATE UNIVERSITY, FRESNO FOUNDATION",
  "org_prnt_uei_num": "CJSRSPWTJUH7",
  "org_uei_num": "CJSRSPWTJUH7"
 },
 "perf_inst": {
  "perf_inst_name": "California State University-Fresno",
  "perf_str_addr": "Maple and Shaw Avenue",
  "perf_city_name": "Fresno",
  "perf_st_code": "CA",
  "perf_st_name": "California",
  "perf_zip_code": "937400000",
  "perf_ctry_code": "US",
  "perf_cong_dist": "20",
  "perf_st_cong_dist": "CA20",
  "perf_ctry_name": "",
  "perf_ctry_flag": "1"
 },
 "pgm_ele": [
  {
   "pgm_ele_code": "131100",
   "pgm_ele_name": "Linguistics"
  }
 ],
 "pgm_ref": [
  {
   "pgm_ref_code": "1311",
   "pgm_ref_txt": "LINGUISTICS"
  },
  {
   "pgm_ref_code": "9229",
   "pgm_ref_txt": "RES IN UNDERGRAD INST-RESEARCH"
  },
  {
   "pgm_ref_code": "SMET",
   "pgm_ref_txt": "SCIENCE, MATH, ENG & TECH EDUCATION"
  }
 ],
 "app_fund": [
  {
   "app_code": "0111",
   "app_name": "NSF RESEARCH & RELATED ACTIVIT",
   "app_symb_id": "040100",
   "fund_code": "01001112DB",
   "fund_name": "NSF RESEARCH & RELATED ACTIVIT",
   "fund_symb_id": "040100"
  }
 ],
 "oblg_fy": [
  {
   "fund_oblg_fiscal_yr": 2011,
   "fund_oblg_amt": 107922.0
  }
 ],
 "por": {
  "por_cntn": "<div class=\"porColContainerWBG\">\n<div class=\"porContentCol\"><p>The purpose of this award was twofold: first, a computational tool was proposed that would simplify the analysis of the complex and vast amounts of data necessary for analysis of speech perception and spoken word recognition under visual-only conditions; second, investigations using this tool would shed light on how lipreading is sensitive to the effects of idiosyncratic speaking styles of different speakers. Although the computational tool in the first purpose listed above was not developed fully, a large amount of useful data was collected for the second purpose. We recorded a large database of stimuli for use in experiments examining the role of stimulus similarity and lexical similarity in lipreading perception. The database consists of 8 native English speakers from the Central Valley region of California. Each speaker has been recorded uttering each of the phonemes of English in VCV or hVd contexts (as appropriate). We have also recorded each speaker uttering 166 phonetically-balanced sentences. All the stimuli have been segmented from the original recordings, and are now available as independent movie files for use in computer-controlled experiments.</p>\n<p>&nbsp;</p>\n<p>We also had 122 Introduction to Psychology students view each of the VCV stimuli and attempt to identify them under visual-only conditions. Each VCV stimulus was viewed 7 times. Thus, the dataset from this experiment consists of 122*7 = 854 responses per phoneme per talker. Across talkers, that is 6832 responses per phoneme. The massive dataset of responses was then analyzed for inter-talker differences in the visual confusability of lipread phonemes. We found some interesting patterns in the set of phonetic equivalence classes across talkers and presented the preliminary results of this analysis at the 94<sup>th</sup> Annual Meeting of the Western Psychological Association. (It should be noted that two co-authors for this presentation were undergraduates at the time that they assisted with this project, consistent with the goals of the RUI program).</p>\n<p>&nbsp;</p>\n<p>We subsequently investigated the ramifications of these inter-talker viseme differences by analyzing the structure of the mental lexicon when transcribed with talker-specific phonemic equivalence classes. Our results showed that a variety of metrics used to assess the complexity of a visually-transcribed mental lexicon were sensitive to inter-talker differences in the visual confusability of phonemes. A poster about these differences was presented at the 95<sup>th</sup> Annual Conference of the Western Psychological Association. As with the first conference presentation, co-authors were students enrolled at Fresno State: 1 was an undergraduate and 2 were Master&rsquo;s-level graduate students.</p>\n<p>&nbsp;</p>\n<p>It is hoped that the new techniques developed for the analysis of the aforementioned data will be fruitful for investigating inter-talker differences in visual phonetic confusability. It is further hoped that revealing differences in inter-talker visual phonetic confusability will guide the development of more sophisticated computational tools for understanding visual-only word recognition. In total, the results of this project may contribute to a better understanding of the cognitive processes involved in spoken word recognition under visual-only conditions, and may lead to a more complete model of human language behavior. Such an understanding might be useful in the design of training programs for cochlear implant recipients and in remediation of hearing loss with age.</p><br>\n<p>\n\t\t\t\t      \tLast Modified: 09/21/2015<br>\n\t\t\t\t\tModified by: Lorin&nbsp;Lachs</p>\n</div>\n<div class=\"porSideCol\"></div>\n</div>",
  "por_txt_cntn": "\nThe purpose of this award was twofold: first, a computational tool was proposed that would simplify the analysis of the complex and vast amounts of data necessary for analysis of speech perception and spoken word recognition under visual-only conditions; second, investigations using this tool would shed light on how lipreading is sensitive to the effects of idiosyncratic speaking styles of different speakers. Although the computational tool in the first purpose listed above was not developed fully, a large amount of useful data was collected for the second purpose. We recorded a large database of stimuli for use in experiments examining the role of stimulus similarity and lexical similarity in lipreading perception. The database consists of 8 native English speakers from the Central Valley region of California. Each speaker has been recorded uttering each of the phonemes of English in VCV or hVd contexts (as appropriate). We have also recorded each speaker uttering 166 phonetically-balanced sentences. All the stimuli have been segmented from the original recordings, and are now available as independent movie files for use in computer-controlled experiments.\n\n \n\nWe also had 122 Introduction to Psychology students view each of the VCV stimuli and attempt to identify them under visual-only conditions. Each VCV stimulus was viewed 7 times. Thus, the dataset from this experiment consists of 122*7 = 854 responses per phoneme per talker. Across talkers, that is 6832 responses per phoneme. The massive dataset of responses was then analyzed for inter-talker differences in the visual confusability of lipread phonemes. We found some interesting patterns in the set of phonetic equivalence classes across talkers and presented the preliminary results of this analysis at the 94th Annual Meeting of the Western Psychological Association. (It should be noted that two co-authors for this presentation were undergraduates at the time that they assisted with this project, consistent with the goals of the RUI program).\n\n \n\nWe subsequently investigated the ramifications of these inter-talker viseme differences by analyzing the structure of the mental lexicon when transcribed with talker-specific phonemic equivalence classes. Our results showed that a variety of metrics used to assess the complexity of a visually-transcribed mental lexicon were sensitive to inter-talker differences in the visual confusability of phonemes. A poster about these differences was presented at the 95th Annual Conference of the Western Psychological Association. As with the first conference presentation, co-authors were students enrolled at Fresno State: 1 was an undergraduate and 2 were Master\u00c6s-level graduate students.\n\n \n\nIt is hoped that the new techniques developed for the analysis of the aforementioned data will be fruitful for investigating inter-talker differences in visual phonetic confusability. It is further hoped that revealing differences in inter-talker visual phonetic confusability will guide the development of more sophisticated computational tools for understanding visual-only word recognition. In total, the results of this project may contribute to a better understanding of the cognitive processes involved in spoken word recognition under visual-only conditions, and may lead to a more complete model of human language behavior. Such an understanding might be useful in the design of training programs for cochlear implant recipients and in remediation of hearing loss with age.\n\n\t\t\t\t\tLast Modified: 09/21/2015\n\n\t\t\t\t\tSubmitted by: Lorin Lachs"
 }
}